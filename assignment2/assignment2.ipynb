{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c016668",
   "metadata": {},
   "source": [
    "# Preface\n",
    "> The questions/exercises given for the assignment has been repeated in this notebook, they have been put in bold. Our answers and explanations have been indented, or printed from the executed code, to easily distinguish them from the exercise text. Throughout the text we are using good guys and heroes synonymously and bad guys and villains synonymously. We also use the word alignment(s) to refer to good and bad collectively.\n",
    "\n",
    "## Imports\n",
    "> The following imports are needed throughout the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d7d83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import nltk\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import re\n",
    "import string\n",
    "import statistics\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "import json\n",
    "import urllib.request\n",
    "import powerlaw\n",
    "\n",
    "from nltk import word_tokenize, FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "from community import community_louvain\n",
    "from collections import Counter\n",
    "from os import listdir\n",
    "from bs4 import BeautifulSoup\n",
    "from fa2 import ForceAtlas2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3682d81",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "> The downloaded files can be found in the folders `dc` and `marvel`, in our [github project](https://github.com/pete414n/02805SG/tree/main/assignment2)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5442aec6",
   "metadata": {},
   "source": [
    "# Basic Stats\n",
    "\n",
    "**Write a short paragraph describing the network. The paragraph should contain the following information**\n",
    "- **The number of nodes and links.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70abdf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Edgelist saved as gml so we can keep the node attributes\n",
    "G = nx.read_gml(\"superhero.edgelist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e72e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of nodes\n",
    "num_nodes = len(G.nodes())\n",
    "print(f\"Number of nodes = {num_nodes}\")\n",
    "\n",
    "# Number of links\n",
    "num_links = len(G.edges()) # Assuming we get an edge list\n",
    "print(f\"Number of links = {num_links}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53ee3ec",
   "metadata": {},
   "source": [
    "- **The average, median, mode, minimum and maximum value of the network's in-degree.s And of the out-degrees.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a8b4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a sorted list of the in degrees and a list of the out degrees\n",
    "in_degree_list = sorted([d for (c, d) in G.in_degree], reverse=True)\n",
    "out_degree_list = sorted([d for (c, d) in G.out_degree], reverse=True)\n",
    "\n",
    "# Average in degree\n",
    "print(f\"average in degree = {np.mean(in_degree_list)}\")\n",
    "\n",
    "# Average out degree\n",
    "print(f\"average out degree = {np.mean(out_degree_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7a2915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Median in degree\n",
    "print(f\"median in degree = {np.median(in_degree_list)}\")\n",
    "\n",
    "# Median out degree\n",
    "print(f\"median out degree = {np.median(out_degree_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c8557f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mode in degree\n",
    "mode = max(set(in_degree_list), key=in_degree_list.count)\n",
    "print(f\"mode in degree = {mode}\")\n",
    "\n",
    "# Mode out degree\n",
    "mode = max(set(out_degree_list), key=out_degree_list.count)\n",
    "print(f\"mode out degree = {mode}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48a725b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we have the sorted list, the first element will be the max and the last will be the min\n",
    "# Minimum and maximum in degree\n",
    "min_in_degree = in_degree_list[-1]\n",
    "max_in_degree = in_degree_list[0]\n",
    "print(f\"minimum in degree = {min_in_degree}\\nmaximum in degree = {max_in_degree}\\n\")\n",
    "\n",
    "# Minimum and maximum out degree\n",
    "min_out_degree = out_degree_list[-1]\n",
    "max_out_degree = out_degree_list[0]\n",
    "print(f\"minimum out degree = {min_out_degree}\\nmaximum out degree = {max_out_degree}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebb4f20",
   "metadata": {},
   "source": [
    "> When creating the network we have tried to get the wikitext as clean as possible and only have the true text without i.e. the contents list. \n",
    "We have created the network so the nodes conatins 3 attributes, universe, txtlength and sentiment. The first one tells whether it's a DC or Marvel character, the next gives the length of the whole wikitext and the last gives the sentiment value of the wikitext. \n",
    "We have chosen not to include all the characters with pages that were redirects. We have done this by checking if the text belonging to the given character were starting with #redirect, if so we didn't include this node in the network. We have also removed all nodes who had a sentiment value of 0. Then we have removed isolates from the network and found the largest connected component. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6d4401",
   "metadata": {},
   "source": [
    "> Our network has 1602 nodes and 19068. The average in- and out-degree are both 11.75..., they are the same since each edge contributes one to the total out-degree and one to the total in-degree, and a difference would indicate that there somehow where a higher total in-degree than a total out-degree, or vice versa. The median in degree is 5, and the median out degree is 9. The mode for in degree is 0, which corresponds to characters that link to other characters, but are not linked to by anyone else. The mode for out degree is 2. These two numbers tells us that the most common in- and out-degree is relatively low, considering the total number of nodes. The minimum in- and out-degree are both 0, an out-degree of 0 corresponds to a character that links to no other characters, but is linked to by at least one other character. The maximum in-degree is 429, this is the character that most other characters are linking to, corresponding to roughly 1/4 of all of our nodes linking to this single node. The maximum out degree is 68, which corresponds to the character that has the most links to other characters. Overall these numbers indicates to us that the graph is not very dense, since it looks like we (might) have quite a few nodes that do not have many edges going in or out of them. At the same time this also indicates that we must have some nodes that have many links, relatively to the others. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6a7051",
   "metadata": {},
   "source": [
    "**We also want the degree distributions and a plot of the network**\n",
    "- **Create and visualize in- and out-going degree distributions as described in Lecture 4. Think about which axes you should use - loglog is great for power-law distributions, but not necessarily for Poisson-like degree distributions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8f0140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusting size of figures\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "\n",
    "# Creating the degree distirbution for in and out degress\n",
    "in_counts, in_bins = np.histogram(in_degree_list, bins=max_in_degree)\n",
    "out_counts, out_bins = np.histogram(out_degree_list, bins=max_out_degree)\n",
    "\n",
    "#Plotting the in-degree distribution with log-log axes \n",
    "#as the in-degree distribution is much like a power-law distribution\n",
    "plt.plot(in_bins[:-1], in_counts, color = 'green')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('Number of Nodes')\n",
    "plt.title('In-degree distributions')\n",
    "plt.figtext(.5, -0.05, f\"Plot of the in-degree distributions for the graph of DC/Marvel heroes. The plot shows that as the in-degree increases there are fewer and fewer nodes with that in-degree.\", ha=\"center\")\n",
    "plt.show()\n",
    "\n",
    "#Plotting the out-degree distribution with normal axes \n",
    "#as the out-degree distribution is much like a Poisson distribution\n",
    "plt.plot(in_bins[:-1], in_counts, color = 'green')\n",
    "plt.plot(out_bins[:-1], out_counts, color = 'red')\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('Number of Nodes')\n",
    "plt.title('Out-degree distributions')\n",
    "plt.figtext(.5, -0.05, f\"Plot of the in-degrees (green) and the out-degrees (red). The plot shows that the out-degree of the nodes overall is smaller, but there are few nodes with a very low out-degree, while there are many nodes with a very low in-degree.\", ha=\"center\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6064726b",
   "metadata": {},
   "source": [
    "> The in- and out-degree plots seem to support what we initially suspected, that we have many nodes with very low, or zero, in-degree. We also see that there are few nodes with an out-degree of zero or one, this is also what we expected. It seems that the in-degree distribution could follow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b66195",
   "metadata": {},
   "source": [
    "- **Estimate, report, and reflect on the slope and starting value of the incoming degree distribtion's power law using the tools described in Lecture 5 - and display the fit on top of your incoming degree-distribution plot**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4928607",
   "metadata": {},
   "source": [
    "Notes from slack, to be deleted:\n",
    "\n",
    "Slack - General, Sune: In terms of which exercises, I’ve tried to be more clear. It’s basically just the fit to the out-degree distribution and reflecting on your findings.\n",
    "\n",
    "Slack - Lecture 5, Sune: Then I also think that a power-law usually forms a straight line on a log-log plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454feffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = powerlaw.Fit(in_degree_list).alpha # in- or out-degree?\n",
    "\n",
    "# Creating the degree distirbution for in and out degress\n",
    "in_counts, in_bins = np.histogram(in_degree_list, bins=max_in_degree)\n",
    "out_counts, out_bins = np.histogram(out_degree_list, bins=max_out_degree)\n",
    "\n",
    "# Plotting the powerlaw fit\n",
    "line_x = np.linspace(1, 500, 50)\n",
    "line = np.array([x**(-alpha) for x in line_x])\n",
    "plt.plot(line_x +5, line*max(in_counts), color=\"orange\")\n",
    "\n",
    "#Plotting the in-degree distribution with log-log axes \n",
    "#as the in-degree distribution is much like a power-law distribution\n",
    "plt.plot(in_bins[:-1], in_counts, color = 'green')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('Number of Nodes')\n",
    "plt.title('In-degree distributions')\n",
    "plt.figtext(.5, -0.05, f\"Plot of the in-degree distributions for the graph of DC/Marvel heroes. The plot shows that as the in-degree increases there are fewer and fewer nodes with that in-degree.\", ha=\"center\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6265ba5c",
   "metadata": {},
   "source": [
    "> From the value of alpha we can see that the in-degrees belong to a power-law distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1760e819",
   "metadata": {},
   "source": [
    "- **Plot the network using the Force Atlas algorithm as described in Lecture 5.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f30af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the graph undirected for the plotting\n",
    "uG = G.to_undirected()\n",
    "\n",
    "# Lists to contain nodes belonging to DC and Marvel\n",
    "dc_nodes = []\n",
    "marvel_nodes = []\n",
    "\n",
    "#Lists to contain the degrees for the DC nodes and the Marvel nodes. To be used when plotting using the Force Atlas\n",
    "dc_degrees = []\n",
    "marvel_degrees = []\n",
    "\n",
    "# Dividing nodes in Marvel and DC universe\n",
    "for n in uG.nodes():\n",
    "    if uG.nodes[n]['universe'] == 'Marvel':\n",
    "        marvel_nodes += [n]    \n",
    "        marvel_degrees += [uG.degree[n]]\n",
    "    else:\n",
    "        dc_nodes += [n]\n",
    "        dc_degrees += [uG.degree[n]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f492c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusting figure size\n",
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "\n",
    "forceatlas2 = ForceAtlas2(\n",
    "                        # Behavior alternatives\n",
    "                        outboundAttractionDistribution=True,  # Dissuade hubs\n",
    "                        linLogMode=False,  # NOT IMPLEMENTED\n",
    "                        adjustSizes=False,  # Prevent overlap (NOT IMPLEMENTED)\n",
    "                        edgeWeightInfluence=1.0,\n",
    "\n",
    "                        # Performance\n",
    "                        jitterTolerance=1.0,  # Tolerance\n",
    "                        barnesHutOptimize=True,\n",
    "                        barnesHutTheta=1.2, # original 1.2\n",
    "                        multiThreaded=False,  # NOT IMPLEMENTED\n",
    "\n",
    "                        # Tuning\n",
    "                        scalingRatio=2.0,\n",
    "                        strongGravityMode=True,\n",
    "                        gravity=0.1, # original 0.5\n",
    "\n",
    "                        # Log\n",
    "                        verbose=True)\n",
    "\n",
    "positions = forceatlas2.forceatlas2_networkx_layout(G, pos=None, iterations=2000)\n",
    "nx.draw_networkx_nodes(G, positions, nodelist = marvel_nodes, node_size=marvel_degrees, node_color=\"blue\", alpha=0.4)\n",
    "nx.draw_networkx_nodes(G, positions, nodelist = dc_nodes, node_size=dc_degrees, node_color=\"red\", alpha=0.4)\n",
    "nx.draw_networkx_edges(G, positions, edge_color=\"green\", alpha=0.05)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8409767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusting figure size to default \n",
    "plt.rcParams['figure.figsize'] = [8.0, 6.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9079b3f8",
   "metadata": {},
   "source": [
    "# Communities\n",
    "\n",
    "**Identify the communities in one or both of the superhero universes (DC/Marvel) as described in Week 7.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d06d328",
   "metadata": {},
   "source": [
    "> We will be focusing on the DC superhero universe. We start by recreating our graph for just the DC characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f08801c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the DC network\n",
    "dc_G = uG.subgraph(dc_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c008b257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example from: https://perso.crans.org/aynaud/communities/\n",
    "# We compute the best partition using the Louvain algorithm\n",
    "partition = community_louvain.best_partition(dc_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585412c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dividing the nodes in lists corresponding to their community\n",
    "partition_list = []\n",
    "for com in set(partition.values()) :\n",
    "    list_nodes = [nodes for nodes in partition.keys()\n",
    "                                if partition[nodes] == com]\n",
    "    partition_list.append(list_nodes)\n",
    "#Sorting the list of communities according to their size\n",
    "partition_list = sorted(partition_list, key=len, reverse=True)\n",
    "#print(partition_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94983cf0",
   "metadata": {},
   "source": [
    "**Visualize the communities by coloring the graph's nodes according to community affiliation - also as described in Week 7.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a9d857",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing the communities by using ForceAtlas\n",
    "forceatlas2 = ForceAtlas2(\n",
    "                        # Behavior alternatives\n",
    "                        outboundAttractionDistribution=True,  # Dissuade hubs\n",
    "                        linLogMode=False,  # NOT IMPLEMENTED\n",
    "                        adjustSizes=False,  # Prevent overlap (NOT IMPLEMENTED)\n",
    "                        edgeWeightInfluence=1,\n",
    "\n",
    "                        # Performance\n",
    "                        jitterTolerance=1.0,  # Tolerance 5.0\n",
    "                        barnesHutOptimize=True,\n",
    "                        barnesHutTheta=1.2, #0.5\n",
    "                        multiThreaded=False,  # NOT IMPLEMENTED\n",
    "\n",
    "                        # Tuning\n",
    "                        scalingRatio=1.5, #3.0\n",
    "                        strongGravityMode=True,\n",
    "                        gravity=0.25, #0.5\n",
    "\n",
    "                        # Log\n",
    "                        verbose=True)\n",
    "\n",
    "positions = forceatlas2.forceatlas2_networkx_layout(dc_G, pos=None, iterations=2000)\n",
    "#Setting the colors to use for giving each community a color\n",
    "cmap = plt.get_cmap('plasma', max(partition.values()) + 1)\n",
    "#Setting the figure size\n",
    "plt.rcParams['figure.figsize'] = [4, 4]\n",
    "nx.draw_networkx_nodes(dc_G, positions, partition.keys(), node_size=40,\n",
    "                        cmap=cmap, node_color=list(partition.values()))\n",
    "nx.draw_networkx_edges(dc_G, positions, edge_color='green', alpha=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3278daf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calcualtions to use for answering the questions below\n",
    "modularity = nx.algorithms.community.modularity(dc_G, partition_list)\n",
    "print('The modularity is: ' + str(modularity))\n",
    "\n",
    "print(\"There are \" + str(len(partition_list)) + \" communities in total\")\n",
    "\n",
    "print(\"The size of each community are:\")\n",
    "for sublist in partition_list:\n",
    "    print(len(sublist))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bd0dea",
   "metadata": {},
   "source": [
    "**Write a paragraph about your work to identify the communities. The paragraph should include**\n",
    "- **Information about the algorithm you used to find communities.**\n",
    "- **The value of modularity for your network.**\n",
    "- **The number of communities and their sizes (in terms of number of nodes).**\n",
    "\n",
    "> We have chosen to work only on the DC network. We have used the Louvain algorithm to find the communities. This algorithm makes communities with focus on optimizing the modularity. \n",
    "As can be seen in the code above the modularity of the network is 0.30 (this varies a little every time we run the code but it is around this value). \n",
    "\n",
    "> As shown in the network science book section 9.4 if the modularity is 0.41 it is an optimal partition and if the modularity is 0.22 it is a suboptimal partition. Our value is in between those, so we at least have an suboptimal partition and almost an optimal partition. So the Louvain algorithm have worked well with dividing our network in communities.  \n",
    "\n",
    "> The network have 10 communitites in total (the number of communities varies between 9-12 for each time we run the code, but mostly it is 10), the smallest community have a size of 1 and the largest is 80. The sizes of each community can be seen in the code above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dcf2d4",
   "metadata": {},
   "source": [
    "**For the 10 largest communities, process the text in order to create a TF-IDF vector for each community. Explain in your own words how TF-IDF works.**\n",
    "\n",
    "> The TF-IDF is telling how important a word is to a document/page. The TF stands for term frequency and is an expression of how many times a word/term appears in a document. There are different ways to calculate this value, e.g. just calculating how many times a word appears in a document (the raw count), calculating the raw count and dividing by the total number of words in the document (relative term frequency), taking the logarithm to 1 plus the raw count (logarithmically scaled frequency). We have chosen to use the relative term frequency as it gives a value of how big of a fraction of the document a term is. \n",
    "The IDF stands for inverse document frequency. It is a measure of how much information a word gives. This measure checks in how many documents the word are appering. It measures the logarithmically scaled inverse frequency, so if a word is appearing in many documents it will get a lower idf value as it won't be giving as much information to the document. This is helping to make sure that words as the, and, a, an etc. won't be considered as important to the text. If we only used the TF value, those words would often be getting the highest values, and thereby be considered as more important to the text. THere are also different ways to calculate the IDF value, we have chosen to use the one called inverse document frequency smooth, which takes the logarithm to the total number of documents divided by how many documents a term appears in + 1 (this is to make sure we won't get a division by zero if a term don't appear in any) and then add 1 to this logarithm. THis makes sure that the value will be positive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03ae420",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Putting together the text from the pages belonging to each community\n",
    "community_texts = []\n",
    "\n",
    "for sublist in partition_list[:10]:\n",
    "    com_txt = []\n",
    "    for character in sublist:\n",
    "        f = open(\"./dc/\"+character+\".txt\")\n",
    "        raw = f.read()\n",
    "        # Tokenizing the pages and only taking actual words\n",
    "        tokens = nltk.wordpunct_tokenize(BeautifulSoup(raw, 'html.parser').get_text())\n",
    "        file_text = [w.lower() for w in tokens if w.isalpha()]\n",
    "        com_txt = com_txt + file_text\n",
    "    community_texts.append(com_txt)\n",
    "    \n",
    "#print(len(community_texts))\n",
    "#print(len(partition_list))\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "#Sorting away all the stopwords as those are not interesting for the text\n",
    "community_strings = []\n",
    "for txt in community_texts:\n",
    "    com_words = [w for w in txt if w not in stopwords]\n",
    "    #com_string = ' '.join(com_words)\n",
    "    community_strings.append(com_words)\n",
    "#print(len(community_strings))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993c9de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(community_strings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e635d6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making a list that contains the unique terms in each community\n",
    "unique_terms = []\n",
    "for community_words in community_strings:\n",
    "    unique_terms.append(list(set(community_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb94c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to calculate the idf value\n",
    "def idf(word, unique_list):\n",
    "    N = len(unique_list)\n",
    "    term_appears = 0\n",
    "    #Looping over the unique terms lists for each community and checking whether the given word appears\n",
    "    for sublist in unique_list:\n",
    "        if word in sublist:\n",
    "            term_appears+=1\n",
    "    #print(word + \", \" + str(term_appears))\n",
    "    idf_val = math.log(N/(1+term_appears))+1\n",
    "    return idf_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7291668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the tf-idf list\n",
    "tfidf_list = []\n",
    "\n",
    "#Looping over each community\n",
    "for community_words in community_strings:\n",
    "    # Calculating the frequency distribution of the words\n",
    "    fdist = FreqDist(community_words)\n",
    "    total_terms = len(community_words)\n",
    "    tfidf=[]\n",
    "    #Finding the tf-idf value for each word\n",
    "    for word in fdist:\n",
    "        idf_val = idf(word, unique_terms)\n",
    "        tf_val = fdist[word]/total_terms\n",
    "        #print(word + \", \" + str(tf_val) + \", \" + str(idf_val) + str(tf_val*idf_val))\n",
    "        tfidf_elem=(word, tf_val*idf_val)\n",
    "        tfidf.append(tfidf_elem)\n",
    "    #print(tfidf[:10])\n",
    "    tfidf_list.append(tfidf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a21ef0",
   "metadata": {},
   "source": [
    "**Then visualize the TF-IDF vectors using wordclouds (as described in Week 7). Remember to comment on your word-clouds (e.g. in the figure captions): Do the wordclouds/TF-IDF lists enable you to understand the communities you have found (or is it just gibberish)? Justify your answer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677424d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig = plt.figure()\n",
    "fig = plt.figure(figsize=(15, 20))\n",
    "for i in range(len(tfidf_list)):\n",
    "    ax = fig.add_subplot(5,2,i+1)\n",
    "    wordcloud = WordCloud(background_color='black', width=1800,\n",
    "                      height=1400, collocations=False).generate_from_frequencies(dict(tfidf_list[i]))\n",
    "\n",
    "    ax.imshow(wordcloud)\n",
    "    ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200e91a4",
   "metadata": {},
   "source": [
    "> This is just a placeholder for the paragraph for our answer and the justification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4ebec0",
   "metadata": {},
   "source": [
    "# Sentiment\n",
    "\n",
    "**First calculate the sentiment for all character pages. Calculate the mean value of the sentiment and the 10th and 90th percentile values for the sentiment. Then create a nice histogram displaying the distribution of sentiment values for all pages. Indicate the mean, etc on that histogram.**\n",
    "\n",
    "> The following code shows how we have calculated and found sentiments for the different files, but since we had to store the graph as an edgelist, this had to be done before we got to this part of the assignment.\n",
    "We start by preparing the data from the files that have been given for the exercise. First we make a list of all the words from the given sentiment list with their associated average happiness score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f563ac13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store tuples of words and their \n",
    "# average happiness score\n",
    "sent_list = []\n",
    "\n",
    "# Read in the .tsv file\n",
    "with open(\"dataset_s1.tsv\", \"r\", encoding=\"utf8\") as sent_file:\n",
    "    tsv_reader = csv.DictReader(sent_file, delimiter=\"\\t\")\n",
    "    # For each sentiment in the file, save the word and average happiness in a tuple\n",
    "    # and add it to the list\n",
    "    for sent in tsv_reader:\n",
    "        word = sent[\"word\"]\n",
    "        average = sent[\"happiness_average\"]\n",
    "        sent_list.append((word, average))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9850a1e2",
   "metadata": {},
   "source": [
    "> Then we create a function to calculate the sentiment from a frequency distribution for a given file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058fe25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating the sentiment of a file from the frequency distribution for that file\n",
    "def sentiment(tokens):\n",
    "    # Total sentiment score of file\n",
    "    sent_sum = 0.0\n",
    "    # Total number of occurences of words\n",
    "    occ_sum = 0\n",
    "    \n",
    "    # For each token and associated number of occurences\n",
    "    for token, occ in tokens.items():\n",
    "        # If the token is in the given list of words with rated happiness\n",
    "        for word, score in sent_list:\n",
    "            if token == word:\n",
    "                sent_sum += (float(score) * occ)\n",
    "                occ_sum += occ\n",
    "    return sent_sum / occ_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2a0495",
   "metadata": {},
   "source": [
    "> We have added an attribute sentiment to the nodes, and as the other attributes were set for each node, the sentiment for that node was calculated using the above `sentiment`function. We then create a list of tuples, where each tuple contains a character name and the associated sentiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1cfa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = nx.get_node_attributes(G, \"sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7042dced",
   "metadata": {},
   "source": [
    "> We are now ready to calculate the mean, 10th percentile, and the 90th percentile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7247cfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source: https://stackoverflow.com/questions/2374640/how-do-i-calculate-percentiles-with-python-numpy\n",
    "# mean\n",
    "mean_total = np.mean(list(sentiments.values()))\n",
    "\n",
    "# 10th percentile\n",
    "per_10th = np.percentile(list(sentiments.values()), 10)\n",
    "\n",
    "# 90 percentile sentiment \n",
    "per_90th =  np.percentile(list(sentiments.values()), 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a651da",
   "metadata": {},
   "source": [
    "> We are now able to make the histogram with the distribution and the calculated values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982fdc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the calculated values\n",
    "print(f\"Calculated values:\")\n",
    "print(f\"mean: {mean_total}\")\n",
    "print(f\"10th percentile: {per_10th}\")\n",
    "print(f\"90th percentile: {per_90th}\")\n",
    "\n",
    "# Setting up figure with histogram\n",
    "result = plt.hist(list(sentiments.values()), bins=20, color='c', edgecolor='k', alpha=0.65)\n",
    "plt.axvline(per_10th, color='blue', linestyle='dashed', linewidth=1, label=\"10th percentile\")\n",
    "plt.axvline(per_90th, color='green', linestyle='dashed', linewidth=1, label=\"90th percentile\")\n",
    "plt.axvline(mean_total, color='orange', linestyle='dashed', linewidth=1, label=\"mean\")\n",
    "plt.xlabel('sentiment')\n",
    "plt.ylabel('counts')\n",
    "plt.figtext(.5,-0.05, f\"Histogram displaying the distribution of sentiments for all of the characters from the Marvel and DC universes. The dashed blue line is the 10th percentile, the dashed green line is the 90th percentile, and the dashed orange line is the mean.\", ha='center', fontsize=6)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971d88f9",
   "metadata": {},
   "source": [
    "> From the histogram and the calculated values it is clear to see that the overall sentiment is pretty low. We would have expected that the sentiment would be centered around 5, or maybe a bit lower, reflecting that the wikipedia articles strive to have a neutral language. However, given that most super hero comics are action oriented, it may be that they have more words with a lower happiness score in their articles from titles of issues, movies, and character names. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5840872c",
   "metadata": {},
   "source": [
    "**Now grab the good/bad character names (from this file, see Week 8 for details) and calculate the same stats for only good/bad characters; also plot the histograms for the good/bad group and indicate the values of the mean, etc.**\n",
    "\n",
    "> We start by creating lists of the good and the bad guys. Since there may be discrepancies between the characters in our graph and the provided file, we make sure to check for this when creating these lists, leaving out characters from our graph that are not represented in the given file and vice versa. Furthermore we have to take into account that some of the characters in the provided file are neither good nor bad, we have decided to take out these characters as well, since we would not be able to tell which group to count them in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfa117b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the given file\n",
    "with open('wiki_meta_data.csv', newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    data = list(reader)\n",
    "\n",
    "# Initialize lists to hold the names of the good and bad guys\n",
    "good_guys = []\n",
    "bad_guys = []\n",
    "characters = []\n",
    "alignments = []\n",
    "\n",
    "\n",
    "# Prepare a list of the characters in our graph\n",
    "characters_in_graph = list(G.nodes())\n",
    "  \n",
    "# For each character in the given file\n",
    "for info in data:\n",
    "    # Get the name using the same format as our nodes\n",
    "    name = info[1].replace(\" \", \"_\")\n",
    "    # Check if the character from the file is in our graph\n",
    "    if name in characters_in_graph:\n",
    "        # If the character is good\n",
    "        if info[6] == '1':\n",
    "            # Add that character name to the list of good guys\n",
    "            good_guys.append(name)\n",
    "            # Add the current alignment for that index in the alignment list\n",
    "            alignments.append(1)  \n",
    "            # Add the character to the list of characters\n",
    "            characters.append(name)\n",
    "        # If the character is bad\n",
    "        if info[7] == '1':\n",
    "            # Add that character name to the list of bad guys\n",
    "            bad_guys.append(name)\n",
    "            # Add the current alignment for that index in the alignment list\n",
    "            alignments.append(0)\n",
    "            # Add the character to the list of characters\n",
    "            characters.append(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad683950",
   "metadata": {},
   "source": [
    "> We can now go through our graph and grab the previously calculated sentiments for each node, and add it to a list of either sentiments for good or bad guys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd188d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init lists to hold sentiment values for good guys (gg) and bad guys (bg)\n",
    "gg_sent = []\n",
    "bg_sent = []\n",
    "\n",
    "# For each tuple consisting of a character name (node) and a sentiment value in the all_nodes list\n",
    "for char in characters:\n",
    "    # If the character is one of the good guys\n",
    "    if char in good_guys:\n",
    "        # Add its sentiment value to the list of sentiments for good guys\n",
    "        gg_sent.append(G.nodes[char][\"sentiment\"])\n",
    "    # If the character is one of the bad guys\n",
    "    if char in bad_guys:\n",
    "        # Add its sentiment value to the list of the sentiments for the bad guys\n",
    "        bg_sent.append(G.nodes[char][\"sentiment\"])  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1ded7f",
   "metadata": {},
   "source": [
    "> The mean, 10th and 90th percentiles can now be calculated for both good and bad guys, and plotted in histograms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86749b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram for good and bad guys\n",
    "gg_counts, gg_bins = np.histogram(gg_sent)\n",
    "bg_counts, bg_bins = np.histogram(bg_sent)\n",
    "\n",
    "# Setting same number of bins for both\n",
    "number_of_bins = 10 \n",
    "\n",
    "# Create two subplots and unpack the output array immediately\n",
    "fig, axs = plt.subplots(1, 2, sharex=True)\n",
    "fig.set_figwidth(15)\n",
    "\n",
    "# Histogram for good guys\n",
    "histogram = np.histogram(gg_sent, number_of_bins)\n",
    "\n",
    "# Calculate values\n",
    "mean_gg = np.mean(gg_sent) # Insert data\n",
    "per_10th_gg = np.percentile(gg_sent, 10) # Insert data\n",
    "per_90th_gg =  np.percentile(gg_sent, 90) # Insert data\n",
    "\n",
    "axs[0].hist(gg_sent, bins = 25, edgecolor='black')\n",
    "axs[0].set_title(f'Histogram of good guys sentiment')\n",
    "axs[0].set_xlabel('sentiment')\n",
    "axs[0].set_ylabel('counts')\n",
    "axs[0].axvline(per_10th_gg, color='blue', linestyle='dashed', linewidth=1, label=\"10th percentile\")\n",
    "axs[0].axvline(per_90th_gg, color='green', linestyle='dashed', linewidth=1, label=\"90th percentile\")\n",
    "axs[0].axvline(mean_gg, color='orange', linestyle='dashed', linewidth=1, label=\"mean\")\n",
    "axs[0].legend()\n",
    "\n",
    "# Histogram for bad guys\n",
    "histogram = np.histogram(bg_sent, number_of_bins)\n",
    "\n",
    "# Calculate values for plot\n",
    "mean_bg = np.mean(bg_sent) # Insert data\n",
    "per_10th_bg = np.percentile(bg_sent, 10) # Insert data\n",
    "per_90th_bg =  np.percentile(bg_sent, 90) # Insert data\n",
    "\n",
    "axs[1].hist(bg_sent, bins = 25, edgecolor='black')\n",
    "axs[1].set_title(f'Histogram of bad guys sentiment')\n",
    "axs[1].set_xlabel('sentiment')\n",
    "axs[1].set_ylabel('counts')\n",
    "axs[1].axvline(per_10th_bg, color='blue', linestyle='dashed', linewidth=1, label=\"10th percentile\")\n",
    "axs[1].axvline(per_90th_bg, color='green', linestyle='dashed', linewidth=1, label=\"90th percentile\")\n",
    "axs[1].axvline(mean_bg, color='orange', linestyle='dashed', linewidth=1, label=\"mean\")\n",
    "plt.figtext(.5,-0.05, f\"Left plot: Histogram displaying the distribution of sentiments for all of the good guys from both universes. The dashed blue line is the 10th percentile, the dashed green line is the 90th percentile, and the dashed orange line is the mean.\\nRight plot: Histogram displaying the distribution of sentiments for all of the bad guys from both universes. The dashed blue line is the 10th percentile, the dashed green line is the 90th percentile, and the dashed orange line is the mean.\", ha='center', fontsize=8)\n",
    "axs[1].legend()\n",
    "\n",
    "# Print the calculated values\n",
    "print(f\"Good guys:\\t\\t\\t\\tBad guys:\")\n",
    "print(f\"mean: {mean_gg}\\t\\tmean: {mean_bg}\")\n",
    "print(f\"10th percentile: {per_10th_gg}\\t10th percentile: {per_10th_bg}\")\n",
    "print(f\"90th percentile: {per_90th_gg}\\t90th percentile: {per_90th_bg}\")\n",
    "\n",
    "# Show both histrograms\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b650d7",
   "metadata": {},
   "source": [
    "> Again we see that overall the sentiments for both good and bad guys are pretty low. There seems to be a slight indication that bad guys actually have a higher sentiment than good guys, if we look at the calculated values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2bf523",
   "metadata": {},
   "source": [
    "**Finally use the label permutation test (see Week 5) to investigate the two hypotheses below**\n",
    "- **H1: Good characters have a higher averge value for sentiment than a similarly sized set of randomly selected characters.**\n",
    "- **H2: Bad characters have a lower average value for sentiment than a similarly sized set of randomly selected characters.**\n",
    "\n",
    "> For the label permutation test we are using two lists. `characters` contains the character names and `alignments` contains 1's and 0's, a 1 indicates good and 0 indicates bad. The indeces of lists corrspond to the same character, such that the i'th character in `characters` has the alignment of the i'th index of `alignments`. This allows us to just make a random sample in `alignments` to reshuffle the labels for each character, since there will always be the same fraction of good and bad labels, and the character names are kept at the same indeces in the `characters` list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b873d1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init lists to hold the values for each sample generated\n",
    "average_sentiment_samples_good_guys = []\n",
    "average_sentiment_samples_bad_guys = []\n",
    "\n",
    "# Create 1000 samples\n",
    "for n in range(1000):\n",
    "    # We want to start by shuffling who is bad and good.\n",
    "    alignments = random.sample(alignments, k=len(alignments))\n",
    "    \n",
    "    # We can then get the alignment of the nodes by finding their index in the characters list\n",
    "    # and then looking up their (shuffled) alignment in the alignments list\n",
    "    sent_bad = 0\n",
    "    sent_good = 0\n",
    "    \n",
    "    #rem_c = []\n",
    "    \n",
    "    # Since we are taking a sample in the size of all the characters we have\n",
    "    # we just go through all of the characters and check their sentiment and\n",
    "    # newly assigned alignment (good/bad)\n",
    "    for c in characters:\n",
    "        a = alignments[characters.index(c)]\n",
    "        if a == 1: # if its a good guy\n",
    "            sent_good += G.nodes[c][\"sentiment\"]\n",
    "        if a == 0: # if its a bad guy\n",
    "            sent_bad += G.nodes[c][\"sentiment\"]\n",
    "    \n",
    "    # Divide the sum by the number of good/bad guys (in this sample)\n",
    "    # Add this average to the average_sentiment_samples list\n",
    "    average_sentiment_samples_good_guys.append(sent_good / len(good_guys))\n",
    "    average_sentiment_samples_bad_guys.append(sent_bad / len(bad_guys))\n",
    "\n",
    "# Plot a histogram of the averages\n",
    "fig, axs = plt.subplots(1, 2, sharex=True, figsize=(15, 5))\n",
    "axs[0].hist(average_sentiment_samples_good_guys, bins = 10, edgecolor='black')\n",
    "axs[0].axvline(mean_gg, color='orange', linestyle='dashed', linewidth=1, label=\"mean\")\n",
    "axs[0].set_title('Result of label permutation for good guys')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].hist(average_sentiment_samples_bad_guys, bins = 10, edgecolor='black')\n",
    "axs[1].axvline(mean_bg, color='orange', linestyle='dashed', linewidth=1, label=\"mean\")\n",
    "axs[1].set_title('Result of label permutation for Bad guys')\n",
    "axs[1].legend()\n",
    "\n",
    "plt.figtext(.5,-0.05, f\"Left plot: Histogram displaying the distribution of sentiments from the label permutation for the good guys from both universes. The dashed orange line is the mean..\\nLeft plot: Histogram displaying the distribution of sentiments from the label permutation for the bad guys from both universes. The dashed orange line is the mean.\", ha='center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec7ffa8",
   "metadata": {},
   "source": [
    "**Write a short paragraph reflecting on your findings.**\n",
    "\n",
    "> Our findings seem to further support what we observed when we were just looking at the good and bad guys previously. Namely that the good guys have a lower sentiment than the average character, and the bad guys have a higher sentiment than the average character. So based on this we would have to reject both H1 and H2, which seems a bit off. While this may simply be a case of the data that we have used, we want to make an argument for this and a tiny investigation into this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256c1244",
   "metadata": {},
   "source": [
    "**Bonus paragraphs: Argument and investigation**\n",
    "\n",
    "> Argument: On one hand we suspect that this may not comepletely unlikely that the opposite of H1 and H2 is the case. Even though most heroes are more well documented with love affairs, families etc, they are also more likely to have a \"sad\" origin story, and have more well documented deaths of other characters around them. Since the heroes are usually the main characters they also appear in more stories, and as stated previously many of these stories may have titles containing \"sad\" words such as war, weapon, fight etc. On the other hand the villains may be less well documented, and appear in fewer comics. As such a villain that has its origin in an origin story love, fantasy, or protection of something, may get a higher sentiment, simply because it has fewer words, and a larger fraction of them are \"happy\". We are not comic book experts, so these are just some thoughts that might explain what we saw above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3c40ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are heroes more well documented than villains?\n",
    "gg_doc = []\n",
    "bg_doc = []\n",
    "\n",
    "for char in characters:\n",
    "    if char in good_guys:\n",
    "        gg_doc.append(G.nodes[char][\"txtlength\"])\n",
    "    if char in bad_guys:\n",
    "        bg_doc.append(G.nodes[char][\"txtlength\"])\n",
    "\n",
    "# Calculate mean, 10th perc, 90th perc for good guys\n",
    "mean_gg = np.mean(gg_doc) \n",
    "per_10th_gg = np.percentile(gg_doc, 10)\n",
    "per_90th_gg =  np.percentile(gg_doc, 90)\n",
    "\n",
    "# Calculate mean, 10th perc, 90th perc for bad guys\n",
    "mean_bg = np.mean(bg_doc) \n",
    "per_10th_bg = np.percentile(bg_doc, 10)\n",
    "per_90th_bg =  np.percentile(bg_doc, 90) \n",
    "\n",
    "# Print the calculated values\n",
    "print(f\"Good guys:\\t\\t\\tBad guys:\")\n",
    "print(f\"mean: {mean_gg}\\t\\tmean: {mean_bg}\")\n",
    "print(f\"10th percentile: {per_10th_gg}\\t\\t10th percentile: {per_10th_bg}\")\n",
    "print(f\"90th percentile: {per_90th_gg}\\t90th percentile: {per_90th_bg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a205b6b",
   "metadata": {},
   "source": [
    "> So the good guys are better documented than the bad guys, in that they have longer texts describing them. Do they also have a higher fraction of negative words? Lets say the average happyness value for neutral is 5, then we count how large a fraction of the words for each character are negative and how many are positive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554e2048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions\n",
    "def frac_sad(tokens):\n",
    "    # Total sentiment score of file\n",
    "    sent_sum = 0.0\n",
    "    # Total number of occurences of words\n",
    "    occ_sum = 0\n",
    "    \n",
    "    # For each token and associated number of occurences\n",
    "    for token, occ in tokens.items():\n",
    "        # If the token is in the given list of words with rated happiness\n",
    "        for word, score in sent_list:\n",
    "            if token == word and float(score) < 5:\n",
    "                sent_sum += occ\n",
    "        occ_sum += occ\n",
    "    return sent_sum / occ_sum\n",
    "\n",
    "def frac_hap(tokens):\n",
    "        # Total sentiment score of file\n",
    "    sent_sum = 0.0\n",
    "    # Total number of occurences of words\n",
    "    occ_sum = 0\n",
    "    \n",
    "    # For each token and associated number of occurences\n",
    "    for token, occ in tokens.items():\n",
    "        # If the token is in the given list of words with rated happiness\n",
    "        for word, score in sent_list:\n",
    "            if token == word and float(score) > 5:\n",
    "                sent_sum += occ\n",
    "        occ_sum += occ\n",
    "    return sent_sum / occ_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f7758f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gg_sad = []\n",
    "bg_sad = []\n",
    "gg_hap = []\n",
    "bg_hap = []\n",
    "uncommented = 0\n",
    "\n",
    "# The code below has been commented out, since it takes a while to execute,\n",
    "# the following 2 code cells are using the code that has been commented out\n",
    "# in this cell. Hence, if this code is not uncommented the other will not \n",
    "# work properly, but we have written about our findings.\n",
    "\n",
    "#for file in os.listdir(\"dc/\"):\n",
    "#    orig_character = file[:-4]\n",
    "#    fileopener = open(\"dc/\" + file, 'r')\n",
    "#    text = fileopener.read()  \n",
    "#    tokens = nltk.wordpunct_tokenize(BeautifulSoup(text, 'html.parser').get_text())\n",
    "#    file_text = [w.lower() for w in tokens if w.isalpha()]\n",
    "#    fdist = FreqDist(file_text)\n",
    "#    if orig_character in good_guys:\n",
    "#        gg_sad.append(frac_sad(fdist))\n",
    "#        gg_hap.append(frac_hap(fdist))\n",
    "#    if orig_character in bad_guys:\n",
    "#        bg_sad.append(frac_sad(fdist))\n",
    "#        bg_hap.append(frac_hap(fdist))\n",
    "#       \n",
    "#for file in os.listdir(\"marvel/\"):\n",
    "#    orig_character = file[:-4]\n",
    "#    fileopener = open(\"marvel/\" + file, 'r')\n",
    "#    text = fileopener.read()  \n",
    "#    tokens = nltk.wordpunct_tokenize(BeautifulSoup(text, 'html.parser').get_text())\n",
    "#    file_text = [w.lower() for w in tokens if w.isalpha()]\n",
    "#    fdist = FreqDist(file_text)\n",
    "#    if orig_character in good_guys:\n",
    "#        gg_sad.append(frac_sad(fdist))\n",
    "#        gg_hap.append(frac_hap(fdist))\n",
    "#    if orig_character in bad_guys:\n",
    "#        bg_sad.append(frac_sad(fdist))\n",
    "#        bg_hap.append(frac_hap(fdist))\n",
    "#\n",
    "#uncommented = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb74444",
   "metadata": {},
   "outputs": [],
   "source": [
    "if uncommented == 1:\n",
    "    # Calculate mean, 10th perc, 90th perc for good guys\n",
    "    mean_gg = np.mean(gg_sad) \n",
    "    per_10th_gg = np.percentile(gg_sad, 10)\n",
    "    per_90th_gg =  np.percentile(gg_sad, 90)\n",
    "\n",
    "    # Calculate mean, 10th perc, 90th perc for bad guys\n",
    "    mean_bg = np.mean(bg_sad) \n",
    "    per_10th_bg = np.percentile(bg_sad, 10)\n",
    "    per_90th_bg =  np.percentile(bg_sad, 90) \n",
    "\n",
    "    # Print the calculated values\n",
    "    print(f\"Good guys:\\t\\t\\t\\tBad guys:\")\n",
    "    print(f\"mean: {mean_gg}\\t\\tmean: {mean_bg}\")\n",
    "    print(f\"10th percentile: {per_10th_gg}\\t10th percentile: {per_10th_bg}\")\n",
    "    print(f\"90th percentile: {per_90th_gg}\\t90th percentile: {per_90th_bg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dbe909",
   "metadata": {},
   "outputs": [],
   "source": [
    "if uncommented == 1:\n",
    "    # Calculate mean, 10th perc, 90th perc for good guys\n",
    "    mean_gg = np.mean(gg_hap) \n",
    "    per_10th_gg = np.percentile(gg_hap, 10)\n",
    "    per_90th_gg =  np.percentile(gg_hap, 90)\n",
    "\n",
    "    # Calculate mean, 10th perc, 90th perc for bad guys\n",
    "    mean_bg = np.mean(bg_hap) \n",
    "    per_10th_bg = np.percentile(bg_hap, 10)\n",
    "    per_90th_bg =  np.percentile(bg_hap, 90) \n",
    "\n",
    "    # Print the calculated values\n",
    "    print(f\"Good guys:\\t\\t\\t\\tBad guys:\")\n",
    "    print(f\"mean: {mean_gg}\\t\\tmean: {mean_bg}\")\n",
    "    print(f\"10th percentile: {per_10th_gg}\\t10th percentile: {per_10th_bg}\")\n",
    "    print(f\"90th percentile: {per_90th_gg}\\t90th percentile: {per_90th_bg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b288bc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lastly we just want to check how many characters are in the two lists\n",
    "print(f\"Number of good guys = {len(good_guys)}\")\n",
    "print(f\"Number of bad guys = {len(bad_guys)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd448e51",
   "metadata": {},
   "source": [
    "> We found that the the good guys had a lower fraction of \"sad\" words and a higher fraction of \"happy\" words. Which conincides with the original H1 and H2. Hence our idea for an alternative hapythesis may be incorrect. We also considered that it might have been caused by us having a much lower number of characters in one of the groups, but as can be seen above, this is not the case. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
