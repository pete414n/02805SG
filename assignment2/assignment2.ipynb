{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c016668",
   "metadata": {},
   "source": [
    "# Preface\n",
    "> The questions/exercises given for the assignment has been repeated in this notebook, they have been put in bold. Our answers and explanations have been indented, or printed from the executed code, to easily distinguish them from the exercise text. Throughout the text we are using good guys and heroes synonymously and bad guys and villains synonymously. We also use the word alignment(s) to refer to good and bad collectively.\n",
    "\n",
    "## Imports\n",
    "> The following imports are needed throughout the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d7d83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import nltk\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import re\n",
    "import string\n",
    "import statistics\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "import json\n",
    "import urllib.request\n",
    "import powerlaw\n",
    "\n",
    "from nltk import word_tokenize, FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "from community import community_louvain\n",
    "from collections import Counter\n",
    "from os import listdir\n",
    "from bs4 import BeautifulSoup\n",
    "from fa2 import ForceAtlas2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3682d81",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "> The downloaded files can be found in the folders `dc` and `marvel`, in our [github project](https://github.com/pete414n/02805SG/tree/main/assignment2)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5442aec6",
   "metadata": {},
   "source": [
    "# Basic Stats\n",
    "\n",
    "**Write a short paragraph describing the network. The paragraph should contain the following information**\n",
    "- **The number of nodes and links.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70abdf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Edgelist saved as gml so we can keep the node attributes\n",
    "G = nx.read_gml(\"superhero.edgelist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e72e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of nodes\n",
    "num_nodes = len(G.nodes())\n",
    "print(f\"Number of nodes = {num_nodes}\")\n",
    "\n",
    "# Number of links\n",
    "num_links = len(G.edges()) # Assuming we get an edge list\n",
    "print(f\"Number of links = {num_links}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53ee3ec",
   "metadata": {},
   "source": [
    "- **The average, median, mode, minimum and maximum value of the network's in-degree.s And of the out-degrees.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a8b4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a sorted list of the in degrees and a list of the out degrees\n",
    "in_degree_list = sorted([d for (c, d) in G.in_degree], reverse=True)\n",
    "out_degree_list = sorted([d for (c, d) in G.out_degree], reverse=True)\n",
    "\n",
    "# Average in degree\n",
    "print(f\"average in degree = {np.mean(in_degree_list)}\")\n",
    "\n",
    "# Average out degree\n",
    "print(f\"average out degree = {np.mean(out_degree_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7a2915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Median in degree\n",
    "print(f\"median in degree = {np.median(in_degree_list)}\")\n",
    "\n",
    "# Median out degree\n",
    "print(f\"median out degree = {np.median(out_degree_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c8557f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mode in degree\n",
    "mode = max(set(in_degree_list), key=in_degree_list.count)\n",
    "print(f\"mode in degree = {mode}\")\n",
    "\n",
    "# Mode out degree\n",
    "mode = max(set(out_degree_list), key=out_degree_list.count)\n",
    "print(f\"mode out degree = {mode}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48a725b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we have the sorted list, the first element will be the max and the last will be the min\n",
    "# Minimum and maximum in degree\n",
    "min_in_degree = in_degree_list[-1]\n",
    "max_in_degree = in_degree_list[0]\n",
    "print(f\"minimum in degree = {min_in_degree}\\nmaximum in degree = {max_in_degree}\\n\")\n",
    "\n",
    "# Minimum and maximum out degree\n",
    "min_out_degree = out_degree_list[-1]\n",
    "max_out_degree = out_degree_list[0]\n",
    "print(f\"minimum out degree = {min_out_degree}\\nmaximum out degree = {max_out_degree}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebb4f20",
   "metadata": {},
   "source": [
    "> When creating the network we have tried to get the wikitext as clean as possible and only have the true text without i.e. the contents list. \n",
    "We have created the network so the nodes conatins 3 attributes, universe, txtlength and sentiment. The first one tells whether it's a DC or Marvel character, the next gives the length of the whole wikitext and the last gives the sentiment value of the wikitext. \n",
    "We have chosen not to include all the characters with pages that were redirects. We have done this by checking if the text belonging to the given character were starting with #redirect, if so we didn't include this node in the network. We have also removed all nodes who had a sentiment value of 0. Then we have removed isolates from the network and found the largest connected component. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6d4401",
   "metadata": {},
   "source": [
    "> Our network has 1602 nodes and 19068. The average in- and out-degree are both 11.75..., they are the same since each edge contributes one to the total out-degree and one to the total in-degree, and a difference would indicate that there somehow where a higher total in-degree than a total out-degree, or vice versa. The median in degree is 5, and the median out degree is 9. The mode for in degree is 0, which corresponds to characters that link to other characters, but are not linked to by anyone else. The mode for out degree is 2. These two numbers tells us that the most common in- and out-degree is relatively low, considering the total number of nodes. The minimum in- and out-degree are both 0, an out-degree of 0 corresponds to a character that links to no other characters, but is linked to by at least one other character. The maximum in-degree is 429, this is the character that most other characters are linking to, corresponding to roughly 1/4 of all of our nodes linking to this single node. The maximum out degree is 68, which corresponds to the character that has the most links to other characters. Overall these numbers indicates to us that the graph is not very dense, since it looks like we (might) have quite a few nodes that do not have many edges going in or out of them. At the same time this also indicates that we must have some nodes that have many links, relatively to the others. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6a7051",
   "metadata": {},
   "source": [
    "**We also want the degree distributions and a plot of the network**\n",
    "- **Create and visualize in- and out-going degree distributions as described in Lecture 4. Think about which axes you should use - loglog is great for power-law distributions, but not necessarily for Poisson-like degree distributions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8f0140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the degree distirbution for in and out degress\n",
    "in_counts, in_bins = np.histogram(in_degree_list, bins=max_in_degree)\n",
    "out_counts, out_bins = np.histogram(out_degree_list, bins=max_out_degree)\n",
    "\n",
    "#Plotting the in-degree distribution with log-log axes \n",
    "#as the in-degree distribution is much like a power-law distribution\n",
    "plt.plot(in_bins[:-1], in_counts, color = 'green')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('Number of Nodes')\n",
    "plt.title('In-degree distributions')\n",
    "plt.show()\n",
    "\n",
    "#Plotting the out-degree distribution with normal axes \n",
    "#as the out-degree distribution is much like a Poisson distribution\n",
    "plt.plot(in_bins[:-1], in_counts, color = 'green')\n",
    "plt.plot(out_bins[:-1], out_counts, color = 'red')\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('Number of Nodes')\n",
    "plt.title('Out-degree distributions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b66195",
   "metadata": {},
   "source": [
    "- **Estimate, report, and reflect on the slope and starting value of the incoming degree distribtion's power law using the tools described in Lecture 5 - and display the fit on top of your incoming degree-distribution plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454feffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "power_law = powerlaw.Fit(in_degree_list)\n",
    "print(power_law.alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6265ba5c",
   "metadata": {},
   "source": [
    "> From the value of alpha we can see that the in-degrees belong to a power-law distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1760e819",
   "metadata": {},
   "source": [
    "- **Plot the network using the Force Atlas algorithm as described in Lecture 5.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f30af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to contain nodes belonging to DC and Marvel\n",
    "dc_nodes = []\n",
    "marvel_nodes = []\n",
    "\n",
    "#Lists to contain the degrees for the DC nodes and the Marvel nodes. To be used when plotting using the Force Atlas\n",
    "dc_degrees = []\n",
    "marvel_degrees = []\n",
    "\n",
    "# Dividing nodes in Marvel and DC universe\n",
    "for n in uG.nodes():\n",
    "    if uG.nodes[n]['universe'] == 'Marvel':\n",
    "        marvel_nodes += [n]    \n",
    "        marvel_degrees += [uG.degree[n]]\n",
    "    else:\n",
    "        dc_nodes += [n]\n",
    "        dc_degrees += [uG.degree[n]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f492c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "forceatlas2 = ForceAtlas2(\n",
    "                        # Behavior alternatives\n",
    "                        outboundAttractionDistribution=True,  # Dissuade hubs\n",
    "                        linLogMode=False,  # NOT IMPLEMENTED\n",
    "                        adjustSizes=False,  # Prevent overlap (NOT IMPLEMENTED)\n",
    "                        edgeWeightInfluence=1.0,\n",
    "\n",
    "                        # Performance\n",
    "                        jitterTolerance=1.0,  # Tolerance\n",
    "                        barnesHutOptimize=True,\n",
    "                        barnesHutTheta=1.2,\n",
    "                        multiThreaded=False,  # NOT IMPLEMENTED\n",
    "\n",
    "                        # Tuning\n",
    "                        scalingRatio=2.0,\n",
    "                        strongGravityMode=False,\n",
    "                        gravity=0.5,\n",
    "\n",
    "                        # Log\n",
    "                        verbose=True)\n",
    "\n",
    "positions = forceatlas2.forceatlas2_networkx_layout(G, pos=None, iterations=2000)\n",
    "nx.draw_networkx_nodes(G, positions, nodelist = marvel_nodes, node_size=marvel_degrees, node_color=\"blue\", alpha=0.4)\n",
    "nx.draw_networkx_nodes(G, positions, nodelist = dc_nodes, node_size=dc_degrees, node_color=\"red\", alpha=0.4)\n",
    "nx.draw_networkx_edges(G, positions, edge_color=\"green\", alpha=0.05)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9079b3f8",
   "metadata": {},
   "source": [
    "# Communities\n",
    "\n",
    "**Identify the communities in one or both of the superhero universes (DC/Marvel) as described in Week 7.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d06d328",
   "metadata": {},
   "source": [
    "> We will be focusing on the DC superhero universe. We start by recreating our graph for just the DC characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34964671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreating the network H, for only the dc universe\n",
    "dc_files = os.listdir('dc')\n",
    "H = nx.DiGraph()\n",
    "\n",
    "dc_nodes = []\n",
    "marvel_nodes = []\n",
    "\n",
    "# For week 5 we are adding the lengths of the wikitext for each character\n",
    "for dc_char in dc_files:\n",
    "  H.add_node(dc_char[:-4], uni=\"dc\", length=0)\n",
    "\n",
    "# Adding all the dc nodes and edges\n",
    "for source in dc_files:\n",
    "  # Remove .txt ending for character name\n",
    "  name = source[0:len(source)-4]\n",
    "\n",
    "  # Get the wikitext saved for this character\n",
    "  with open('dc/' + source) as f:\n",
    "    lines = f.read()\n",
    "\n",
    "  # Find all the links in this wikitext\n",
    "  matches = re.findall(r'[[]{2}.*?[]]{2}', lines)\n",
    "\n",
    "  # Count the number of words in this wikitext and set that attribute\n",
    "  word_count = len(re.findall(r'\\w+', lines))\n",
    "  H.nodes[name]['length'] = word_count\n",
    "\n",
    "  # Construct possible file name:\n",
    "  links = []\n",
    "  for match in matches: \n",
    "    link = match[2:len(match)-2]\n",
    "    link = link.split(\"|\")[0]\n",
    "    link = re.sub(r'\\s', '_', link) \n",
    "    #link.replace(\"Ã±\", \"n\")\n",
    "    link.replace(\"/\", \"-\")\n",
    "    links.append(link)\n",
    "\n",
    "  # Remove duplicates\n",
    "  links = [*set(links)]\n",
    "\n",
    "  # Add an edge for each link that can be found in either fold\n",
    "  for link in links:\n",
    "    if os.path.isfile('dc/' + link + \".txt\") or os.path.isfile('marvel/' + link + \".txt\"):\n",
    "      H.add_edge(name, link)\n",
    "\n",
    "largest_cc = max(nx.weakly_connected_components(H), key=len)\n",
    "gcc = nx.subgraph(H, largest_cc)\n",
    "uH = gcc.to_undirected(reciprocal=False, as_view=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f08801c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the DC network\n",
    "dc_G = uG.subgraph(dc_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c008b257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example from: https://perso.crans.org/aynaud/communities/\n",
    "#first compute the best partition\n",
    "partition = community_louvain.best_partition(dc_G)\n",
    "\n",
    "\n",
    "partition\n",
    "# Some code to find the number of communities and the number of nodes in each, needed for next question\n",
    "\n",
    "#set(partition.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b8f89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Not staying in assignment, just to see size of communitites and which colors they are getting\n",
    "for com in set(partition.values()) :\n",
    " #   count += 1\n",
    "    #print(\"com: \" + str(com))\n",
    "    #print(\"count: \" + str(count))\n",
    "    list_nodes = [nodes for nodes in partition.keys() if partition[nodes] == com]\n",
    "    print(len(list_nodes))\n",
    "cmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94983cf0",
   "metadata": {},
   "source": [
    "**Visualize the communities by coloring the graph's nodes according to community affiliation - also as described in Week 7.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a9d857",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drawing using ForceAtlas\n",
    "forceatlas2 = ForceAtlas2(\n",
    "                        # Behavior alternatives\n",
    "                        outboundAttractionDistribution=True,  # Dissuade hubs\n",
    "                        linLogMode=False,  # NOT IMPLEMENTED\n",
    "                        adjustSizes=False,  # Prevent overlap (NOT IMPLEMENTED)\n",
    "                        edgeWeightInfluence=1,\n",
    "\n",
    "                        # Performance\n",
    "                        jitterTolerance=5.0,  # Tolerance\n",
    "                        barnesHutOptimize=True,\n",
    "                        barnesHutTheta=0.5,\n",
    "                        multiThreaded=False,  # NOT IMPLEMENTED\n",
    "\n",
    "                        # Tuning\n",
    "                        scalingRatio=3.0,\n",
    "                        strongGravityMode=True,\n",
    "                        gravity=0.5,\n",
    "\n",
    "                        # Log\n",
    "                        verbose=True)\n",
    "\n",
    "positions = forceatlas2.forceatlas2_networkx_layout(dc_G, pos=None, iterations=2000)\n",
    "cmap = plt.get_cmap('plasma', max(partition.values()) + 1)\n",
    "\n",
    "nx.draw_networkx_nodes(dc_G, positions, partition.keys(), node_size=40,\n",
    "                        cmap=cmap, node_color=list(partition.values()))\n",
    "nx.draw_networkx_edges(dc_G, positions, edge_color='green', alpha=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bd0dea",
   "metadata": {},
   "source": [
    "**Write a paragraph about your work to identify the communities. The paragraph should include**\n",
    "- **Information about the algorithm you used to find communities.**\n",
    "- **The value of modularity for your network.**\n",
    "- **The number of communities and their sizes (in terms of number of nodes).**\n",
    "\n",
    "> This is just a placeholder for the paragraph. Something about the Louvain algorithm. Something about modularity. Something about the number of communities and their sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dcf2d4",
   "metadata": {},
   "source": [
    "**For the 10 largest communities, process the text in order to create a TF-IDF vector for each community. Explain in your own words how TF-IDF works.**\n",
    "\n",
    "> This is just a placeholder for the paragraph explaining how TF-IDF works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03ae420",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3a21ef0",
   "metadata": {},
   "source": [
    "**Then visualize the TF-IDF vectors using wordclouds (as described in Week 7). Remember to comment on your word-clouds (e.g. in the figure captions): Do the wordclouds/TF-IDF lists enable you to understand the communities you have found (or is it just gibberish)? Justify your answer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232fac98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "200e91a4",
   "metadata": {},
   "source": [
    "> This is just a placeholder for the paragraph for our answer and the justification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4ebec0",
   "metadata": {},
   "source": [
    "# Sentiment\n",
    "\n",
    "**First calculate the sentiment for all character pages. Calculate the mean value of the sentiment and the 10th and 90th percentile values for the sentiment. Then create a nice histogram displaying the distribution of sentiment values for all pages. Indicate the mean, etc on that histogram.**\n",
    "\n",
    "> The following code shows how we have calculated and found sentiments for the different files, but since we had to store the graph as an edgelist, this had to be done before we got to this part of the assignment.\n",
    "We start by preparing the data from the files that have been given for the exercise. First we make a list of all the words from the given sentiment list with their associated average happiness score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f563ac13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store tuples of words and their \n",
    "# average happiness score\n",
    "sent_list = []\n",
    "\n",
    "# Read in the .tsv file\n",
    "with open(\"dataset_s1.tsv\", \"r\", encoding=\"utf8\") as sent_file:\n",
    "    tsv_reader = csv.DictReader(sent_file, delimiter=\"\\t\")\n",
    "    # For each sentiment in the file, save the word and average happiness in a tuple\n",
    "    # and add it to the list\n",
    "    for sent in tsv_reader:\n",
    "        word = sent[\"word\"]\n",
    "        average = sent[\"happiness_average\"]\n",
    "        sent_list.append((word, average))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9850a1e2",
   "metadata": {},
   "source": [
    "> Then we create a function to calculate the sentiment from a frequency distribution for a given file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058fe25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating the sentiment of a file from the frequency distribution for that file\n",
    "def sentiment(tokens):\n",
    "    # Total sentiment score of file\n",
    "    sent_sum = 0.0\n",
    "    # Total number of occurences of words\n",
    "    occ_sum = 0\n",
    "    \n",
    "    # For each token and associated number of occurences\n",
    "    for token, occ in tokens.items():\n",
    "        # If the token is in the given list of words with rated happiness\n",
    "        for word, score in sent_list:\n",
    "            if token == word:\n",
    "                sent_sum += (float(score) * occ)\n",
    "                occ_sum += occ\n",
    "    return sent_sum / occ_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2a0495",
   "metadata": {},
   "source": [
    "> We have added an attribute sentiment to the nodes, and as the other attributes were set for each node, the sentiment for that node was calculated using the above `sentiment`function. We then create a list of tuples, where each tuple contains a character name and the associated sentiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1cfa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = nx.get_node_attributes(G, \"sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7042dced",
   "metadata": {},
   "source": [
    "> We are now ready to calculate the mean, 10th percentile, and the 90th percentile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7247cfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source: https://stackoverflow.com/questions/2374640/how-do-i-calculate-percentiles-with-python-numpy\n",
    "# mean\n",
    "mean_total = np.mean(list(sentiments.values()))\n",
    "\n",
    "# 10th percentile\n",
    "per_10th = np.percentile(list(sentiments.values()), 10)\n",
    "\n",
    "# 90 percentile sentiment \n",
    "per_90th =  np.percentile(list(sentiments.values()), 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a651da",
   "metadata": {},
   "source": [
    "> We are now able to make the histogram with the distribution and the calculated values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982fdc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the calculated values\n",
    "print(f\"Calculated values:\")\n",
    "print(f\"mean: {mean_total}\")\n",
    "print(f\"10th percentile: {per_10th}\")\n",
    "print(f\"90th percentile: {per_90th}\")\n",
    "\n",
    "# Setting up figure with histogram\n",
    "result = plt.hist(list(sentiments.values()), bins=20, color='c', edgecolor='k', alpha=0.65)\n",
    "plt.axvline(per_10th, color='blue', linestyle='dashed', linewidth=1, label=\"10th percentile\")\n",
    "plt.axvline(per_90th, color='green', linestyle='dashed', linewidth=1, label=\"90th percentile\")\n",
    "plt.axvline(mean_total, color='orange', linestyle='dashed', linewidth=1, label=\"mean\")\n",
    "plt.xlabel('sentiment')\n",
    "plt.ylabel('counts')\n",
    "plt.figtext(.5,-0.05, f\"Histogram displaying the distribution of sentiments for all of the characters from the Marvel and DC universes. The dashed blue line is the 10th percentile, the dashed green line is the 90th percentile, and the dashed orange line is the mean.\", ha='center', fontsize=6)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971d88f9",
   "metadata": {},
   "source": [
    "> From the histogram and the calculated values it is clear to see that the overall sentiment is pretty low. We would have expected that the sentiment would be centered around 5, or maybe a bit lower, reflecting that the wikipedia articles strive to have a neutral language. However, given that most super hero comics are action oriented, it may be that they have more words with a lower happiness score in their articles from titles of issues, movies, and character names. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5840872c",
   "metadata": {},
   "source": [
    "**Now grab the good/bad character names (from this file, see Week 8 for details) and calculate the same stats for only good/bad characters; also plot the histograms for the good/bad group and indicate the values of the mean, etc.**\n",
    "\n",
    "> We start by creating lists of the good and the bad guys. Since there may be discrepancies between the characters in our graph and the provided file, we make sure to check for this when creating these lists, leaving out characters from our graph that are not represented in the given file and vice versa. Furthermore we have to take into account that some of the characters in the provided file are neither good nor bad, we have decided to take out these characters as well, since we would not be able to tell which group to count them in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfa117b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the given file\n",
    "with open('wiki_meta_data.csv', newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    data = list(reader)\n",
    "\n",
    "# Initialize lists to hold the names of the good and bad guys\n",
    "good_guys = []\n",
    "bad_guys = []\n",
    "characters = []\n",
    "alignments = []\n",
    "\n",
    "\n",
    "# Prepare a list of the characters in our graph\n",
    "characters_in_graph = list(G.nodes())\n",
    "  \n",
    "# For each character in the given file\n",
    "for info in data:\n",
    "    # Get the name using the same format as our nodes\n",
    "    name = info[1].replace(\" \", \"_\")\n",
    "    # Check if the character from the file is in our graph\n",
    "    if name in characters_in_graph:\n",
    "        # If the character is good\n",
    "        if info[6] == '1':\n",
    "            # Add that character name to the list of good guys\n",
    "            good_guys.append(name)\n",
    "            # Add the current alignment for that index in the alignment list\n",
    "            alignments.append(1)  \n",
    "            # Add the character to the list of characters\n",
    "            characters.append(name)\n",
    "        # If the character is bad\n",
    "        if info[7] == '1':\n",
    "            # Add that character name to the list of bad guys\n",
    "            bad_guys.append(name)\n",
    "            # Add the current alignment for that index in the alignment list\n",
    "            alignments.append(0)\n",
    "            # Add the character to the list of characters\n",
    "            characters.append(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad683950",
   "metadata": {},
   "source": [
    "> We can now go through our graph and grab the previously calculated sentiments for each node, and add it to a list of either sentiments for good or bad guys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd188d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init lists to hold sentiment values for good guys (gg) and bad guys (bg)\n",
    "gg_sent = []\n",
    "bg_sent = []\n",
    "\n",
    "# For each tuple consisting of a character name (node) and a sentiment value in the all_nodes list\n",
    "for char in characters:\n",
    "    # If the character is one of the good guys\n",
    "    if char in good_guys:\n",
    "        # Add its sentiment value to the list of sentiments for good guys\n",
    "        gg_sent.append(G.nodes[char][\"sentiment\"])\n",
    "    # If the character is one of the bad guys\n",
    "    if char in bad_guys:\n",
    "        # Add its sentiment value to the list of the sentiments for the bad guys\n",
    "        bg_sent.append(G.nodes[char][\"sentiment\"])  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1ded7f",
   "metadata": {},
   "source": [
    "> The mean, 10th and 90th percentiles can now be calculated for both good and bad guys, and plotted in histograms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86749b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram for good and bad guys\n",
    "gg_counts, gg_bins = np.histogram(gg_sent)\n",
    "bg_counts, bg_bins = np.histogram(bg_sent)\n",
    "\n",
    "# Setting same number of bins for both\n",
    "number_of_bins = 10 \n",
    "\n",
    "# Create two subplots and unpack the output array immediately\n",
    "fig, axs = plt.subplots(1, 2, sharex=True)\n",
    "fig.set_figwidth(15)\n",
    "\n",
    "# Histogram for good guys\n",
    "histogram = np.histogram(gg_sent, number_of_bins)\n",
    "\n",
    "# Calculate values\n",
    "mean_gg = np.mean(gg_sent) # Insert data\n",
    "per_10th_gg = np.percentile(gg_sent, 10) # Insert data\n",
    "per_90th_gg =  np.percentile(gg_sent, 90) # Insert data\n",
    "\n",
    "axs[0].hist(gg_sent, bins = 25, edgecolor='black')\n",
    "axs[0].set_title(f'Histogram of good guys sentiment')\n",
    "axs[0].set_xlabel('sentiment')\n",
    "axs[0].set_ylabel('counts')\n",
    "axs[0].axvline(per_10th_gg, color='blue', linestyle='dashed', linewidth=1, label=\"10th percentile\")\n",
    "axs[0].axvline(per_90th_gg, color='green', linestyle='dashed', linewidth=1, label=\"90th percentile\")\n",
    "axs[0].axvline(mean_gg, color='orange', linestyle='dashed', linewidth=1, label=\"mean\")\n",
    "axs[0].legend()\n",
    "\n",
    "# Histogram for bad guys\n",
    "histogram = np.histogram(bg_sent, number_of_bins)\n",
    "\n",
    "# Calculate values for plot\n",
    "mean_bg = np.mean(bg_sent) # Insert data\n",
    "per_10th_bg = np.percentile(bg_sent, 10) # Insert data\n",
    "per_90th_bg =  np.percentile(bg_sent, 90) # Insert data\n",
    "\n",
    "axs[1].hist(bg_sent, bins = 25, edgecolor='black')\n",
    "axs[1].set_title(f'Histogram of bad guys sentiment')\n",
    "axs[1].set_xlabel('sentiment')\n",
    "axs[1].set_ylabel('counts')\n",
    "axs[1].axvline(per_10th_bg, color='blue', linestyle='dashed', linewidth=1, label=\"10th percentile\")\n",
    "axs[1].axvline(per_90th_bg, color='green', linestyle='dashed', linewidth=1, label=\"90th percentile\")\n",
    "axs[1].axvline(mean_bg, color='orange', linestyle='dashed', linewidth=1, label=\"mean\")\n",
    "plt.figtext(.5,-0.05, f\"Left plot: Histogram displaying the distribution of sentiments for all of the good guys from both universes. The dashed blue line is the 10th percentile, the dashed green line is the 90th percentile, and the dashed orange line is the mean.\\nRight plot: Histogram displaying the distribution of sentiments for all of the bad guys from both universes. The dashed blue line is the 10th percentile, the dashed green line is the 90th percentile, and the dashed orange line is the mean.\", ha='center', fontsize=8)\n",
    "axs[1].legend()\n",
    "\n",
    "# Print the calculated values\n",
    "print(f\"Good guys:\\t\\t\\t\\tBad guys:\")\n",
    "print(f\"mean: {mean_gg}\\t\\tmean: {mean_bg}\")\n",
    "print(f\"10th percentile: {per_10th_gg}\\t10th percentile: {per_10th_bg}\")\n",
    "print(f\"90th percentile: {per_90th_gg}\\t90th percentile: {per_90th_bg}\")\n",
    "\n",
    "# Show both histrograms\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b650d7",
   "metadata": {},
   "source": [
    "> Again we see that overall the sentiments for both good and bad guys are pretty low. There seems to be a slight indication that bad guys actually have a higher sentiment than good guys, if we look at the calculated values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2bf523",
   "metadata": {},
   "source": [
    "**Finally use the label permutation test (see Week 5) to investigate the two hypotheses below**\n",
    "- **H1: Good characters have a higher averge value for sentiment than a similarly sized set of randomly selected characters.**\n",
    "- **H2: Bad characters have a lower average value for sentiment than a similarly sized set of randomly selected characters.**\n",
    "\n",
    "> For the label permutation test we are using two lists. `characters` contains the character names and `alignments` contains 1's and 0's, a 1 indicates good and 0 indicates bad. The indeces of lists corrspond to the same character, such that the i'th character in `characters` has the alignment of the i'th index of `alignments`. This allows us to just make a random sample in `alignments` to reshuffle the labels for each character, since there will always be the same fraction of good and bad labels, and the character names are kept at the same indeces in the `characters` list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b873d1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init lists to hold the values for each sample generated\n",
    "average_sentiment_samples_good_guys = []\n",
    "average_sentiment_samples_bad_guys = []\n",
    "\n",
    "# Create 1000 samples\n",
    "for n in range(1000):\n",
    "    # We want to start by shuffling who is bad and good.\n",
    "    alignments = random.sample(alignments, k=len(alignments))\n",
    "    \n",
    "    # We can then get the alignment of the nodes by finding their index in the characters list\n",
    "    # and then looking up their (shuffled) alignment in the alignments list\n",
    "    sent_bad = 0\n",
    "    sent_good = 0\n",
    "    \n",
    "    #rem_c = []\n",
    "    \n",
    "    # Since we are taking a sample in the size of all the characters we have\n",
    "    # we just go through all of the characters and check their sentiment and\n",
    "    # newly assigned alignment (good/bad)\n",
    "    for c in characters:\n",
    "        a = alignments[characters.index(c)]\n",
    "        if a == 1: # if its a good guy\n",
    "            sent_good += G.nodes[c][\"sentiment\"]\n",
    "        if a == 0: # if its a bad guy\n",
    "            sent_bad += G.nodes[c][\"sentiment\"]\n",
    "    \n",
    "    # Divide the sum by the number of good/bad guys (in this sample)\n",
    "    # Add this average to the average_sentiment_samples list\n",
    "    average_sentiment_samples_good_guys.append(sent_good / len(good_guys))\n",
    "    average_sentiment_samples_bad_guys.append(sent_bad / len(bad_guys))\n",
    "\n",
    "# Plot a histogram of the averages\n",
    "fig, axs = plt.subplots(1, 2, sharex=True, figsize=(15, 5))\n",
    "axs[0].hist(average_sentiment_samples_good_guys, bins = 10, edgecolor='black')\n",
    "axs[0].axvline(mean_gg, color='orange', linestyle='dashed', linewidth=1, label=\"mean\")\n",
    "axs[0].set_title('Result of label permutation for good guys')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].hist(average_sentiment_samples_bad_guys, bins = 10, edgecolor='black')\n",
    "axs[1].axvline(mean_bg, color='orange', linestyle='dashed', linewidth=1, label=\"mean\")\n",
    "axs[1].set_title('Result of label permutation for Bad guys')\n",
    "axs[1].legend()\n",
    "\n",
    "plt.figtext(.5,-0.05, f\"Left plot: Histogram displaying the distribution of sentiments from the label permutation for the good guys from both universes. The dashed orange line is the mean..\\nLeft plot: Histogram displaying the distribution of sentiments from the label permutation for the bad guys from both universes. The dashed orange line is the mean.\", ha='center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec7ffa8",
   "metadata": {},
   "source": [
    "**Write a short paragraph reflecting on your findings.**\n",
    "\n",
    "> Our findings seem to further support what we observed when we were just looking at the good and bad guys previously. Namely that the good guys have a lower sentiment than the average character, and the bad guys have a higher sentiment than the average character. So based on this we would have to reject both H1 and H2, which seems a bit off. While this may simply be a case of the data that we have used, we want to make an argument for this and a tiny investigation into this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256c1244",
   "metadata": {},
   "source": [
    "**Bonus paragraphs: Argument and investigation**\n",
    "\n",
    "> Argument: On one hand we suspect that this may not comepletely unlikely that the opposite of H1 and H2 is the case. Even though most heroes are more well documented with love affairs, families etc, they are also more likely to have a \"sad\" origin story, and have more well documented deaths of other characters around them. Since the heroes are usually the main characters they also appear in more stories, and as stated previously many of these stories may have titles containing \"sad\" words such as war, weapon, fight etc. On the other hand the villains may be less well documented, and appear in fewer comics. As such a villain that has its origin in an origin story love, fantasy, or protection of something, may get a higher sentiment, simply because it has fewer words, and a larger fraction of them are \"happy\". We are not comic book experts, so these are just some thoughts that might explain what we saw above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3c40ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are heroes more well documented than villains?\n",
    "gg_doc = []\n",
    "bg_doc = []\n",
    "\n",
    "for char in characters:\n",
    "    if char in good_guys:\n",
    "        gg_doc.append(G.nodes[char][\"txtlength\"])\n",
    "    if char in bad_guys:\n",
    "        bg_doc.append(G.nodes[char][\"txtlength\"])\n",
    "\n",
    "# Calculate mean, 10th perc, 90th perc for good guys\n",
    "mean_gg = np.mean(gg_doc) \n",
    "per_10th_gg = np.percentile(gg_doc, 10)\n",
    "per_90th_gg =  np.percentile(gg_doc, 90)\n",
    "\n",
    "# Calculate mean, 10th perc, 90th perc for bad guys\n",
    "mean_bg = np.mean(bg_doc) \n",
    "per_10th_bg = np.percentile(bg_doc, 10)\n",
    "per_90th_bg =  np.percentile(bg_doc, 90) \n",
    "\n",
    "# Print the calculated values\n",
    "print(f\"Good guys:\\t\\t\\tBad guys:\")\n",
    "print(f\"mean: {mean_gg}\\t\\tmean: {mean_bg}\")\n",
    "print(f\"10th percentile: {per_10th_gg}\\t\\t10th percentile: {per_10th_bg}\")\n",
    "print(f\"90th percentile: {per_90th_gg}\\t90th percentile: {per_90th_bg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a205b6b",
   "metadata": {},
   "source": [
    "> So the good guys are better documented than the bad guys, in that they have longer texts describing them. Do they also have a higher fraction of negative words? Lets say the average happyness value for neutral is 5, then we count how large a fraction of the words for each character are negative and how many are positive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554e2048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions\n",
    "def frac_sad(tokens):\n",
    "    # Total sentiment score of file\n",
    "    sent_sum = 0.0\n",
    "    # Total number of occurences of words\n",
    "    occ_sum = 0\n",
    "    \n",
    "    # For each token and associated number of occurences\n",
    "    for token, occ in tokens.items():\n",
    "        # If the token is in the given list of words with rated happiness\n",
    "        for word, score in sent_list:\n",
    "            if token == word and float(score) < 5:\n",
    "                sent_sum += occ\n",
    "        occ_sum += occ\n",
    "    return sent_sum / occ_sum\n",
    "\n",
    "def frac_hap(tokens):\n",
    "        # Total sentiment score of file\n",
    "    sent_sum = 0.0\n",
    "    # Total number of occurences of words\n",
    "    occ_sum = 0\n",
    "    \n",
    "    # For each token and associated number of occurences\n",
    "    for token, occ in tokens.items():\n",
    "        # If the token is in the given list of words with rated happiness\n",
    "        for word, score in sent_list:\n",
    "            if token == word and float(score) > 5:\n",
    "                sent_sum += occ\n",
    "        occ_sum += occ\n",
    "    return sent_sum / occ_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f7758f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gg_sad = []\n",
    "bg_sad = []\n",
    "gg_hap = []\n",
    "bg_hap = []\n",
    "uncommented = 0\n",
    "\n",
    "# The code below has been commented out, since it takes a while to execute,\n",
    "# the following 2 code cells are using the code that has been commented out\n",
    "# in this cell. Hence, if this code is not uncommented the other will not \n",
    "# work properly, but we have written about our findings.\n",
    "\n",
    "#for file in os.listdir(\"dc/\"):\n",
    "#    orig_character = file[:-4]\n",
    "#    fileopener = open(\"dc/\" + file, 'r')\n",
    "#    text = fileopener.read()  \n",
    "#    tokens = nltk.wordpunct_tokenize(BeautifulSoup(text, 'html.parser').get_text())\n",
    "#    file_text = [w.lower() for w in tokens if w.isalpha()]\n",
    "#    fdist = FreqDist(file_text)\n",
    "#    if orig_character in good_guys:\n",
    "#        gg_sad.append(frac_sad(fdist))\n",
    "#        gg_hap.append(frac_hap(fdist))\n",
    "#    if orig_character in bad_guys:\n",
    "#        bg_sad.append(frac_sad(fdist))\n",
    "#        bg_hap.append(frac_hap(fdist))\n",
    "#       \n",
    "#for file in os.listdir(\"marvel/\"):\n",
    "#    orig_character = file[:-4]\n",
    "#    fileopener = open(\"marvel/\" + file, 'r')\n",
    "#    text = fileopener.read()  \n",
    "#    tokens = nltk.wordpunct_tokenize(BeautifulSoup(text, 'html.parser').get_text())\n",
    "#    file_text = [w.lower() for w in tokens if w.isalpha()]\n",
    "#    fdist = FreqDist(file_text)\n",
    "#    if orig_character in good_guys:\n",
    "#        gg_sad.append(frac_sad(fdist))\n",
    "#        gg_hap.append(frac_hap(fdist))\n",
    "#    if orig_character in bad_guys:\n",
    "#        bg_sad.append(frac_sad(fdist))\n",
    "#        bg_hap.append(frac_hap(fdist))\n",
    "#\n",
    "#uncommented = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb74444",
   "metadata": {},
   "outputs": [],
   "source": [
    "if uncommented == 1:\n",
    "    # Calculate mean, 10th perc, 90th perc for good guys\n",
    "    mean_gg = np.mean(gg_sad) \n",
    "    per_10th_gg = np.percentile(gg_sad, 10)\n",
    "    per_90th_gg =  np.percentile(gg_sad, 90)\n",
    "\n",
    "    # Calculate mean, 10th perc, 90th perc for bad guys\n",
    "    mean_bg = np.mean(bg_sad) \n",
    "    per_10th_bg = np.percentile(bg_sad, 10)\n",
    "    per_90th_bg =  np.percentile(bg_sad, 90) \n",
    "\n",
    "    # Print the calculated values\n",
    "    print(f\"Good guys:\\t\\t\\t\\tBad guys:\")\n",
    "    print(f\"mean: {mean_gg}\\t\\tmean: {mean_bg}\")\n",
    "    print(f\"10th percentile: {per_10th_gg}\\t10th percentile: {per_10th_bg}\")\n",
    "    print(f\"90th percentile: {per_90th_gg}\\t90th percentile: {per_90th_bg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dbe909",
   "metadata": {},
   "outputs": [],
   "source": [
    "if uncommented == 1:\n",
    "    # Calculate mean, 10th perc, 90th perc for good guys\n",
    "    mean_gg = np.mean(gg_hap) \n",
    "    per_10th_gg = np.percentile(gg_hap, 10)\n",
    "    per_90th_gg =  np.percentile(gg_hap, 90)\n",
    "\n",
    "    # Calculate mean, 10th perc, 90th perc for bad guys\n",
    "    mean_bg = np.mean(bg_hap) \n",
    "    per_10th_bg = np.percentile(bg_hap, 10)\n",
    "    per_90th_bg =  np.percentile(bg_hap, 90) \n",
    "\n",
    "    # Print the calculated values\n",
    "    print(f\"Good guys:\\t\\t\\t\\tBad guys:\")\n",
    "    print(f\"mean: {mean_gg}\\t\\tmean: {mean_bg}\")\n",
    "    print(f\"10th percentile: {per_10th_gg}\\t10th percentile: {per_10th_bg}\")\n",
    "    print(f\"90th percentile: {per_90th_gg}\\t90th percentile: {per_90th_bg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b288bc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lastly we just want to check how many characters are in the two lists\n",
    "print(f\"Number of good guys = {len(good_guys)}\")\n",
    "print(f\"Number of bad guys = {len(bad_guys)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd448e51",
   "metadata": {},
   "source": [
    "> We found that the the good guys had a lower fraction of \"sad\" words and a higher fraction of \"happy\" words. Which conincides with the original H1 and H2. Hence our idea for an alternative hapythesis may be incorrect. We also considered that it might have been caused by us having a much lower number of characters in one of the groups, but as can be seen above, this is not the case. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
