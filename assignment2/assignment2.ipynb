{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c016668",
   "metadata": {},
   "source": [
    "# Preface\n",
    "> The questions/exercises given for the assignment has been repeated in this notebook, they have been put in bold. To easily distinguish our answers and explanations have been indented, or printed from the executed code. \n",
    "\n",
    "## Imports\n",
    "> The following imports are needed throughout the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90d7d83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import nltk\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "import statistics\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "import json\n",
    "import urllib.request\n",
    "\n",
    "from nltk import word_tokenize, FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "from community import community_louvain\n",
    "from collections import Counter\n",
    "from os import listdir\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3682d81",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "> The downloaded files can be found in the folders `dc` and `marvel`, in our [github project](https://github.com/pete414n/02805SG/tree/main/assignment2)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5442aec6",
   "metadata": {},
   "source": [
    "# Basic Stats\n",
    "\n",
    "**Write a short paragraph describing the network. The paragraph should contain the following information**\n",
    "- **The number of nodes and links.**\n",
    "- **The average, median, mode, minimum and maximum value of the network's in-degree.s And of the out-degrees.**\n",
    "\n",
    "> This is just a placeholder for the short paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6a7051",
   "metadata": {},
   "source": [
    "**We also want the degree distributions and a plot of the network**\n",
    "- **Create in- and out-going degree distributions as described in Lecture 5.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34409278",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70b66195",
   "metadata": {},
   "source": [
    "- **Estimate the slope of the incoming degree distribtion as described in Lecture 5.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454feffa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1760e819",
   "metadata": {},
   "source": [
    "- **Plot the network using the Force Atlas algorithm as described in Lecture 5.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f30af3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9079b3f8",
   "metadata": {},
   "source": [
    "# Communities\n",
    "\n",
    "**Identify the communities in one or both of the superhero universes (DC/Marvel) as described in Week 7.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34964671",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94983cf0",
   "metadata": {},
   "source": [
    "**Visualize the communities by coloring the graph's nodes according to community affiliation - also as described in Week 7.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a9d857",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0bd0dea",
   "metadata": {},
   "source": [
    "**Write a paragraph about your work to identify the communities. The paragraph should include**\n",
    "- **Information about the algorithm you used to find communities.**\n",
    "- **The value of modularity for your network.**\n",
    "- **The number of communities and their sizes (in terms of number of nodes).**\n",
    "\n",
    "> This is just a placeholder for the paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dcf2d4",
   "metadata": {},
   "source": [
    "**For the 10 largest communities, process the text in order to create a TF-IDF vector for each community. Explain in your own words how TF-IDF works.**\n",
    "\n",
    "> This is just a placeholder for the paragraph explaining how TF-IDF works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03ae420",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3a21ef0",
   "metadata": {},
   "source": [
    "**Then visualize the TF-IDF vectors using wordclouds (as described in Week 7). Remember to comment on your word-clouds (e.g. in the figure captions): Do the wordclouds/TF-IDF lists enable you to understand the communities you have found (or is it just gibberish)? Justify your answer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232fac98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "200e91a4",
   "metadata": {},
   "source": [
    "> This is just a placeholder for the paragraph for our answer and the justification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4ebec0",
   "metadata": {},
   "source": [
    "# Sentiment\n",
    "\n",
    "**First calculate the sentiment for all character pages. Calculate the mean value of the sentiment and the 10th and 90th percentile values for the sentiment. Then create a nice histogram displaying the distribution of sentiment values for all pages. Indicate the mean, etc on that histogram.**\n",
    "\n",
    "> We start by preparing the data from the files that have been given for the exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f563ac13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store tuples of words and their \n",
    "# average happiness score\n",
    "sent_list = []\n",
    "\n",
    "# Read in the .tsv file\n",
    "with open(\"dataset_s1.tsv\", \"r\", encoding=\"utf8\") as sent_file:\n",
    "    tsv_reader = csv.DictReader(sent_file, delimiter=\"\\t\")\n",
    "    # For each sentiment in the file, save the word and average happiness in a tuple\n",
    "    # and add it to the list\n",
    "    for sent in tsv_reader:\n",
    "        word = sent[\"word\"]\n",
    "        average = sent[\"happiness_average\"]\n",
    "        sent_list.append((word, average))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058fe25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating the sentiment of a file from the frequency distribution for that file\n",
    "def sentiment(tokens):\n",
    "    # Total sentiment score of file\n",
    "    sent_sum = 0.0\n",
    "    # Total number of occurences of words\n",
    "    occ_sum = 0\n",
    "    \n",
    "    # For each token and associated number of occurences\n",
    "    for token, occ in tokens.items():\n",
    "        # If the token is in the given list of words with rated happiness\n",
    "        if token in [i[0] for i in sent_list]:\n",
    "            # For the token and score in the list of words with rated happiness\n",
    "            for (token, score) in sent_list:\n",
    "                # Multiply the average happiness score with the number of occurences\n",
    "                sent_sum += (float(score) * occ)\n",
    "                # Add the number of occurences to the total number of occurences\n",
    "                occ_sum += occ\n",
    "    return sent_sum / occ_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5840872c",
   "metadata": {},
   "source": [
    "**Now grab the good/bad character names (from this file, see Week 8 for details) and calculate the same stats for only good/bad characters; also plot the histograms for the good/bad group and indicate the values of the mean, etc.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfa117b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c2bf523",
   "metadata": {},
   "source": [
    "**Finally use the label permutation test (see Week 5) to investigate the two hypotheses below**\n",
    "- **H1: Good characters have a higher averge value for sentiment than a similarly sized set of randomly selected characters.**\n",
    "- **H2: Bad characters have a lower average value for sentiment than a similarly sized set of randomly selected characters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa26d9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20c46082",
   "metadata": {},
   "source": [
    "> This is just a placeholder for the paragraph for our findings in regards to H1 and H2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec7ffa8",
   "metadata": {},
   "source": [
    "**Write a short paragraph reflecting on your findings.**\n",
    "\n",
    "> This is just a placeholder for the paragraph"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
