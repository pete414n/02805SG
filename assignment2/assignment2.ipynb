{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c016668",
   "metadata": {},
   "source": [
    "# Preface\n",
    "> The questions/exercises given for the assignment has been repeated in this notebook, they have been put in bold. To easily distinguish our answers and explanations have been indented, or printed from the executed code. \n",
    "\n",
    "## Imports\n",
    "> The following imports are needed throughout the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90d7d83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import nltk\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "import statistics\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "import json\n",
    "import urllib.request\n",
    "\n",
    "from nltk import word_tokenize, FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "from community import community_louvain\n",
    "from collections import Counter\n",
    "from os import listdir\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3682d81",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "> The downloaded files can be found in the folders `dc` and `marvel`, in our [github project](https://github.com/pete414n/02805SG/tree/main/assignment2)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5442aec6",
   "metadata": {},
   "source": [
    "# Basic Stats\n",
    "\n",
    "**Write a short paragraph describing the network. The paragraph should contain the following information**\n",
    "- **The number of nodes and links.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e72e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of nodes\n",
    "\n",
    "\n",
    "# Number of links\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53ee3ec",
   "metadata": {},
   "source": [
    "- **The average, median, mode, minimum and maximum value of the network's in-degree.s And of the out-degrees.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2a8b4a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'G' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Making a sorted list of the in degrees\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m in_degree_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m([d \u001b[38;5;28;01mfor\u001b[39;00m (c, d) \u001b[38;5;129;01min\u001b[39;00m \u001b[43mG\u001b[49m\u001b[38;5;241m.\u001b[39min_degree], reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Average in degree\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28msum\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'G' is not defined"
     ]
    }
   ],
   "source": [
    "# Making a sorted list of the in degrees and a list of the out degrees\n",
    "in_degree_list = sorted([d for (c, d) in G.in_degree], reverse=True)\n",
    "out_degree_list = sorted([d for (c, d) in G.in_degree], reverse=True)\n",
    "\n",
    "# Average in degree\n",
    "sum = 0\n",
    "for d in in_degree_list:\n",
    "  sum += d\n",
    "print(f\"average in degree = {sum / len(in_degree_list)}\")\n",
    "\n",
    "# Average out degree\n",
    "sum = 0\n",
    "for d in out_degree_list:\n",
    "  sum += d\n",
    "print(f\"average out degree = {sum / len(out_degree_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7a2915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Median in degree\n",
    "median = -1\n",
    "mid = len(in_degree_list) // 2\n",
    "if len(in_degree_list) % 2 == 0:\n",
    "  median = (in_degree_list[mid] + in_degree_list[mid+1]) / 2\n",
    "else:\n",
    "  median = in_degree_list[mid]\n",
    "print(f\"median in degree = {median}\")\n",
    "\n",
    "# Median out degree\n",
    "median = -1\n",
    "mid = len(in_degree_list) // 2\n",
    "if len(in_degree_list) % 2 == 0:\n",
    "  median = (in_degree_list[mid] + in_degree_list[mid+1]) / 2\n",
    "else:\n",
    "  median = in_degree_list[mid]\n",
    "print(f\"median out degree = {median}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c8557f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mode in degree\n",
    "mode = max(set(in_degree_list), key=in_degree_list.count)\n",
    "print(f\"mode in degree = {mode}\")\n",
    "\n",
    "# Mode out degree\n",
    "mode = max(set(in_degree_list), key=in_degree_list.count)\n",
    "print(f\"mode out degree = {mode}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48a725b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we have the sorted list, the first element will be the max and the last will be the min\n",
    "# Minimum and maximum in degree\n",
    "min_in_degree = in_degree_list[-1]\n",
    "max_in_degree = in_degree_list[0]\n",
    "print(f\"minimum in degree = {min_in_degree}\\nmaximum in degree = {max_in_degree}\\n\")\n",
    "\n",
    "# Minimum and maximum out degree\n",
    "min_out_degree = out_degree_list[-1]\n",
    "max_out_degree = out_degree_list[0]\n",
    "print(f\"minimum out degree = {min_out_degree}\\nmaximum out degree = {max_out_degree}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebb4f20",
   "metadata": {},
   "source": [
    "> This is just a placeholder for the short paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6a7051",
   "metadata": {},
   "source": [
    "**We also want the degree distributions and a plot of the network**\n",
    "- **Create in- and out-going degree distributions as described in Lecture 5.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34409278",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70b66195",
   "metadata": {},
   "source": [
    "- **Estimate the slope of the incoming degree distribtion as described in Lecture 5.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454feffa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1760e819",
   "metadata": {},
   "source": [
    "- **Plot the network using the Force Atlas algorithm as described in Lecture 5.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f30af3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9079b3f8",
   "metadata": {},
   "source": [
    "# Communities\n",
    "\n",
    "**Identify the communities in one or both of the superhero universes (DC/Marvel) as described in Week 7.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d06d328",
   "metadata": {},
   "source": [
    "> We will be focusing on the DC superhefore universe. We start by recreating our graph for just the DC characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34964671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreating the network H, for only the dc universe\n",
    "dc_files = os.listdir('dc')\n",
    "H = nx.DiGraph()\n",
    "\n",
    "dc_nodes = []\n",
    "marvel_nodes = []\n",
    "\n",
    "# For week 5 we are adding the lengths of the wikitext for each character\n",
    "for dc_char in dc_files:\n",
    "  H.add_node(dc_char[:-4], uni=\"dc\", length=0)\n",
    "\n",
    "# Adding all the dc nodes and edges\n",
    "for source in dc_files:\n",
    "  # Remove .txt ending for character name\n",
    "  name = source[0:len(source)-4]\n",
    "\n",
    "  # Get the wikitext saved for this character\n",
    "  with open('dc/' + source) as f:\n",
    "    lines = f.read()\n",
    "\n",
    "  # Find all the links in this wikitext\n",
    "  matches = re.findall(r'[[]{2}.*?[]]{2}', lines)\n",
    "\n",
    "  # Count the number of words in this wikitext and set that attribute\n",
    "  word_count = len(re.findall(r'\\w+', lines))\n",
    "  H.nodes[name]['length'] = word_count\n",
    "\n",
    "  # Construct possible file name:\n",
    "  links = []\n",
    "  for match in matches: \n",
    "    link = match[2:len(match)-2]\n",
    "    link = link.split(\"|\")[0]\n",
    "    link = re.sub(r'\\s', '_', link) \n",
    "    #link.replace(\"Ã±\", \"n\")\n",
    "    link.replace(\"/\", \"-\")\n",
    "    links.append(link)\n",
    "\n",
    "  # Remove duplicates\n",
    "  links = [*set(links)]\n",
    "\n",
    "  # Add an edge for each link that can be found in either fold\n",
    "  for link in links:\n",
    "    if os.path.isfile('dc/' + link + \".txt\") or os.path.isfile('marvel/' + link + \".txt\"):\n",
    "      H.add_edge(name, link)\n",
    "\n",
    "print(f\"Did the number of nodes change? = {len(G.nodes)}\")\n",
    "\n",
    "largest_cc = max(nx.weakly_connected_components(H), key=len)\n",
    "gcc = nx.subgraph(H, largest_cc)\n",
    "uH = gcc.to_undirected(reciprocal=False, as_view=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c008b257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example from: https://perso.crans.org/aynaud/communities/\n",
    "#first compute the best partition\n",
    "partition = community_louvain.best_partition(uH)\n",
    "\n",
    "# Some code to find the number of communities and the number of nodes in each, needed for next question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94983cf0",
   "metadata": {},
   "source": [
    "**Visualize the communities by coloring the graph's nodes according to community affiliation - also as described in Week 7.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a9d857",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drawing\n",
    "size = float(len(set(partition.values())))\n",
    "pos = nx.spring_layout(uH)\n",
    "count = 0.\n",
    "for com in set(partition.values()) :\n",
    "    count = count + 1.\n",
    "    list_nodes = [nodes for nodes in partition.keys()\n",
    "                                if partition[nodes] == com]\n",
    "    nx.draw_networkx_nodes(uH, pos, list_nodes, node_size = 20,\n",
    "                                node_color = str(count / size))\n",
    "\n",
    "\n",
    "nx.draw_networkx_edges(uH,pos, alpha=0.5)\n",
    "\n",
    "# Needs coloring for the communities and smaller node size(?) or something else to make it more readable\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bd0dea",
   "metadata": {},
   "source": [
    "**Write a paragraph about your work to identify the communities. The paragraph should include**\n",
    "- **Information about the algorithm you used to find communities.**\n",
    "- **The value of modularity for your network.**\n",
    "- **The number of communities and their sizes (in terms of number of nodes).**\n",
    "\n",
    "> This is just a placeholder for the paragraph. Something about the Louvain algorithm. Something about modularity. Something about the number of communities and their sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dcf2d4",
   "metadata": {},
   "source": [
    "**For the 10 largest communities, process the text in order to create a TF-IDF vector for each community. Explain in your own words how TF-IDF works.**\n",
    "\n",
    "> This is just a placeholder for the paragraph explaining how TF-IDF works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03ae420",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3a21ef0",
   "metadata": {},
   "source": [
    "**Then visualize the TF-IDF vectors using wordclouds (as described in Week 7). Remember to comment on your word-clouds (e.g. in the figure captions): Do the wordclouds/TF-IDF lists enable you to understand the communities you have found (or is it just gibberish)? Justify your answer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232fac98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "200e91a4",
   "metadata": {},
   "source": [
    "> This is just a placeholder for the paragraph for our answer and the justification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4ebec0",
   "metadata": {},
   "source": [
    "# Sentiment\n",
    "\n",
    "**First calculate the sentiment for all character pages. Calculate the mean value of the sentiment and the 10th and 90th percentile values for the sentiment. Then create a nice histogram displaying the distribution of sentiment values for all pages. Indicate the mean, etc on that histogram.**\n",
    "\n",
    "> We start by preparing the data from the files that have been given for the exercise. First we make a list of all the words from the given sentiment list with their associated average happiness score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f563ac13",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'csv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Read in the .tsv file\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset_s1.tsv\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m sent_file:\n\u001b[0;32m----> 7\u001b[0m     tsv_reader \u001b[38;5;241m=\u001b[39m \u001b[43mcsv\u001b[49m\u001b[38;5;241m.\u001b[39mDictReader(sent_file, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# For each sentiment in the file, save the word and average happiness in a tuple\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# and add it to the list\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m tsv_reader:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'csv' is not defined"
     ]
    }
   ],
   "source": [
    "# Create an empty list to store tuples of words and their \n",
    "# average happiness score\n",
    "sent_list = []\n",
    "\n",
    "# Read in the .tsv file\n",
    "with open(\"dataset_s1.tsv\", \"r\", encoding=\"utf8\") as sent_file:\n",
    "    tsv_reader = csv.DictReader(sent_file, delimiter=\"\\t\")\n",
    "    # For each sentiment in the file, save the word and average happiness in a tuple\n",
    "    # and add it to the list\n",
    "    for sent in tsv_reader:\n",
    "        word = sent[\"word\"]\n",
    "        average = sent[\"happiness_average\"]\n",
    "        sent_list.append((word, average))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9850a1e2",
   "metadata": {},
   "source": [
    "> Then we create a function to calculate the sentiment from a frequency distribution for a given file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058fe25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating the sentiment of a file from the frequency distribution for that file\n",
    "def sentiment(tokens):\n",
    "    # Total sentiment score of file\n",
    "    sent_sum = 0.0\n",
    "    # Total number of occurences of words\n",
    "    occ_sum = 0\n",
    "    \n",
    "    # For each token and associated number of occurences\n",
    "    for token, occ in tokens.items():\n",
    "        # If the token is in the given list of words with rated happiness\n",
    "        if token in [i[0] for i in sent_list]:\n",
    "            # For the token and score in the list of words with rated happiness\n",
    "            for (token, score) in sent_list:\n",
    "                # Multiply the average happiness score with the number of occurences\n",
    "                sent_sum += (float(score) * occ)\n",
    "                # Add the number of occurences to the total number of occurences\n",
    "                occ_sum += occ\n",
    "    return sent_sum / occ_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2a0495",
   "metadata": {},
   "source": [
    "> Then we create lists of the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1cfa81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7042dced",
   "metadata": {},
   "source": [
    "> We are now ready to calculate the mean, 10th percentile, and the 90th percentile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7247cfbb",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (591039494.py, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [2], line 7\u001b[0;36m\u001b[0m\n\u001b[0;31m    per_10th = np.percentile(, 10) # Insert data\u001b[0m\n\u001b[0m                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#Source: https://stackoverflow.com/questions/2374640/how-do-i-calculate-percentiles-with-python-numpy\n",
    "\n",
    "# mean\n",
    "mean_total = np.mean() # Insert data\n",
    "\n",
    "# 10th percentile\n",
    "per_10th = np.percentile(, 10) # Insert data\n",
    "\n",
    "# 90 percentile sentiment \n",
    "per_90th =  np.percentile(, 90) # Insert data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a651da",
   "metadata": {},
   "source": [
    "> We are now able to make the histogram with the distribution and the calculated values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982fdc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = plt.hist(, bins=20, color='c', edgecolor='k', alpha=0.65) # Insert data set\n",
    "plt.axvline(per_10th, color='blue', linestyle='dashed', linewidth=1, label=\"10th percentile\")\n",
    "plt.axvline(per_90th, color='green', linestyle='dashed', linewidth=1, label=\"90th percentile\")\n",
    "plt.axvline(mean_total, color='orange', linestyle='dashed', linewidth=1, label=\"mean\")\n",
    "plt.xlabel('sentiment')\n",
    "plt.ylabel('counts')\n",
    "plt.figtext(.5,-0.05, f\"Histogram displaying the distribution of sentiments for all of the characters from the Marvel and DC universes. The dashed blue line is the 10th percentile, the dashed green line is the 90th percentile, and the dashed orange line is the mean.\", ha='center')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5840872c",
   "metadata": {},
   "source": [
    "**Now grab the good/bad character names (from this file, see Week 8 for details) and calculate the same stats for only good/bad characters; also plot the histograms for the good/bad group and indicate the values of the mean, etc.**\n",
    "\n",
    "> We start by creating lists of the good and the bad guys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfa117b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the given file\n",
    "with open('wiki_meta_data.csv', newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    data = list(reader)\n",
    "\n",
    "# Initialize lists to hold the names of the good and bad guys\n",
    "good_guys = []\n",
    "bad_guys = []\n",
    "characters = []\n",
    "alignments = []\n",
    "  \n",
    "# For each character in the given file\n",
    "for info in data:\n",
    "    # If the character is good\n",
    "    if info[6] == '1':\n",
    "        # Add that character name to the list of good guys\n",
    "        good_guys.append(info[1])\n",
    "        \n",
    "        alignments.append(info[6])\n",
    "    # If the character is bad\n",
    "    if info[7] == '0':\n",
    "        # Add that character name to the list of bad guys\n",
    "        bad_guys.append(info[1])\n",
    "        # Add the current alignment for that index in the alignment list\n",
    "        alignments.append(info[7])\n",
    "    # Add the character name to the full list of characters    \n",
    "    characters.append(info[1])\n",
    "\n",
    "# Remove the first element of the list since this contains the titles of the columns from the file\n",
    "characters = characters[1:]\n",
    "        \n",
    "#### NOTE: Remember to remove the bad/good guys that have been removed from the graph "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad683950",
   "metadata": {},
   "source": [
    "> We can now go through our graph and grab the previously calculated sentiments for each node, and add it to a list of either sentiments for good or bad guys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd188d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each node in the graph, take out the character name and the sentiment value\n",
    "all_nodes = [(k,v) for k,v in nx.get_node_attributes(G, \"sent\").items()]\n",
    "\n",
    "# Init lists to hold sentiment values for good guys (gg) and bad guys (bg)\n",
    "gg_sent = []\n",
    "bg_sent = []\n",
    "\n",
    "# For each tuple consisting of a character name (node) and a sentiment value in the all_nodes list\n",
    "for node, sent in all_nodes:\n",
    "    # If the character is one of the good guys\n",
    "    if node in good_guys:\n",
    "        # Add its sentiment value to the list of sentiments for good guys\n",
    "        gg_sent.append(sent)\n",
    "    # If the character is one of the bad guys\n",
    "    if node in bad_guys:\n",
    "        # Add its sentiment value to the list of the sentiments for the bad guys\n",
    "        bg_sent.append(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1ded7f",
   "metadata": {},
   "source": [
    "> The mean, 10th and 90th percentiles can now be calculated for both good and bad guys, and plotted in histograms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86749b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram for good and bad guys\n",
    "gg_counts, gg_bins = np.histogram(gg_sent)\n",
    "bg_counts, bg_bins = np.histogram(bg_sent)\n",
    "\n",
    "# Setting same number of bins for both\n",
    "number_of_bins = 10 \n",
    "\n",
    "# Create two subplots and unpack the output array immediately\n",
    "fig, axs = plt.subplots(1, 2, sharex=True)\n",
    "\n",
    "# Histogram for good guys\n",
    "histogram = np.histogram(gg_sent, number_of_bins)\n",
    "\n",
    "# Calculate values\n",
    "mean_gg = np.mean() # Insert data\n",
    "per_10th_gg = np.percentile(, 10) # Insert data\n",
    "per_90th_gg =  np.percentile(, 90) # Insert data\n",
    "\n",
    "axs[0].hist(gg_sent, bins = 25, edgecolor='black')\n",
    "axs[0].set_title(f'Histogram of good guys sentiment')\n",
    "axs[0].xlabel('sentiment')\n",
    "axs[0].ylabel('counts')\n",
    "axs[0].axvline(per_10th_gg, color='blue', linestyle='dashed', linewidth=1, label=\"10th percentile\")\n",
    "axs[0].axvline(per_90th, color='green', linestyle='dashed', linewidth=1, label=\"90th percentile\")\n",
    "axs[0].axvline(per_90th_gg, color='orange', linestyle='dashed', linewidth=1, label=\"mean\")\n",
    "axs[0].figtext(.5,-0.05, f\"Histogram displaying the distribution of sentiments for all of the good guys from both universes. The dashed blue line is the 10th percentile, the dashed green line is the 90th percentile, and the dashed orange line is the mean.\", ha='center')\n",
    "axs[0].legend()\n",
    "\n",
    "# Histogram for bad guys\n",
    "histogram = np.histogram(bg_sent, number_of_bins)\n",
    "\n",
    "# Calculate values for plot\n",
    "mean_bg = np.mean() # Insert data\n",
    "per_10th_bg = np.percentile(, 10) # Insert data\n",
    "per_90th_bg =  np.percentile(, 90) # Insert data\n",
    "\n",
    "axs[1].hist(bg_sent, bins = 25, edgecolor='black')\n",
    "axs[1].set_title(f'Histogram of bad guys sentiment')\n",
    "axs[1].xlabel('sentiment')\n",
    "axs[1].ylabel('counts')\n",
    "axs[1].axvline(per_10th, color='blue', linestyle='dashed', linewidth=1, label=\"10th percentile\")\n",
    "axs[1].axvline(per_90th, color='green', linestyle='dashed', linewidth=1, label=\"90th percentile\")\n",
    "axs[1].axvline(mean_total, color='orange', linestyle='dashed', linewidth=1, label=\"mean\")\n",
    "axs[1].figtext(.5,-0.05, f\"Histogram displaying the distribution of sentiments for all of the bad guys from both universes. The dashed blue line is the 10th percentile, the dashed green line is the 90th percentile, and the dashed orange line is the mean.\", ha='center')\n",
    "axs[1].legend()\n",
    "\n",
    "# Print the calculated values\n",
    "print(f\"Good guys:\\t\\t\\tBad guys:\")\n",
    "print(f\"mean: {mean_gg}\\t\\t\\tmean: {mean_bg}\")\n",
    "print(f\"10th percentile: {per_10th_gg}\\t\\t\\t10th percentile: {per_10th_bg}\")\n",
    "print(f\"90th percentile: {per_90th_gg}\\t\\t\\t90th percentile{per_90th_bg}\")\n",
    "\n",
    "# Show both histrograms\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b650d7",
   "metadata": {},
   "source": [
    "> Placeholder for paragraph about the histograms (i.e. initial findings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2bf523",
   "metadata": {},
   "source": [
    "**Finally use the label permutation test (see Week 5) to investigate the two hypotheses below**\n",
    "- **H1: Good characters have a higher averge value for sentiment than a similarly sized set of randomly selected characters.**\n",
    "- **H2: Bad characters have a lower average value for sentiment than a similarly sized set of randomly selected characters.**\n",
    "\n",
    "> Placeholder for text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b873d1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init lists to hold the values for each sample generated\n",
    "average_sentiment_samples_good_guys = []\n",
    "average_sentiment_samples_bad_guys = []\n",
    "\n",
    "# Create 1000 samples\n",
    "for n in range(1000):\n",
    "    # We want to start by shuffling who is bad and good.\n",
    "    alignments = random.sample(alignments, k=len(alignments))\n",
    "    \n",
    "    # We can then get the alignment of the nodes by finding their index in the characters list\n",
    "    # and then looking up their (shuffled) alignment in the alignments list\n",
    "    sent_bad = 0\n",
    "    sent_good = 0\n",
    "    \n",
    "    rem_c = []\n",
    "    \n",
    "    # Since we are taking a sample in the size of all the characters we have\n",
    "    # we just go through all of the characters and check their sentiment and\n",
    "    # newly assigned alignment (good/bad)\n",
    "    for c in characters:\n",
    "        if G.has_node(c):\n",
    "            a = alignments[characters.index(c)]\n",
    "            if a == '1': # if its a good guy\n",
    "                sent_good += G.nodes[c][\"sentiment\"]\n",
    "            if a == '0': # if its a bad guy\n",
    "                sent_bad += G.nodes[c][\"sentiment\"]\n",
    "        else:\n",
    "            rem_c.append(c)\n",
    "    \n",
    "    characters = [c for c in characters if c not in rem_c]\n",
    "\n",
    "    # Divide the sum by the number of good/bad guys (in this sample)\n",
    "    # Add this average to the average_sentiment_samples list\n",
    "    average_sentiment_samples_good_guys.append(sent_good / len(good_guys))\n",
    "    average_sentiment_samples_bad_guys.append(sent_bad / len(bad_guys))\n",
    "\n",
    "# Plot a histogram of the averages\n",
    "fig, axs = plt.subplots(1, 2, sharex=True, figsize=(15, 5))\n",
    "axs[0].hist(average_sentiment_samples_good_guys, bins = 10, edgecolor='black')\n",
    "axs[0].axvline(gg_mean, color='orange', linestyle='dashed', linewidth=1, label=\"mean\")\n",
    "axs[0].set_title('Result of label permutation for good guys')\n",
    "axs[1].figtext(.5,-0.05, f\"Histogram displaying the distribution of sentiments from the label permutation for the good guys from both universes. The dashed orange line is the mean.\", ha='center')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].hist(average_sentiment_samples_bad_guys, bins = 10, edgecolor='black')\n",
    "axs[1].axvline(bg_mean, color='orange', linestyle='dashed', linewidth=1, label=\"mean\")\n",
    "axs[1].set_title('Result of label permutation for Bad guys')\n",
    "axs[1].figtext(.5,-0.05, f\"Histogram displaying the distribution of sentiments from the label permutation for the bad guys from both universes. The dashed orange line is the mean.\", ha='center')\n",
    "axs[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec7ffa8",
   "metadata": {},
   "source": [
    "**Write a short paragraph reflecting on your findings.**\n",
    "\n",
    "> This is just a placeholder for the paragraph"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
