{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d09732d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from nltk import word_tokenize, FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from community import community_louvain\n",
    "from fa2 import ForceAtlas2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21a0c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create character tuple list\n",
    "import csv\n",
    "import os\n",
    "\n",
    "characters = []\n",
    "\n",
    "with open(\"HP_characters.csv\", \"r\", encoding=\"utf8\") as sent_file:\n",
    "    csv = csv.reader(sent_file, delimiter=\",\")\n",
    "    for row in csv:\n",
    "        name = row[0].replace(' ', '_')\n",
    "        parentage = row[1]\n",
    "        house = row[2]\n",
    "        occupation = row[3]\n",
    "        characters.append((name, parentage, house, occupation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15273e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm that we have a file for each character in our list\n",
    "unzipped = list(list(zip(*characters))[0])\n",
    "names = []\n",
    "\n",
    "for file in os.listdir(\"characters\"):\n",
    "    names.append(file[:-4])\n",
    "\n",
    "# The difference between the sets is the empty set\n",
    "print(set(unzipped) - set(names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7879449b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import networkx as nx\n",
    "\n",
    "# Init graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Insert nodes\n",
    "for character in characters:\n",
    "    G.add_node(character[0], parentage=character[1], house=character[2], occupation=[3])\n",
    "    \n",
    "for file in os.listdir(\"characters\"):\n",
    "    orig_character = file[:-4]\n",
    "    \n",
    "    link_list = []\n",
    "    fileopener = open(\"characters/\" + file, 'r')\n",
    "    text = fileopener.read()    \n",
    "    links = re.findall(r'[[]{2}.*?[]]{2}', text)\n",
    "        \n",
    "    for link in links:\n",
    "        character = link.replace(\"[\", \"\").replace(\"]\", \"\").replace(\" \", \"_\")\n",
    "        character = character.partition(\"|\")[0]\n",
    "        link_list.append(character)\n",
    "    link_list = [*set(link_list)]\n",
    "    \n",
    "    for character in link_list:\n",
    "        filename = character + \".txt\"\n",
    "        if os.path.isfile(\"characters/\" + filename) or os.path.isfile(\"characters/\" + filename):\n",
    "            G.add_edge(orig_character, character)\n",
    "\n",
    "print(\"Number of nodes: \" + str(G.number_of_nodes()))\n",
    "print(\"Number of edges: \" + str(G.number_of_edges()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f13ae32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "in_degrees = [i for (c, i) in G.in_degree]\n",
    "out_degrees = [o for (c, o) in G.out_degree]\n",
    "\n",
    "print(f\"sum in_degrees = {sum(in_degrees)}\")\n",
    "print(f\"sum out_degrees = {sum(out_degrees)}\")\n",
    "\n",
    "in_counts, in_bins = np.histogram(in_degrees)\n",
    "out_counts, out_bins = np.histogram(out_degrees)\n",
    "\n",
    "plt.plot(in_bins[:-1], in_counts, color = 'green', label='in-degrees')\n",
    "plt.plot(out_bins[:-1], out_counts, color = 'red', label = 'out-degrees')\n",
    "plt.legend()\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('Number of Nodes')\n",
    "plt.title('In- and out-degrees')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d923af",
   "metadata": {},
   "source": [
    "## Communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33214327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the graph undirected for the plotting\n",
    "uG = G.to_undirected()\n",
    "\n",
    "nodes = list(uG.nodes)\n",
    "degrees = [d for (n,d) in list(uG.degree)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bea4098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusting figure size\n",
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "\n",
    "forceatlas2 = ForceAtlas2(\n",
    "                        # Behavior alternatives\n",
    "                        outboundAttractionDistribution=True,  # Dissuade hubs\n",
    "                        linLogMode=False,  # NOT IMPLEMENTED\n",
    "                        adjustSizes=False,  # Prevent overlap (NOT IMPLEMENTED)\n",
    "                        edgeWeightInfluence=1.0,\n",
    "\n",
    "                        # Performance\n",
    "                        jitterTolerance=1.0,  # Tolerance\n",
    "                        barnesHutOptimize=True,\n",
    "                        barnesHutTheta=1.2, # original 1.2\n",
    "                        multiThreaded=False,  # NOT IMPLEMENTED\n",
    "\n",
    "                        # Tuning\n",
    "                        scalingRatio=2.0,\n",
    "                        strongGravityMode=True,\n",
    "                        gravity=0.1, # original 0.5\n",
    "\n",
    "                        # Log\n",
    "                        verbose=True)\n",
    "\n",
    "positions = forceatlas2.forceatlas2_networkx_layout(G, pos=None, iterations=2000)\n",
    "nx.draw_networkx_nodes(G, positions, nodelist = nodes, node_size=degrees, node_color=\"red\", alpha=0.4)\n",
    "nx.draw_networkx_edges(G, positions, edge_color=\"green\", alpha=0.05)\n",
    "plt.axis('off')\n",
    "plt.title('FA2-plot of DC/Marvel network')\n",
    "plt.figtext(.5, -0.05, f\"The red dots represent Marvel characters and the blue dots represents DC characters.\\nThe size of a dot indicates how many links it has. The figure shows a big grouping for each of the two universes.\", ha=\"center\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6df728d",
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = community_louvain.best_partition(uG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc0a9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"communities_from_fandom.csv\", \"w\")\n",
    "for character, community in list(partition.items()): \n",
    "    f.write(character + \",\" + str(community) + \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000ce899",
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_list = []\n",
    "for com in set(partition.values()) :\n",
    "    list_nodes = [nodes for nodes in partition.keys()\n",
    "                                if partition[nodes] == com]\n",
    "    partition_list.append(list_nodes)\n",
    "partition_list = sorted(partition_list, key=len, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98818470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusting figure size\n",
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "\n",
    "#drawing using ForceAtlas\n",
    "forceatlas2 = ForceAtlas2(\n",
    "                        # Behavior alternatives\n",
    "                        outboundAttractionDistribution=False,  # Dissuade hubs\n",
    "                        linLogMode=False,  # NOT IMPLEMENTED\n",
    "                        adjustSizes=False,  # Prevent overlap (NOT IMPLEMENTED)\n",
    "                        edgeWeightInfluence=0.1,\n",
    "\n",
    "                        # Performance\n",
    "                        jitterTolerance=0.5,  # Tolerance\n",
    "                        barnesHutOptimize=True,\n",
    "                        barnesHutTheta=0.5, # original 0.5\n",
    "                        multiThreaded=False,  # NOT IMPLEMENTED\n",
    "\n",
    "                        # Tuning\n",
    "                        scalingRatio=10.0,\n",
    "                        strongGravityMode=True,\n",
    "                        gravity=0.1,\n",
    "\n",
    "                        # Log\n",
    "                        verbose=True)\n",
    "\n",
    "positions = forceatlas2.forceatlas2_networkx_layout(uG, pos=None, iterations=2000)\n",
    "cmap = plt.get_cmap('plasma', max(partition.values()) + 1)\n",
    "\n",
    "nx.draw_networkx_nodes(uG, positions, partition.keys(), node_size=40,\n",
    "                        cmap=cmap, node_color=list(partition.values()))\n",
    "nx.draw_networkx_edges(uG, positions, edge_color='green', alpha=0.1)\n",
    "plt.title('FA2-plot of communities')\n",
    "plt.figtext(.5, 0, f\"The plot contains the 10 communities found in our DC data set.\\nEach community has a distinct color, edges have been colored green\", ha=\"center\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73e121d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Putting together the text from the pages belonging to each community\n",
    "community_texts = []\n",
    "maxrange = 0\n",
    "#if we have less than 10 communities\n",
    "if len(partition_list) < 10:\n",
    "    maxrange = len(partition_list)\n",
    "else:\n",
    "    maxrange = 10\n",
    "    \n",
    "for sublist in partition_list[:maxrange]:\n",
    "    com_txt = []\n",
    "    for character in sublist:\n",
    "        f = open(\"./characters/\"+character+\".txt\")\n",
    "        raw = f.read()\n",
    "        tokens = nltk.wordpunct_tokenize(BeautifulSoup(raw, 'html.parser').get_text())\n",
    "        file_text = [w.lower() for w in tokens if w.isalpha()]\n",
    "        com_txt = com_txt + file_text\n",
    "    community_texts.append(com_txt)\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "community_strings = []\n",
    "for txt in community_texts:\n",
    "    com_words = [w for w in txt if w not in stopwords]\n",
    "    community_strings.append(com_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f75fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_terms = []\n",
    "for community_words in community_strings:\n",
    "    unique_terms.append(list(set(community_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5358bbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf(word, unique_list):\n",
    "    N = len(unique_list)\n",
    "    term_appears = 0\n",
    "    for sublist in unique_list:\n",
    "        if word in sublist:\n",
    "            term_appears+=1\n",
    "    idf_val = math.log(N/(1+term_appears))+1\n",
    "    return idf_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58a7cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the tf list\n",
    "tfidf_list = []\n",
    "\n",
    "for community_words in community_strings:\n",
    "    fdist = FreqDist(community_words)\n",
    "    total_terms = len(community_words)\n",
    "    tfidf=[]\n",
    "    for word in fdist:\n",
    "        idf_val = idf(word, unique_terms)\n",
    "        tf_val = fdist[word]/total_terms\n",
    "        tfidf_elem=(word, tf_val*idf_val)\n",
    "        tfidf.append(tfidf_elem)\n",
    "    tfidf_list.append(tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a282b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.rcParams['figure.figsize'] = [15, 20]\n",
    "captions = [\"Gibberish 1\",\n",
    "            \"Batman\",\n",
    "            \"Superman\",\n",
    "            \"Vandal Savage\",\n",
    "            \"Teen Titans\",\n",
    "            \"Outsiders\",\n",
    "            \"Justice League 2\",\n",
    "            \"Wonder Woman\",\n",
    "            \"Green Lantern\",\n",
    "            \"Legion of Super-Heroes\"]\n",
    "\n",
    "for i in range(len(tfidf_list)):\n",
    "    ax = fig.add_subplot(5,2,i+1)\n",
    "    wordcloud = WordCloud(background_color='black', width=2200,\n",
    "                      height=1800, collocations=False).generate_from_frequencies(dict(tfidf_list[i]))\n",
    "\n",
    "    ax.imshow(wordcloud)\n",
    "    ax.text(0.5,-0.1, captions[i], size=12, ha=\"center\", \n",
    "         transform=ax.transAxes)\n",
    "    ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b996c48",
   "metadata": {},
   "source": [
    "## Temporal graphs\n",
    "\n",
    "The function ```temporal_graph(dict, outer, inner)``` is meant to take our dictionary of characters with their aliases, and look for any matches in ```outer```, which will be a path to the text that will give us the nodes of the graph. e.g. a chapter or a book. ```Ã¬nner``` will be path to the texts making up the text file(s) in ```outer```, for which we will be looking for links. e.g. if ```outer```is a chapter, then ```inner``` could be the pages for that chapter. \n",
    "\n",
    "Note: Why cant we do the same for both arguments? Then the graph will be complete, since all the characters of that chapter/book will have links to all other characters from that book, since they will be in that part together.\n",
    "The idea is to get a graph of all the characters in a book, and then construct a graph for each of the chapters, this will allow us to see how the network change throughout the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c71e0446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the characters that appear within some boundary (chapter, book, etc)\n",
    "# and link them according to those they appear along with (paragraph, page, chapter)\n",
    "# To add links such that all the nodes are not just linking each other \n",
    "# the boundary for the links must be a proper subset of the boundary for the nodes (?)\n",
    "# Find all sentiments for characters from aliases in a book\n",
    "\n",
    "# inputs:\n",
    "# dict_aliases: A dictionary of characters with their aliases\n",
    "# path_to_bound_one: Path to the text that represents the larger boundary (chapter, book)\n",
    "# path_to_bound_two: Path to the text that represents the smaller boundary (paragrah, page, chapter)\n",
    "def temporal_graph(dict_aliases, path_to_bound_one, path_to_bound_two):\n",
    "    # Making the dict into a list of tuples (character, aliases)\n",
    "    character_aliases = list(dict_aliases.items())\n",
    "    nodes_bound = os.listdir(path_to_bound_one)\n",
    "    links_bound = os.listdir(path_to_bound_two)\n",
    "\n",
    "    # Init graph\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Find all the characters that appear in the outher bound, add them as nodes\n",
    "    for character, aliases in character_aliases: # For each character\n",
    "        # read in text to look for nodes\n",
    "        f = open(path_to_bound_one + chapter)\n",
    "        raw = f.read()\n",
    "        tokens = nltk.wordpunct_tokenize(raw)\n",
    "        text = nltk.Text(tokens)\n",
    "        character_found = False\n",
    "        \n",
    "        for alias in aliases: # For each alias for that character see if there is an occurence\n",
    "                if alias in text:\n",
    "                    character_found = True\n",
    "                    break\n",
    "        if character_found:# The character was in the outer bound\n",
    "            G.add_node(character)\n",
    "    \n",
    "    # Find all the characters that are named in the same inner bound, add links between them\n",
    "    for character_source, aliases_source in character_aliases: # For each character\n",
    "        # read in text to look for nodes\n",
    "        f = open(path_to_bound_two + chapter)\n",
    "        raw = f.read()\n",
    "        tokens = nltk.wordpunct_tokenize(raw)\n",
    "        text = nltk.Text(tokens)\n",
    "        link_found = False\n",
    "        \n",
    "        for alias_source in aliases_source:\n",
    "            for character_target, aliases_target in character_aliases:\n",
    "                if alias_source in text and alias_target in text:\n",
    "                    G.add_edge(character_source, character_target)\n",
    "                    break                    \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40c236cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Idea\n",
    "#temporal_graph(dict_aliases, book7, chapter1)\n",
    "#temporal_graph(dict_aliases, book7, chapter2)\n",
    "# etc, to see how the network changes throughout book7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10413b7",
   "metadata": {},
   "source": [
    "Since the graph is undirected we cannot examine the in- and out-degrees, but we can see how the number of \"active\" nodes and links changes. For the book example, we may see how the number of characters in each chapter changes, and how many interactions they have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6a97b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming we have a list of graphs, by running temporal_graph on a book and all of its chapters,\n",
    "# this function takes the list of graphs as an argument and plots the number of active nodes\n",
    "# and the number of links\n",
    "\n",
    "# Expected output: A graph showing the number of \"active\" nodes for the chapter\n",
    "\n",
    "def plot_graph_dev(graph_list):\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8729ac8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_graph(dict_aliases, path_to_bound_one, path_to_bound_two):\n",
    "    # Making the dict into a list of tuples (character, aliases)\n",
    "    character_aliases = list(dict_aliases.items())\n",
    "    nodes_bound = os.listdir(path_to_bound_one)\n",
    "    links_bound = os.listdir(path_to_bound_two)\n",
    "\n",
    "    # Init graph\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Find all the characters that appear in the outher bound, add them as nodes\n",
    "    for character, aliases in character_aliases: # For each character\n",
    "        # read in text to look for nodes\n",
    "        f = open(path_to_bound_one + chapter)\n",
    "        raw = f.read()\n",
    "        tokens = nltk.wordpunct_tokenize(raw)\n",
    "        text = nltk.Text(tokens)\n",
    "        character_found = False\n",
    "        \n",
    "        for alias in aliases: # For each alias for that character see if there is an occurence\n",
    "                if alias in text:\n",
    "                    character_found = True\n",
    "                    break\n",
    "        if character_found:# The character was in the outer bound\n",
    "            G.add_node(character)\n",
    "    \n",
    "    # Find all the characters that are named in the same inner bound, add links between them\n",
    "    for character_source, aliases_source in character_aliases: # For each character\n",
    "        # read in text to look for nodes\n",
    "        f = open(path_to_bound_two + chapter)\n",
    "        raw = f.read()\n",
    "        tokens = nltk.wordpunct_tokenize(raw)\n",
    "        text = nltk.Text(tokens)\n",
    "        link_found = False\n",
    "        \n",
    "        for alias_source in aliases_source:\n",
    "            for character_target, aliases_target in character_aliases:\n",
    "                if alias_source in text and alias_target in text:\n",
    "                    G.add_edge(character_source, character_target)\n",
    "                    break                    \n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cd0e26",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
