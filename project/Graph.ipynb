{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09732d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import csv\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import networkx as nx\n",
    "from networkx.algorithms import community\n",
    "\n",
    "from nltk import word_tokenize, FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from community import community_louvain\n",
    "from fa2 import ForceAtlas2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21a0c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create character tuple list\n",
    "\n",
    "characters = []\n",
    "\n",
    "with open(\"HP_characters.csv\", \"r\", encoding=\"utf8\") as sent_file:\n",
    "    csv_file = csv.reader(sent_file, delimiter=\",\")\n",
    "    for row in csv_file:\n",
    "        name = row[0].replace(' ', '_')\n",
    "        # lower to make sure they are spelled the same way\n",
    "        parentage = row[1].lower()\n",
    "        house = row[2].lower()\n",
    "        occupation = row[3].lower()\n",
    "        loyalty = row[4]\n",
    "        characters.append((name, parentage, house, occupation, loyalty))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cde76f5",
   "metadata": {},
   "source": [
    "# Graph with weights\n",
    "Making a graph for each book. The nodes are the characters in the book, and edges are between characters that are in the same chapter. Edges have weight corresponding to the number of times those two characters are in the same chapter. Nodes have the attributes parentage, house, occupation and loyalty. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a8b592",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input: A list of character names with their attributes, \n",
    "       the path of the book,\n",
    "       how many sentences to look at at time\n",
    "Output: A weighted graph\n",
    "\"\"\"\n",
    "def weighted_temporal_graphs(character_list, path, sentence_no):\n",
    "    \n",
    "    G = nx.Graph()\n",
    "  \n",
    "    # Go throug each chapter in the book\n",
    "    for chapter in os.listdir(path): \n",
    "        #only look at the files where aliases have been replaced with character names\n",
    "        if \"replaced\" in chapter:   \n",
    "\n",
    "            # Get text\n",
    "            with open(path + chapter) as f:\n",
    "                text = f.read()\n",
    "            \n",
    "            # Put all characters from the chapter in the graph if they are not already there\n",
    "            for character in character_list:\n",
    "                if character[0] in text and character[0] not in list(G.nodes):\n",
    "                    G.add_node(character[0], parentage = character[1], \n",
    "                               house = character[2], occupation = character[3], loyalty = character[4])\n",
    "            \n",
    "            # Split the text in sentences \n",
    "            sentences = text.split(\". \")\n",
    "            count_start = 0\n",
    "            count_end = sentence_no\n",
    "            \n",
    "            # Look at specified amount of senteces at a time\n",
    "            while (count_start < len(sentences)):\n",
    "                current = sentences[count_start:count_end]\n",
    "                current = \" \".join(current)\n",
    "                \n",
    "                # Go through the nodes and check if two diffferent nodes appear in the same text piece\n",
    "                # if so add an edge\n",
    "                # weight is the amount of times they appear together throughout the book\n",
    "                for character_source in list(G.nodes):\n",
    "                    #print(character_source)\n",
    "                    for character_target in list(G.nodes):\n",
    "                        #print(character_target)\n",
    "                        if character_source is character_target:\n",
    "                            continue\n",
    "                        elif (character_source in current and character_target in current):\n",
    "                            if G.has_edge(character_source, character_target):\n",
    "                                G[character_source][character_target]['weight'] += 1\n",
    "                            else:\n",
    "                                G.add_edge(character_source, character_target, weight=1)\n",
    "                            #print(\"added some edge\")\n",
    "                count_start = count_end\n",
    "                count_end += sentence_no\n",
    "    # Remove nodes without edges\n",
    "    print(list(nx.isolates(G)))\n",
    "    G.remove_nodes_from(list(nx.isolates(G)))\n",
    "    #print(\"Done with graph\")\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19383129",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.operators.binary.compose.html#networkx.algorithms.operators.binary.compose\n",
    "\n",
    "\"\"\"\n",
    "Input: two graphs to combine\n",
    "Output: the combined graph, including the weights of the edges added together if same edges\n",
    "\"\"\"\n",
    "\n",
    "def combine_graphs(g1, g2):\n",
    "    combined= nx.compose(g1, g2)\n",
    "    edge_data = {e: g1.edges[e]['weight'] + g2.edges[e]['weight'] \n",
    "                 for e in g1.edges & g2.edges}\n",
    "    nx.set_edge_attributes(combined, edge_data, 'weight')\n",
    "    \n",
    "    return combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e4f1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Input: a graph\n",
    "Output: A list of of the summed weights for the edges for each node,\n",
    "        this list is ordered as the list of nodes returned from graph.nodes\n",
    "\"\"\"\n",
    "def get_weight_sums(graph):\n",
    "    weight_sums = []\n",
    "    for node in list(graph.nodes):\n",
    "        sum = 0\n",
    "        for source, target in list(graph.edges):\n",
    "            if node is source or node is target:\n",
    "                sum += graph[source][target][\"weight\"]\n",
    "        weight_sums.append(sum)\n",
    "    return weight_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86f9899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/5294955/how-to-scale-down-a-range-of-numbers-with-a-known-min-and-max-value\n",
    "\"\"\"\n",
    "Input: a = minmum value for scaled weights\n",
    "       b = maximum value for scaled weights\n",
    "       G = graph\n",
    "Output: A list with the scaled weights\n",
    "\"\"\"\n",
    "\n",
    "def scaled_weights(a, b, G):\n",
    "    weights = get_weight_sums(G)\n",
    "    max_weight = max(weights)\n",
    "    min_weight = min(weights)\n",
    "    scaled = []\n",
    "    for w in weights:\n",
    "        scaled.append(((b - a) * (w - min_weight) // (max_weight - min_weight)) + a)\n",
    "    return scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c009a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/13070461/get-indices-of-the-top-n-values-of-a-list\n",
    "\n",
    "\"\"\"\n",
    "Input: G = graph\n",
    "       n = Number of nodes from top to bottom\n",
    "       f = flag, True = top, False = bottom\n",
    "Output: A list with names of hte n nodes\n",
    "        A list of node sizes as a tuple\n",
    "        A list of the indices\n",
    "\"\"\"\n",
    "\n",
    "def get_nodes_extreme(G, n, f):\n",
    "    if f:\n",
    "        n_indices = np.argsort(get_weight_sums(G))[-n:]\n",
    "    else:\n",
    "        n_indices = np.argsort(get_weight_sums(G))[0:n]\n",
    "    sc_weights = scaled_weights(50, 800, G)\n",
    "    names = []\n",
    "    weights = []\n",
    "    indices = []\n",
    "    for i in n_indices:\n",
    "        names.append(list(G.nodes())[i])\n",
    "        weights.append(sc_weights[i])\n",
    "        indices.append(i)\n",
    "    return (names, weights, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93dd8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colors to use when drwaing the communities\n",
    "community_colors = [\"#B6F20D\", \"#0DF2BC\", \"#490DF2\", \"#F20D43\", \"#13EC33\", \"#1360EC\", \"#EC13CC\", \"#EC9F13\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b1739c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input: a graph,\n",
    "       title for the plot, default is empty string\n",
    "Output: plots the graph using forceAtlas\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def draw_network(graph, title=\"\"):\n",
    "    # Adjusting figure size\n",
    "    plt.rcParams['figure.figsize'] = [10, 10]\n",
    "\n",
    "    forceatlas2 = ForceAtlas2(\n",
    "                            # Behavior alternatives\n",
    "                            outboundAttractionDistribution=True,  # Dissuade hubs\n",
    "                            linLogMode=False,  # NOT IMPLEMENTED\n",
    "                            adjustSizes=False,  # Prevent overlap (NOT IMPLEMENTED)\n",
    "                            edgeWeightInfluence=1.0,\n",
    "\n",
    "                            # Performance\n",
    "                            jitterTolerance=1.0,  # Tolerance\n",
    "                            barnesHutOptimize=True,\n",
    "                            barnesHutTheta=1.2, # original 1.2\n",
    "                            multiThreaded=False,  # NOT IMPLEMENTED\n",
    "\n",
    "                            # Tuning\n",
    "                            scalingRatio=2.0,\n",
    "                            strongGravityMode=True,\n",
    "                            gravity=0.1, # original 0.5\n",
    "\n",
    "                            # Log\n",
    "                            verbose=True)\n",
    "\n",
    "    positions = forceatlas2.forceatlas2_networkx_layout(graph, pos=None, iterations=2000)\n",
    "    nx.draw_networkx_edges(graph, positions, edge_color=\"black\", alpha=0.1)\n",
    "       \n",
    "    # Making 3 lists: top n max_weights, bottom n min_weights, rest\n",
    "    max_nodes, max_sizes, max_indices = get_nodes_extreme(graph, 10, True)\n",
    "    min_nodes, min_sizes, min_indices = get_nodes_extreme(graph, 10, False)\n",
    "    \n",
    "    rest_nodes = [n for n in list(graph.nodes()) if n not in max_nodes and n not in min_nodes]\n",
    "    rest_sizes = []\n",
    "    indices_to_remove = max_indices + min_indices\n",
    "    G_scaled_weights = scaled_weights(50, 800, graph)\n",
    "    for i in range(len(G_scaled_weights)):\n",
    "        if i not in indices_to_remove:\n",
    "            rest_sizes = G_scaled_weights[i]\n",
    "            \n",
    "    nx.draw_networkx_nodes(graph, positions, nodelist=rest_nodes, node_color='#efbc2f', node_size=rest_sizes,edgecolors = 'black', alpha=1)\n",
    "    nx.draw_networkx_nodes(graph, positions, nodelist=min_nodes, node_color='#366447', node_size=min_sizes, edgecolors = 'black', alpha=1)\n",
    "    nx.draw_networkx_nodes(graph, positions, nodelist=max_nodes, node_color='#a6332e', node_size=max_sizes, edgecolors = 'black', alpha=1)\n",
    "    \n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    plt.figtext(.5, -0.05, f\"The size of a note indicates the scaled sum of its weights.\", ha=\"center\")\n",
    "    # Used to save the fig for the paper\n",
    "    plt.savefig('networkCombined.png', format='png', transparent=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eee5466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making graphs of book 1 where the no of senteces are changed\n",
    "\n",
    "graph_list = []\n",
    "graph_list.append(weighted_temporal_graphs(characters, \"B1/\", 5))\n",
    "graph_list.append(weighted_temporal_graphs(characters, \"B1/\", 10))\n",
    "graph_list.append(weighted_temporal_graphs(characters, \"B1/\", 20))\n",
    "graph_list.append(weighted_temporal_graphs(characters, \"B1/\", 30))\n",
    "graph_list.append(weighted_temporal_graphs(characters, \"B1/\", 40))\n",
    "graph_list.append(weighted_temporal_graphs(characters, \"B1/\", 50))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5f8de1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plotting the networks with different no of sentences\n",
    "sentence_len = [5, 10, 20, 30, 40, 50]\n",
    "\n",
    "for i, graph in enumerate(graph_list):\n",
    "    draw_network(graph, \"Book one network with interval of \"+str(sentence_len[i])+\" sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1d4d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making network separately for each book\n",
    "\n",
    "book_graphs = []\n",
    "\n",
    "book_graphs.append(weighted_temporal_graphs(characters, \"B1/\", 5))\n",
    "book_graphs.append(weighted_temporal_graphs(characters, \"B2/\", 5))\n",
    "book_graphs.append(weighted_temporal_graphs(characters, \"B3/\", 5))\n",
    "book_graphs.append(weighted_temporal_graphs(characters, \"B4/\", 5))\n",
    "book_graphs.append(weighted_temporal_graphs(characters, \"B5/\", 5))\n",
    "book_graphs.append(weighted_temporal_graphs(characters, \"B6/\", 5))\n",
    "book_graphs.append(weighted_temporal_graphs(characters, \"B7/\", 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a634d795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to create a figure for the paper\n",
    "draw_network(book_graphs[0], \"Network of book 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e500df",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Drawing network of each book\n",
    "for i, graph in enumerate(book_graphs):\n",
    "    draw_network(graph, \"Network of book \" + str(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5609f8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the networks of the books\n",
    "combined_nx = [book_graphs[0]]\n",
    "combined_nx.append(combine_graphs(combined_nx[0], book_graphs[1]))\n",
    "combined_nx.append(combine_graphs(combined_nx[1], book_graphs[2]))\n",
    "combined_nx.append(combine_graphs(combined_nx[2], book_graphs[3]))\n",
    "combined_nx.append(combine_graphs(combined_nx[3], book_graphs[4]))\n",
    "combined_nx.append(combine_graphs(combined_nx[4], book_graphs[5]))\n",
    "combined_nx.append(combine_graphs(combined_nx[5], book_graphs[6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9031cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to create a figure for the paper\n",
    "draw_network(combined_nx[6], \"Combined network of all books\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056f3012",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Drawing network of combined books\n",
    "for i, graph in enumerate(combined_nx):\n",
    "    title = ''\n",
    "    if i == 0:\n",
    "        title = \"Network of book 1\"\n",
    "    else:\n",
    "        title = \"Network of book 1-\" + str(i+1) \n",
    "    draw_network(graph, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db72e2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print number of nodes and edges in each combined network\n",
    "\n",
    "for i, graph in enumerate(combined_nx):\n",
    "    title = ''\n",
    "    if i == 0:\n",
    "        title = \" edges in the network of book 1\"\n",
    "    else:\n",
    "        title = \" edges in the network of book 1-\" + str(i+1) \n",
    "    print('There are ' +str(graph.number_of_nodes()) + ' nodes and ' + str(graph.number_of_edges()) + title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac87c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the character with biggest increase in percentage of its edges\n",
    "# Very hacky solution\n",
    "edge_increase = []\n",
    "startG = book_graphs[0]\n",
    "endG = combined_nx[6]\n",
    "for char in characters:\n",
    "    edgeStart = len(startG.edges(char[0]))\n",
    "    if edgeStart == 0:\n",
    "        edgeStart = len(book_graphs[1].edges(char[0]))\n",
    "        if edgeStart == 0:\n",
    "            edgeStart = len(book_graphs[2].edges(char[0]))\n",
    "            if edgeStart == 0:\n",
    "                edgeStart = len(book_graphs[3].edges(char[0]))\n",
    "                if edgeStart == 0:\n",
    "                    edgeStart = len(book_graphs[4].edges(char[0]))\n",
    "                    if edgeStart == 0:\n",
    "                        edgeStart = len(book_graphs[5].edges(char[0]))\n",
    "                        if edgeStart == 0:\n",
    "                            edgeStart = len(book_graphs[6].edges(char[0]))\n",
    "    if edgeStart == 0:\n",
    "        print(char[0])\n",
    "        continue\n",
    "    edgeEnd = len(endG.edges(char[0]))\n",
    "    increase = edgeEnd-edgeStart\n",
    "    percentage = increase*100/edgeStart\n",
    "    edge_increase += [(char[0], edgeStart, edgeEnd, percentage)]\n",
    "\n",
    "print(max(edge_increase,key=lambda item:item[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9854415b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to find info for the paper\n",
    "print(len(book_graphs[0].edges('Harry_Potter')))\n",
    "print(len(combined_nx[6].edges('Harry_Potter')))\n",
    "print(len(book_graphs[0].edges('Ronald_Weasley')))\n",
    "print(len(combined_nx[6].edges('Ronald_Weasley')))\n",
    "print(len(book_graphs[0].edges('Hermione_Granger')))\n",
    "print(len(combined_nx[6].edges('Hermione_Granger')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735863d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of the increase of edges and nodes throughout the books. With different y-axes in same plot. \n",
    "# Not used in paper as it was difficult to read\n",
    "no_nodes = []\n",
    "no_edges = []\n",
    "networks = [1, 2, 3, 4, 5, 6, 7]\n",
    "for graph in combined_nx:\n",
    "    no_nodes.append(graph.number_of_nodes())\n",
    "    no_edges.append(graph.number_of_edges())\n",
    "\n",
    "ax1 = plt.subplot()\n",
    "l1, = ax1.plot(networks, no_nodes, color='red')\n",
    "ax2 = ax1.twinx()\n",
    "l2, = ax2.plot(networks, no_edges, color='blue')\n",
    "ax1.set_xlabel('No. of books combined')\n",
    "ax1.tick_params(axis=\"y\", labelcolor='red')\n",
    "ax2.tick_params(axis=\"y\", labelcolor='blue')\n",
    "ax1.set_ylabel(\"No. of nodes\")\n",
    "ax2.set_ylabel(\"No. of edges\")\n",
    "\n",
    "plt.legend([l1, l2], [\"No. of nodes\", \"No. of edges\"])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1ffacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting number of edges belong to ten characters throughout the series:\n",
    "char = [\"Harry_Potter\", \"Ronald_Weasley\", \"Hermione_Granger\", \"Albus_Dumbledore\", \"Severus_Snape\", \"Tom_Riddle\", \n",
    "        \"Rubeus_Hagrid\", \"Draco_Malfoy\", \"Ginevra_Weasley\", \"Neville_Longbottom\"]\n",
    "font = 15\n",
    "no_edges = []\n",
    "networks = [1, 2, 3, 4, 5, 6, 7]\n",
    "plt.rcParams[\"figure.figsize\"] = (10,11)\n",
    "for c in char:\n",
    "    edges = []\n",
    "    for graph in combined_nx:\n",
    "        edges.append(len(list(graph.edges(c))))\n",
    "    no_edges.append(edges)\n",
    "\n",
    "colors = ['#a6332e', '#efbc2f', '#3c4e91', '#366447', '#aaaaaa', '#946b2d', 'orchid', '#d3a625', 'orangered', 'green']\n",
    "for i, e_list in enumerate(no_edges):\n",
    "    plt.plot(networks, e_list, label = char[i].replace('_', ' '), color = colors[i])\n",
    "plt.xlabel(\"No. of books combined\", fontsize = font)\n",
    "plt.ylabel(\"No. of edges\", fontsize = font)\n",
    "plt.legend(fontsize = font)\n",
    "plt.xticks(fontsize=font)\n",
    "plt.yticks(fontsize=font)\n",
    "plt.title(\"Evolution of the network of ten main characters\")\n",
    "plt.savefig('plotMainCharacters.png', format='png', transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28ed504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot showing how many nodes there are in each book and for the books combined\n",
    "no_bookNodes = []\n",
    "no_combinedNodes = []\n",
    "font = 20\n",
    "#no_edges = []\n",
    "networks = [1, 2, 3, 4, 5, 6, 7]\n",
    "for graph in combined_nx:\n",
    "    no_combinedNodes.append(graph.number_of_nodes())\n",
    "for graph in book_graphs:   \n",
    "    no_bookNodes.append(graph.number_of_nodes())\n",
    "    \n",
    "counts, bins = np.histogram(no_combinedNodes, bins = 7)\n",
    "\n",
    "plt.plot(networks, no_combinedNodes, color = '#efbc2f', alpha = 1, label = 'Combined books')\n",
    "plt.bar(networks, no_bookNodes, color = '#3c4e91', edgecolor = 'black', alpha = 0.8)\n",
    "plt.xlabel('Book no.', fontsize=font)\n",
    "plt.ylabel('Number of nodes', fontsize=font)\n",
    "plt.legend(fontsize = font)\n",
    "plt.xticks(fontsize=font)\n",
    "plt.yticks(fontsize=font)\n",
    "plt.savefig('plotNodes.png', format='png', transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00cf52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot showing how many edges there are in each book and for the books combined\n",
    "no_bookEdges = []\n",
    "no_combinedEdges = []\n",
    "#no_edges = []\n",
    "networks = [1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "font = 15\n",
    "for graph in combined_nx:\n",
    "    no_combinedEdges.append(graph.number_of_edges())\n",
    "for graph in book_graphs:   \n",
    "    no_bookEdges.append(graph.number_of_edges())\n",
    "\n",
    "plt.plot(networks, no_combinedEdges, color = '#366447', alpha = 1, label = 'Combined books')\n",
    "plt.bar(networks, no_bookEdges, color = '#a6332e', edgecolor = 'black', alpha = 0.8)\n",
    "plt.xlabel('Book no.', fontsize=font)\n",
    "plt.ylabel('Number of nodes', fontsize=font)\n",
    "plt.legend(fontsize = font)\n",
    "plt.xticks(fontsize=font)\n",
    "plt.yticks(fontsize=font)\n",
    "plt.savefig('plotEdges.png', format='png', transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d923af",
   "metadata": {},
   "source": [
    "## Communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62220943",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input: a graph to divide in communities\n",
    "Output: A list with the different communities\n",
    "\"\"\"\n",
    "\n",
    "def communities(graph):\n",
    "    partition = community_louvain.best_partition(graph)\n",
    "    #print(partition)\n",
    "    partition_list = []\n",
    "    \n",
    "    for com in set(partition.values()) :\n",
    "        list_nodes = [nodes for nodes in partition.keys()\n",
    "                                if partition[nodes] == com]\n",
    "        partition_list.append(list_nodes)\n",
    "    partition_list = sorted(partition_list, key=len, reverse=True)\n",
    "    #print(partition_list)\n",
    "    return partition_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e736cc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make dictionary wordclouds with the character names and their weights\n",
    "# equal attributes are summarized\n",
    "# Was used to try and make wordclouds with the names of \n",
    "#the characters in the communities and their corresponding attributes\n",
    "#But this did not end up as we wanted so not used in the paper\n",
    "\n",
    "\"\"\"\n",
    "Input: a community and the graph it is extracted from\n",
    "Output: A dictionary of all the characters in the community,\n",
    "        the parentages, houses and occupations belonging to the characters.\n",
    "        Each character gets a value according to their sum\n",
    "        Each parentage, house and accupation is summed up for the total no. of characters belonging\n",
    "        to that parentage, house or occupation.\n",
    "\"\"\"\n",
    "\n",
    "def wordcloud_dict(community, graph):\n",
    "    cloud_freq = {}   \n",
    "    subG = graph.subgraph(community)\n",
    "    nodes = list(subG.nodes)\n",
    "    weights = get_weight_sums(subG)\n",
    "    parentages = nx.get_node_attributes(subG, 'parentage')\n",
    "    houses = nx.get_node_attributes(subG, 'house')\n",
    "    occupations = nx.get_node_attributes(subG, 'occupation')\n",
    "    \n",
    "    \n",
    "    for character in community:\n",
    "        parentage = parentages[character]\n",
    "        house = houses[character]\n",
    "        occupation = occupations[character]\n",
    "        \n",
    "        cloud_freq[character.replace('_', ' ')] = weights[nodes.index(character)]\n",
    "        \n",
    "        if parentage != 'other':\n",
    "            if parentage in cloud_freq:\n",
    "                cloud_freq[parentage] = cloud_freq.get(parentage) + 1\n",
    "            else:\n",
    "                cloud_freq[parentage] = 1\n",
    "        \n",
    "        if house != 'other':\n",
    "            if house in cloud_freq:\n",
    "                cloud_freq[house] = cloud_freq.get(house) + 1\n",
    "            else:\n",
    "                cloud_freq[house] = 1\n",
    "        \n",
    "        if occupation != 'other':\n",
    "            if occupation in cloud_freq:\n",
    "                cloud_freq[occupation] = cloud_freq.get(occupation) + 1\n",
    "            else:\n",
    "                cloud_freq[occupation] = 1\n",
    "    \n",
    "    return cloud_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58508f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Was used to try and make wordclouds with the names of \n",
    "#the characters in the communities and their corresponding attributes\n",
    "#But this did not end up as we wanted so not used in the paper\n",
    "\n",
    "\"\"\"\n",
    "Input: a list of communities and the graph they're extracted from\n",
    "Output: a list with a dictionary for each community\n",
    "\"\"\"\n",
    "\n",
    "def make_com_dicts(com_list, graph):\n",
    "    com_dicts = []\n",
    "\n",
    "    for com in com_list:\n",
    "        com_dicts.append(wordcloud_dict(com, graph))\n",
    "    \n",
    "    return com_dicts\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce063ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Was used to try and make wordclouds with the names of \n",
    "#the characters in the communities and their corresponding attributes\n",
    "#But this did not end up as we wanted so not used in the paper\n",
    "\n",
    "\"\"\"\n",
    "Input: a list of dictionaries\n",
    "Output: Wordclouds plotted for the community dictionaries given\n",
    "\"\"\"\n",
    "def draw_word_cloud(dicts):\n",
    "    fig = plt.figure()\n",
    "    plt.rcParams['figure.figsize'] = [15, 20]\n",
    "\n",
    "    for i in range(len(dicts)):\n",
    "        ax = fig.add_subplot(5,2,i+1)\n",
    "        wordcloud = WordCloud(background_color='black', width=2200,\n",
    "                          height=1800, collocations=False, \n",
    "                              # Different colormaps https://matplotlib.org/stable/tutorials/colors/colormaps.html\n",
    "                          colormap = plt.get_cmap('hsv', max(partition.values()) + 30)\n",
    "                        ).generate_from_frequencies(dicts[i])\n",
    "\n",
    "        ax.imshow(wordcloud)\n",
    "        ax.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6df728d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Communitites from all books\n",
    "all_communities = communities(combined_nx[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb53750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# communities in each book:\n",
    "\n",
    "book_communities = []\n",
    "\n",
    "for graph in book_graphs:\n",
    "    book_communities.append(communities(graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95475a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print which book, the communities and the size of each community\n",
    "\n",
    "for i, com in enumerate(book_communities):\n",
    "    print('Book no.: ' + str(i+1))\n",
    "    print(com)\n",
    "    for part in com:\n",
    "        print(len(part))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0152fb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All those are not used. Was to see how it would turn out with using the names and attributes\n",
    "\n",
    "# Wordclouds for all books combined\n",
    "dictionary = make_com_dicts(all_communities, combined_nx[6])\n",
    "draw_word_cloud(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311cda5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wordclouds for book 1:\n",
    "dictionary1 = make_com_dicts(book_communities[0], book_graphs[0])\n",
    "draw_word_cloud(dictionary1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bdc490",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wordclouds for book 2:\n",
    "dictionary2 = make_com_dicts(book_communities[1], book_graphs[1])\n",
    "draw_word_cloud(dictionary2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fe300f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wordclouds for book 3:\n",
    "dictionary3 = make_com_dicts(book_communities[2], book_graphs[2])\n",
    "draw_word_cloud(dictionary3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4ffcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wordclouds for book 4:\n",
    "dictionary4 = make_com_dicts(book_communities[3], book_graphs[3])\n",
    "draw_word_cloud(dictionary4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8244e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wordclouds for book 5:\n",
    "dictionary5 = make_com_dicts(book_communities[4], book_graphs[4])\n",
    "draw_word_cloud(dictionary5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc093c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wordclouds for book 6:\n",
    "dictionary6 = make_com_dicts(book_communities[5], book_graphs[5])\n",
    "draw_word_cloud(dictionary6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f3b49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wordclouds for book 7:\n",
    "dictionary7 = make_com_dicts(book_communities[6], book_graphs[6])\n",
    "draw_word_cloud(dictionary7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c22042",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52a3f7b6",
   "metadata": {},
   "source": [
    "### Extracting texts from books belonging to communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec99f0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input: A list of communities.\n",
    "       the directory of the book,\n",
    "       how many sentences to read at a time\n",
    "Output: A list with the strings belonging to each community\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def extract_com_texts(com_list, book_dir, sentence_no):\n",
    "    community_texts = []\n",
    "    maxrange = 0\n",
    "    #if we have less than 10 communities\n",
    "    if len(com_list) < 10:\n",
    "        maxrange = len(com_list)\n",
    "    else:\n",
    "        maxrange = 10\n",
    "\n",
    "\n",
    "    for community in com_list[:maxrange]:\n",
    "        \n",
    "        com_txt = []        \n",
    "        for chap in os.listdir(book_dir):\n",
    "            #if chap == 'replaced_ch1.txt':\n",
    "            with open(book_dir + chap) as f:\n",
    "                    text = f.read()\n",
    "\n",
    "            sentences = text.split(\". \")\n",
    "            count_start = 0\n",
    "            count_end = sentence_no\n",
    "            add_text_start = []\n",
    "            \n",
    "            while (count_start < len(sentences)):\n",
    "                    current = sentences[count_start:count_end]\n",
    "                    current = \" \".join(current)\n",
    "                    for char in community:\n",
    "                        if char in current:\n",
    "                            # If we haven't already added the textpiece to this community, then add it\n",
    "                            # To make sure that we won't get the same textpiece several times in one community\n",
    "                            if not count_start in add_text_start:\n",
    "                                tokens = word_tokenize(current)\n",
    "                                com_txt = com_txt + tokens\n",
    "                                add_text_start.append(count_start)                                      \n",
    "                    count_start = count_end\n",
    "                    count_end += sentence_no\n",
    "                    \n",
    "        community_texts.append(com_txt)\n",
    "\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "    community_strings = []\n",
    "    for txt in community_texts:\n",
    "        com_words = [w for w in txt if w not in stopwords and len(w)>1]\n",
    "        community_strings.append(com_words)\n",
    "\n",
    "    return community_strings\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e225f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the strings for the communities in each book\n",
    "com_strings1 = extract_com_texts(book_communities[0], './B1/', 5)\n",
    "com_strings2 = extract_com_texts(book_communities[1], './B2/', 5)\n",
    "com_strings3 = extract_com_texts(book_communities[2], './B3/', 5)\n",
    "com_strings4 = extract_com_texts(book_communities[3], './B4/', 5)\n",
    "com_strings5 = extract_com_texts(book_communities[4], './B5/', 5)\n",
    "com_strings6 = extract_com_texts(book_communities[5], './B6/', 5)\n",
    "com_strings7 = extract_com_texts(book_communities[6], './B7/', 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b05297",
   "metadata": {},
   "source": [
    "### Extract wikitext for each community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa33394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to find the wikitexts belonging to each character in a community \n",
    "# Not used as we went with the text from the books instead\n",
    "def extract_com_wikitexts(com_list, directory):\n",
    "    community_texts = []\n",
    "    maxrange = 0\n",
    "    #if we have less than 10 communities\n",
    "    if len(com_list) < 10:\n",
    "        maxrange = len(com_list)\n",
    "    else:\n",
    "        maxrange = 10\n",
    "\n",
    "\n",
    "    for community in com_list[:maxrange]:\n",
    "        \n",
    "        com_txt = []  \n",
    "        \n",
    "        for char in community:\n",
    "            with open(directory + 'clean_' + char + '.txt') as f:\n",
    "                    text = f.read()\n",
    "            \n",
    "            tokens = nltk.word_tokenize(BeautifulSoup(text, 'html.parser').get_text())\n",
    "            #tokens = word_tokenize(current)\n",
    "            #file_text = [w.lower() for w in tokens if w.isalpha()]\n",
    "            com_txt = com_txt + tokens\n",
    "            \n",
    "        community_texts.append(com_txt)\n",
    "\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "    community_strings = []\n",
    "    for txt in community_texts:\n",
    "        com_words = [w for w in txt if w.lower() not in stopwords and len(w)>2]\n",
    "        community_strings.append(com_words)\n",
    "\n",
    "    return community_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202cb6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "com_wikistrings1 = extract_com_wikitexts(book_communities[0], './characters/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70c3b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input: List of community strings\n",
    "Output: A list with the unique terms for each community\n",
    "\"\"\"\n",
    "def unique(com_str):\n",
    "    unique_terms = []\n",
    "    for community_words in com_str:\n",
    "        unique_terms.append(list(set(community_words)))\n",
    "    \n",
    "    return unique_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f75fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_terms1 = unique(com_strings1)\n",
    "unique_terms2 = unique(com_strings2)\n",
    "unique_terms3 = unique(com_strings3)\n",
    "unique_terms4 = unique(com_strings4)\n",
    "unique_terms5 = unique(com_strings5)\n",
    "unique_terms6 = unique(com_strings6)\n",
    "unique_terms7 = unique(com_strings7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c71fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_wikiterms1 = unique(com_wikistrings1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973e4c5d",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5358bbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input: A word,\n",
    "       a list with the uniqe term for each community\n",
    "Output: The IDF value foudn for the word\n",
    "\"\"\"\n",
    "\n",
    "def idf(word, unique_list):\n",
    "    N = len(unique_list)\n",
    "    term_appears = 0\n",
    "    for sublist in unique_list:\n",
    "        if word in sublist:\n",
    "            term_appears+=1\n",
    "    idf_val = math.log(N/(1+term_appears))+1\n",
    "    return idf_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c2bf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input: A list with the community string,\n",
    "       a list with the uniqe terms for each community\n",
    "Output: A list containg the words and their TF-IDF values for each community\n",
    "\"\"\"\n",
    "\n",
    "def tfidf(community_str, unique_words):\n",
    "    tfidf_list = []\n",
    "\n",
    "    for community_words in community_str:\n",
    "        fdist = FreqDist(community_words)\n",
    "        total_terms = len(community_words)\n",
    "        tfidf=[]\n",
    "        for word in fdist:\n",
    "            idf_val = idf(word, unique_words)\n",
    "            tf_val = fdist[word]/total_terms\n",
    "            tfidf_elem=(word, tf_val*idf_val)\n",
    "            tfidf.append(tfidf_elem)\n",
    "        tfidf_list.append(tfidf)\n",
    "        \n",
    "    return tfidf_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58a7cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the tfidf list for each book\n",
    "tfidf_list1 = tfidf(com_strings1, unique_terms1)\n",
    "tfidf_list2 = tfidf(com_strings2, unique_terms2)\n",
    "tfidf_list3 = tfidf(com_strings3, unique_terms3)\n",
    "tfidf_list4 = tfidf(com_strings4, unique_terms4)\n",
    "tfidf_list5 = tfidf(com_strings5, unique_terms5)\n",
    "tfidf_list6 = tfidf(com_strings6, unique_terms6)\n",
    "tfidf_list7 = tfidf(com_strings7, unique_terms7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cad88c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_wikilist1 = tfidf(com_wikistrings1, unique_wikiterms1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed1a9a4",
   "metadata": {},
   "source": [
    "## Wordclouds for communities with book text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b4e888",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input: A list with the words and their TF-IDF values for each community\n",
    "Output: A wordcloud plot for each community with their corresponding words\n",
    "\"\"\"\n",
    "\n",
    "def wordCloud(tfidf_list):\n",
    "    fig = plt.figure()\n",
    "    plt.rcParams['figure.figsize'] = [15, 20]\n",
    "\n",
    "    for i in range(len(tfidf_list)):\n",
    "        ax = fig.add_subplot(5,2,i+1)\n",
    "        wordcloud = WordCloud(background_color='white', width=2200,\n",
    "                          height=1800, collocations=False).generate_from_frequencies(dict(tfidf_list[i]))\n",
    "\n",
    "        ax.imshow(wordcloud)\n",
    "        ax.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56326d20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wordCloud(tfidf_list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce17439",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCloud(tfidf_list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064ffc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCloud(tfidf_list3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763a1a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCloud(tfidf_list4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf92cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCloud(tfidf_list5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af4fead",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCloud(tfidf_list6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6292a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCloud(tfidf_list7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4aa4ea",
   "metadata": {},
   "source": [
    "## Wordclouds for communities with wiki text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a377e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wordCloud(tfidf_wikilist1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
