{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09732d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import csv\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import networkx as nx\n",
    "from networkx.algorithms import community\n",
    "\n",
    "from nltk import word_tokenize, FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from community import community_louvain\n",
    "from fa2 import ForceAtlas2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21a0c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create character tuple list\n",
    "\n",
    "characters = []\n",
    "\n",
    "with open(\"HP_characters.csv\", \"r\", encoding=\"utf8\") as sent_file:\n",
    "    csv_file = csv.reader(sent_file, delimiter=\",\")\n",
    "    for row in csv_file:\n",
    "        name = row[0].replace(' ', '_')\n",
    "        # lower to make sure they are spelled the same way\n",
    "        parentage = row[1].lower()\n",
    "        house = row[2].lower()\n",
    "        occupation = row[3].lower()\n",
    "        loyalty = row[4]\n",
    "        characters.append((name, parentage, house, occupation, loyalty))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cde76f5",
   "metadata": {},
   "source": [
    "# Graph with weights\n",
    "Making a graphs for each book. The nodes are the characters in the book, and edges are between characters that are in the same chapter. Edges have weight corresponding to the number of times those two characters are in the same chapter. Nodes have the attributes parentage, house and occupation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a8b592",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input: A list of character names with their attributes, \n",
    "       the path of the book,\n",
    "       how many sentences to look at at time\n",
    "Output: A weighted graph\n",
    "\"\"\"\n",
    "def weighted_temporal_graphs(character_list, path, sentence_no):\n",
    "    \n",
    "    G = nx.Graph()\n",
    "  \n",
    "    # Go throug each chapter in the book\n",
    "    for chapter in os.listdir(path): \n",
    "        #only look at the files where aliases have been replaced with character names\n",
    "        if \"replaced\" in chapter:   \n",
    "\n",
    "            # Get text\n",
    "            with open(path + chapter) as f:\n",
    "                text = f.read()\n",
    "            \n",
    "            # Put all characters from the chapter in the graph if they are not already there\n",
    "            for character in character_list:\n",
    "                if character[0] in text and character[0] not in list(G.nodes):\n",
    "                    G.add_node(character[0], parentage = character[1], \n",
    "                               house = character[2], occupation = character[3], loyalty = character[4])\n",
    "            \n",
    "            # Split the text in sentences \n",
    "            sentences = text.split(\". \")\n",
    "            count_start = 0\n",
    "            count_end = sentence_no\n",
    "            \n",
    "            # Look at specified amount of senteces at a time\n",
    "            while (count_start < len(sentences)):\n",
    "                current = sentences[count_start:count_end]\n",
    "                current = \" \".join(current)\n",
    "                \n",
    "                # Go through the nodes and check if two diffferent nodes appear in the same text piece\n",
    "                # if so add an edge\n",
    "                # weight is the amount of times they appear together throughout the book\n",
    "                for character_source in list(G.nodes):\n",
    "                    #print(character_source)\n",
    "                    for character_target in list(G.nodes):\n",
    "                        #print(character_target)\n",
    "                        if character_source is character_target:\n",
    "                            continue\n",
    "                        elif (character_source in current and character_target in current):\n",
    "                            if G.has_edge(character_source, character_target):\n",
    "                                G[character_source][character_target]['weight'] += 1\n",
    "                            else:\n",
    "                                G.add_edge(character_source, character_target, weight=1)\n",
    "                            #print(\"added some edge\")\n",
    "                count_start = count_end\n",
    "                count_end += sentence_no\n",
    "    # Remove nodes without edges\n",
    "    print(list(nx.isolates(G)))\n",
    "    G.remove_nodes_from(list(nx.isolates(G)))\n",
    "    #print(\"Done with graph\")\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19383129",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.operators.binary.compose.html#networkx.algorithms.operators.binary.compose\n",
    "\n",
    "\"\"\"\n",
    "Input: two graphs to combine\n",
    "Output: the combined graph, including the weights of the edges added together if same edges\n",
    "\"\"\"\n",
    "\n",
    "def combine_graphs(g1, g2):\n",
    "    combined= nx.compose(g1, g2)\n",
    "    edge_data = {e: g1.edges[e]['weight'] + g2.edges[e]['weight'] \n",
    "                 for e in g1.edges & g2.edges}\n",
    "    nx.set_edge_attributes(combined, edge_data, 'weight')\n",
    "    \n",
    "    return combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e4f1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Input: a graph\n",
    "Output: A list of of the summed weights for the edges for each node,\n",
    "        this list is ordered as the list of nodes returned from graph.nodes\n",
    "\"\"\"\n",
    "def get_weight_sums(graph):\n",
    "    weight_sums = []\n",
    "    for node in list(graph.nodes):\n",
    "        sum = 0\n",
    "        for source, target in list(graph.edges):\n",
    "            if node is source or node is target:\n",
    "                sum += graph[source][target][\"weight\"]\n",
    "        weight_sums.append(sum)\n",
    "    return weight_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67e64cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86f9899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/5294955/how-to-scale-down-a-range-of-numbers-with-a-known-min-and-max-value\n",
    "# a = minmum value for scaled weights\n",
    "# b = maximum value for scaled weights\n",
    "def scaled_weights(a, b, G):\n",
    "    weights = get_weight_sums(G)\n",
    "    max_weight = max(weights)\n",
    "    min_weight = min(weights)\n",
    "    scaled = []\n",
    "    for w in weights:\n",
    "        scaled.append(((b - a) * (w - min_weight) // (max_weight - min_weight)) + a)\n",
    "    return scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c009a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/13070461/get-indices-of-the-top-n-values-of-a-list\n",
    "# Find the n nodes with highest and lowest weights\n",
    "import numpy as np\n",
    "# n = number of nodes from top or bottom\n",
    "# f = flag, True = top , False = bottom\n",
    "# returns a list of names of the n nodes, and a list of node sizes as a tuple\n",
    "# and a list of the indices\n",
    "def get_nodes_extreme(G, n, f):\n",
    "    if f:\n",
    "        n_indices = np.argsort(get_weight_sums(G))[-n:]\n",
    "    else:\n",
    "        n_indices = np.argsort(get_weight_sums(G))[0:n]\n",
    "    sc_weights = scaled_weights(50, 800, G)\n",
    "    names = []\n",
    "    weights = []\n",
    "    indices = []\n",
    "    for i in n_indices:\n",
    "        names.append(list(G.nodes())[i])\n",
    "        weights.append(sc_weights[i])\n",
    "        indices.append(i)\n",
    "    return (names, weights, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93dd8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_green = \"#12D82E\" # Green\n",
    "my_red = \"#D82E12\" # Red\n",
    "my_blue = \"#2E12D8\" # Blue\n",
    "\n",
    "\n",
    "community_colors = [\"#B6F20D\", \"#0DF2BC\", \"#490DF2\", \"#F20D43\", \"#13EC33\", \"#1360EC\", \"#EC13CC\", \"#EC9F13\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b1739c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input: a graph,\n",
    "       title for the plot, default is empty string\n",
    "Output: plots the graph using forceAtlas\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def draw_network(graph, title=\"\"):\n",
    "    # Adjusting figure size\n",
    "    plt.rcParams['figure.figsize'] = [10, 10]\n",
    "\n",
    "    forceatlas2 = ForceAtlas2(\n",
    "                            # Behavior alternatives\n",
    "                            outboundAttractionDistribution=True,  # Dissuade hubs\n",
    "                            linLogMode=False,  # NOT IMPLEMENTED\n",
    "                            adjustSizes=False,  # Prevent overlap (NOT IMPLEMENTED)\n",
    "                            edgeWeightInfluence=1.0,\n",
    "\n",
    "                            # Performance\n",
    "                            jitterTolerance=1.0,  # Tolerance\n",
    "                            barnesHutOptimize=True,\n",
    "                            barnesHutTheta=1.2, # original 1.2\n",
    "                            multiThreaded=False,  # NOT IMPLEMENTED\n",
    "\n",
    "                            # Tuning\n",
    "                            scalingRatio=2.0,\n",
    "                            strongGravityMode=True,\n",
    "                            gravity=0.1, # original 0.5\n",
    "\n",
    "                            # Log\n",
    "                            verbose=True)\n",
    "\n",
    "    positions = forceatlas2.forceatlas2_networkx_layout(graph, pos=None, iterations=2000)\n",
    "    nx.draw_networkx_edges(graph, positions, edge_color=\"black\", alpha=0.1)\n",
    "    # Picking colors based on: https://blog.datawrapper.de/beautifulcolors/\n",
    "    # Using triadic here: https://www.canva.com/colors/color-wheel/\n",
    "    # defined in cell above\n",
    "    rest_color = my_green # Green\n",
    "    min_color = my_red # Red\n",
    "    max_color = my_blue # Blue\n",
    "    \n",
    "    # Making 3 lists: top n max_weights, bottom n min_weights, rest\n",
    "    max_nodes, max_sizes, max_indices = get_nodes_extreme(graph, 10, True)\n",
    "    min_nodes, min_sizes, min_indices = get_nodes_extreme(graph, 10, False)\n",
    "    \n",
    "    rest_nodes = [n for n in list(graph.nodes()) if n not in max_nodes and n not in min_nodes]\n",
    "    rest_sizes = []\n",
    "    indices_to_remove = max_indices + min_indices\n",
    "    G_scaled_weights = scaled_weights(50, 800, graph)\n",
    "    for i in range(len(G_scaled_weights)):\n",
    "        if i not in indices_to_remove:\n",
    "            rest_sizes = G_scaled_weights[i]\n",
    "            \n",
    "    # Testing using scaled_weights for node_size, first number = smallest size, last number = largest size\n",
    "    # Scaling hardcoded, since it has to be called to find the sizes of the max/min nodes\n",
    "    nx.draw_networkx_nodes(graph, positions, nodelist=rest_nodes, node_color=rest_color, node_size=rest_sizes, alpha=1)\n",
    "    nx.draw_networkx_nodes(graph, positions, nodelist=min_nodes, node_color=min_color, node_size=min_sizes, alpha=1)\n",
    "    nx.draw_networkx_nodes(graph, positions, nodelist=max_nodes, node_color=max_color, node_size=max_sizes, alpha=1)\n",
    "    \n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    plt.figtext(.5, -0.05, f\"The size of a note indicates the sum of its weights.\", ha=\"center\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eee5466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making graphs of book 1 where the no of senteces are changed\n",
    "\n",
    "graph_list = []\n",
    "#(character_list, path, graph_list, sentence_no):\n",
    "graph_list.append(weighted_temporal_graphs(characters, \"B1/\", 5))\n",
    "graph_list.append(weighted_temporal_graphs(characters, \"B1/\", 10))\n",
    "graph_list.append(weighted_temporal_graphs(characters, \"B1/\", 20))\n",
    "graph_list.append(weighted_temporal_graphs(characters, \"B1/\", 30))\n",
    "graph_list.append(weighted_temporal_graphs(characters, \"B1/\", 40))\n",
    "graph_list.append(weighted_temporal_graphs(characters, \"B1/\", 50))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5f8de1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plotting the networks with different no of sentences\n",
    "sentence_len = [5, 10, 20, 30, 40, 50]\n",
    "\n",
    "for i, graph in enumerate(graph_list):\n",
    "    draw_network(graph, \"Book one network with interval of \"+str(sentence_len[i])+\" sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1d4d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making network separately for each book\n",
    "\n",
    "book_graphs = []\n",
    "\n",
    "book_graphs.append(weighted_temporal_graphs(characters, \"B1/\", 5))\n",
    "book_graphs.append(weighted_temporal_graphs(characters, \"B2/\", 5))\n",
    "book_graphs.append(weighted_temporal_graphs(characters, \"B3/\", 5))\n",
    "book_graphs.append(weighted_temporal_graphs(characters, \"B4/\", 5))\n",
    "book_graphs.append(weighted_temporal_graphs(characters, \"B5/\", 5))\n",
    "book_graphs.append(weighted_temporal_graphs(characters, \"B6/\", 5))\n",
    "book_graphs.append(weighted_temporal_graphs(characters, \"B7/\", 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a634d795",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e500df",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Drawing network of each book\n",
    "for i, graph in enumerate(book_graphs):\n",
    "    draw_network(graph, \"Network of book \" + str(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5609f8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the networks of the books\n",
    "combined_nx = [book_graphs[0]]\n",
    "combined_nx.append(combine_graphs(combined_nx[0], book_graphs[1]))\n",
    "combined_nx.append(combine_graphs(combined_nx[1], book_graphs[2]))\n",
    "combined_nx.append(combine_graphs(combined_nx[2], book_graphs[3]))\n",
    "combined_nx.append(combine_graphs(combined_nx[3], book_graphs[4]))\n",
    "combined_nx.append(combine_graphs(combined_nx[4], book_graphs[5]))\n",
    "combined_nx.append(combine_graphs(combined_nx[5], book_graphs[6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056f3012",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Drawing network of combined books\n",
    "for i, graph in enumerate(combined_nx):\n",
    "    title = ''\n",
    "    if i == 0:\n",
    "        title = \"Network of book 1\"\n",
    "    else:\n",
    "        title = \"Network of book 1-\" + str(i+1) \n",
    "    draw_network(graph, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eda84b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_temporal_graphs_by_chapter(character_list, path, sentence_no):\n",
    "    \n",
    "    list_of_graphs = []\n",
    "  \n",
    "    # Go throug each chapter in the book\n",
    "    for chapter in os.listdir(path): \n",
    "        #only look at the files where aliases have been replaced with character names\n",
    "        if \"replaced\" in chapter: \n",
    "            G = nx.Graph()\n",
    "\n",
    "            # Get text\n",
    "            with open(path + chapter) as f:\n",
    "                text = f.read()\n",
    "            \n",
    "            # Put all characters from the chapter in the graph if they are not already there\n",
    "            for character in character_list:\n",
    "                if character[0] in text and character[0] not in list(G.nodes):\n",
    "                    G.add_node(character[0], parentage = character[1], \n",
    "                               house = character[2], occupation = character[3], loyalty = character[4])\n",
    "            \n",
    "            # Split the text in sentences \n",
    "            sentences = text.split(\". \")\n",
    "            count_start = 0\n",
    "            count_end = sentence_no\n",
    "            \n",
    "            # Look at specified amount of senteces at a time\n",
    "            while (count_start < len(sentences)):\n",
    "                current = sentences[count_start:count_end]\n",
    "                current = \" \".join(current)\n",
    "                \n",
    "                # Go through the nodes and check if two diffferent nodes appear in the same text piece\n",
    "                # if so add an edge\n",
    "                # weight is the amount of times they appear together throughout the book\n",
    "                for character_source in list(G.nodes):\n",
    "                    #print(character_source)\n",
    "                    for character_target in list(G.nodes):\n",
    "                        #print(character_target)\n",
    "                        if character_source is character_target:\n",
    "                            continue\n",
    "                        elif (character_source in current and character_target in current):\n",
    "                            if G.has_edge(character_source, character_target):\n",
    "                                G[character_source][character_target]['weight'] += 1\n",
    "                            else:\n",
    "                                G.add_edge(character_source, character_target, weight=1)\n",
    "                            #print(\"added some edge\")\n",
    "                count_start = count_end\n",
    "                count_end += sentence_no\n",
    "        # Remove nodes without edges\n",
    "        #print(list(nx.isolates(G)))\n",
    "        G.remove_nodes_from(list(nx.isolates(G)))\n",
    "        #print(\"Done with graph\")\n",
    "        list_of_graphs.append(G)\n",
    "    return list_of_graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271454d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print number of nodes and edges for each chapter of book 1\n",
    "for i, graph in enumerate(weighted_temporal_graphs_by_chapter(characters, \"B1/\", 5)):\n",
    "    title = ''\n",
    "    if i == 0:\n",
    "        title = \" edges in the network of book 1\"\n",
    "    else:\n",
    "        title = \" edges in the network of book 1-\" + str(i+1) \n",
    "    print('There are ' +str(graph.number_of_nodes()) + ' nodes and ' + str(graph.number_of_edges()) + title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ead954",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (5,2.5)\n",
    "# Plot showing number of nodes/edges for each chapter of book 1\n",
    "no_nodes = []\n",
    "no_edges = []\n",
    "networks = [n for n in range(1,len(os.listdir(\"B1\"))+1)]\n",
    "for graph in weighted_temporal_graphs_by_chapter(characters, \"B1/\", 5):\n",
    "    no_nodes.append(graph.number_of_nodes())\n",
    "    no_edges.append(graph.number_of_edges())\n",
    "\n",
    "ax1 = plt.subplot()\n",
    "l1, = ax1.plot(networks, no_nodes, color=my_red)\n",
    "ax2 = ax1.twinx()\n",
    "l2, = ax2.plot(networks, no_edges, color=my_blue)\n",
    "ax1.set_xlabel('Chapter')\n",
    "ax1.tick_params(axis=\"y\", labelcolor=my_red)\n",
    "ax2.tick_params(axis=\"y\", labelcolor=my_blue)\n",
    "ax1.set_ylabel(\"No. of nodes\")\n",
    "ax2.set_ylabel(\"No. of edges\")\n",
    "\n",
    "#plt.legend([l1, l2], [\"No. of nodes\", \"No. of edges\"])\n",
    "\n",
    "plt.show()\n",
    "plt.rcParams[\"figure.figsize\"] = plt.rcParamsDefault[\"figure.figsize\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1aaa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_graphs = []\n",
    "for book in [\"B1/\", \"B2/\", \"B3/\", \"B4/\", \"B5/\", \"B6/\", \"B7/\"]:\n",
    "    book_graphs.append(weighted_temporal_graphs(characters, book, 5))\n",
    "    \n",
    "print(len(book_graphs))\n",
    "    \n",
    "plt.rcParams[\"figure.figsize\"] = (5,2.5)\n",
    "# Plot showing number of nodes/edges for each chapter of book 1\n",
    "no_nodes = []\n",
    "no_edges = []\n",
    "networks = [1, 2, 3, 4, 5, 6, 7]\n",
    "for graph in book_graphs:\n",
    "    no_nodes.append(graph.number_of_nodes())\n",
    "    no_edges.append(graph.number_of_edges())\n",
    "\n",
    "ax1 = plt.subplot()\n",
    "l1, = ax1.plot(networks, no_nodes, color=my_red)\n",
    "ax2 = ax1.twinx()\n",
    "l2, = ax2.plot(networks, no_edges, color=my_blue)\n",
    "ax1.set_xlabel('Book')\n",
    "ax1.tick_params(axis=\"y\", labelcolor=my_red)\n",
    "ax2.tick_params(axis=\"y\", labelcolor=my_blue)\n",
    "ax1.set_ylabel(\"No. of nodes\")\n",
    "ax2.set_ylabel(\"No. of edges\")\n",
    "\n",
    "#plt.legend([l1, l2], [\"No. of nodes\", \"No. of edges\"])\n",
    "\n",
    "plt.show()\n",
    "plt.rcParams[\"figure.figsize\"] = plt.rcParamsDefault[\"figure.figsize\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db72e2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print number of nodes and edges in each combined network\n",
    "\n",
    "for i, graph in enumerate(combined_nx):\n",
    "    title = ''\n",
    "    if i == 0:\n",
    "        title = \" edges in the network of book 1\"\n",
    "    else:\n",
    "        title = \" edges in the network of book 1-\" + str(i+1) \n",
    "    print('There are ' +str(graph.number_of_nodes()) + ' nodes and ' + str(graph.number_of_edges()) + title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735863d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting on making a plot to show how nodes and edges increase throughout the book series\n",
    "no_nodes = []\n",
    "no_edges = []\n",
    "networks = [1, 2, 3, 4, 5, 6, 7]\n",
    "for graph in combined_nx:\n",
    "    no_nodes.append(graph.number_of_nodes())\n",
    "    no_edges.append(graph.number_of_edges())\n",
    "\n",
    "ax1 = plt.subplot()\n",
    "l1, = ax1.plot(networks, no_nodes, color='red')\n",
    "ax2 = ax1.twinx()\n",
    "l2, = ax2.plot(networks, no_edges, color='blue')\n",
    "ax1.set_xlabel('No. of books combined')\n",
    "ax1.tick_params(axis=\"y\", labelcolor='red')\n",
    "ax2.tick_params(axis=\"y\", labelcolor='blue')\n",
    "ax1.set_ylabel(\"No. of nodes\")\n",
    "ax2.set_ylabel(\"No. of edges\")\n",
    "\n",
    "plt.legend([l1, l2], [\"No. of nodes\", \"No. of edges\"])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1ffacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting number of edges belong to ten characters throughout the series:\n",
    "char = [\"Harry_Potter\", \"Ronald_Weasley\", \"Hermione_Granger\", \"Albus_Dumbledore\", \"Severus_Snape\", \"Tom_Riddle\", \n",
    "        \"Rubeus_Hagrid\", \"Draco_Malfoy\", \"Ginevra_Weasley\", \"Neville_Longbottom\"]\n",
    "\n",
    "no_edges = []\n",
    "networks = [1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "for c in char:\n",
    "    edges = []\n",
    "    for graph in combined_nx:\n",
    "        edges.append(len(list(graph.edges(c))))\n",
    "    no_edges.append(edges)\n",
    "\n",
    "for i, e_list in enumerate(no_edges):\n",
    "    plt.plot(networks, e_list, label = char[i].replace('_', ' '))\n",
    "plt.xlabel(\"No. of books combined\")\n",
    "plt.ylabel(\"No. of edges\")\n",
    "plt.title(\"Evolution of the network of ten main characters\")\n",
    "plt.legend(fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d923af",
   "metadata": {},
   "source": [
    "## Communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62220943",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input: a graph to divide in communities\n",
    "Output: A list with the different communities\n",
    "\"\"\"\n",
    "\n",
    "def communities(graph):\n",
    "    partition = community_louvain.best_partition(graph)\n",
    "    #print(partition)\n",
    "    partition_list = []\n",
    "    \n",
    "    for com in set(partition.values()) :\n",
    "        list_nodes = [nodes for nodes in partition.keys()\n",
    "                                if partition[nodes] == com]\n",
    "        partition_list.append(list_nodes)\n",
    "    partition_list = sorted(partition_list, key=len, reverse=True)\n",
    "    #print(partition_list)\n",
    "    return partition_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e736cc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input: a community and the graph it is extracted from\n",
    "Output: A dictionary of all the characters in the community,\n",
    "        the parentages, houses and occupations belonging to the characters.\n",
    "        Each character gets a value according to their sum\n",
    "        Each parentage, house and accupation is summed up for the total no. of characters belonging\n",
    "        to that parentage, house or occupation.\n",
    "\"\"\"\n",
    "\n",
    "#Make dictionary wordclouds with the character names and their weights\n",
    "# equal attributes are summarized\n",
    "\n",
    "def wordcloud_dict(community, graph):\n",
    "    cloud_freq = {}   \n",
    "    subG = graph.subgraph(community)\n",
    "    nodes = list(subG.nodes)\n",
    "    weights = get_weight_sums(subG)\n",
    "    parentages = nx.get_node_attributes(subG, 'parentage')\n",
    "    houses = nx.get_node_attributes(subG, 'house')\n",
    "    occupations = nx.get_node_attributes(subG, 'occupation')\n",
    "    \n",
    "    \n",
    "    for character in community:\n",
    "        parentage = parentages[character]\n",
    "        house = houses[character]\n",
    "        occupation = occupations[character]\n",
    "        \n",
    "        cloud_freq[character.replace('_', ' ')] = weights[nodes.index(character)]\n",
    "        \n",
    "        if parentage != 'other':\n",
    "            if parentage in cloud_freq:\n",
    "                cloud_freq[parentage] = cloud_freq.get(parentage) + 1\n",
    "            else:\n",
    "                cloud_freq[parentage] = 1\n",
    "        \n",
    "        if house != 'other':\n",
    "            if house in cloud_freq:\n",
    "                cloud_freq[house] = cloud_freq.get(house) + 1\n",
    "            else:\n",
    "                cloud_freq[house] = 1\n",
    "        \n",
    "        if occupation != 'other':\n",
    "            if occupation in cloud_freq:\n",
    "                cloud_freq[occupation] = cloud_freq.get(occupation) + 1\n",
    "            else:\n",
    "                cloud_freq[occupation] = 1\n",
    "    \n",
    "    return cloud_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58508f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input: a list of communities and the graph they're extracted from\n",
    "Output: a list with a dictionary for each community\n",
    "\"\"\"\n",
    "\n",
    "def make_com_dicts(com_list, graph):\n",
    "    com_dicts = []\n",
    "\n",
    "    for com in com_list:\n",
    "        com_dicts.append(wordcloud_dict(com, graph))\n",
    "    \n",
    "    return com_dicts\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce063ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input: a list of dictionaries\n",
    "Output: Wordclouds plotted for the community dictionaries given\n",
    "\"\"\"\n",
    "def draw_word_cloud(dicts):\n",
    "    fig = plt.figure()\n",
    "    plt.rcParams['figure.figsize'] = [15, 20]\n",
    "\n",
    "    for i in range(len(dicts)):\n",
    "        ax = fig.add_subplot(5,2,i+1)\n",
    "        wordcloud = WordCloud(background_color='black', width=2200,\n",
    "                          height=1800, collocations=False, \n",
    "                              # Different colormaps https://matplotlib.org/stable/tutorials/colors/colormaps.html\n",
    "                          colormap = plt.get_cmap('hsv', max(partition.values()) + 30)\n",
    "                        ).generate_from_frequencies(dicts[i])\n",
    "\n",
    "        ax.imshow(wordcloud)\n",
    "        ax.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6df728d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Communitites from all books\n",
    "all_communities = communities(combined_nx[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a17524d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_communities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb53750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# communities in each book:\n",
    "\n",
    "book_communities = []\n",
    "\n",
    "for graph in book_graphs:\n",
    "    book_communities.append(communities(graph))\n",
    "    \n",
    "#print(len(book_communities))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e87f2fd",
   "metadata": {},
   "source": [
    "### Kode til at plotte communites fra grafer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933077c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coloring communites in graph:\n",
    "# book 1\n",
    "\n",
    "# Adjusting figure size\n",
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "\n",
    "partition = community_louvain.best_partition(book_graphs[0])\n",
    "#drawing using ForceAtlas\n",
    "forceatlas2 = ForceAtlas2(\n",
    "                        # Behavior alternatives\n",
    "                        outboundAttractionDistribution=False,  # Dissuade hubs\n",
    "                        linLogMode=False,  # NOT IMPLEMENTED\n",
    "                        adjustSizes=False,  # Prevent overlap (NOT IMPLEMENTED)\n",
    "                        edgeWeightInfluence=0.1,\n",
    "\n",
    "                        # Performance\n",
    "                        jitterTolerance=0.5,  # Tolerance\n",
    "                        barnesHutOptimize=True,\n",
    "                        barnesHutTheta=0.5, # original 0.5\n",
    "                        multiThreaded=False,  # NOT IMPLEMENTED\n",
    "\n",
    "                        # Tuning\n",
    "                        scalingRatio=10.0,\n",
    "                        strongGravityMode=True,\n",
    "                        gravity=0.1,\n",
    "\n",
    "                        # Log\n",
    "                        verbose=True)\n",
    "\n",
    "positions = forceatlas2.forceatlas2_networkx_layout(book_graphs[0], pos=None, iterations=2000)\n",
    "cmap = plt.get_cmap('hsv', max(partition.values()) + 30)\n",
    "\n",
    "nx.draw_networkx_nodes(book_graphs[0], positions, partition.keys(), node_size=150,\n",
    "                        cmap=cmap, node_color=list(partition.values()))\n",
    "nx.draw_networkx_edges(book_graphs[0], positions, edge_color='black', alpha=0.05)\n",
    "plt.title('FA2-plot of communities')\n",
    "plt.figtext(.5, 0, f\"The plot contains the 10 communities found in our DC data set.\\nEach community has a distinct color, edges have been colored green\", ha=\"center\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2020769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coloring communites in graph:\n",
    "# books 1-7 combined\n",
    "\n",
    "# Adjusting figure size\n",
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "\n",
    "partition = community_louvain.best_partition(combined_nx[6])\n",
    "#drawing using ForceAtlas\n",
    "forceatlas2 = ForceAtlas2(\n",
    "                        # Behavior alternatives\n",
    "                        outboundAttractionDistribution=False,  # Dissuade hubs\n",
    "                        linLogMode=False,  # NOT IMPLEMENTED\n",
    "                        adjustSizes=False,  # Prevent overlap (NOT IMPLEMENTED)\n",
    "                        edgeWeightInfluence=0.1,\n",
    "\n",
    "                        # Performance\n",
    "                        jitterTolerance=0.5,  # Tolerance\n",
    "                        barnesHutOptimize=True,\n",
    "                        barnesHutTheta=0.5, # original 0.5\n",
    "                        multiThreaded=False,  # NOT IMPLEMENTED\n",
    "\n",
    "                        # Tuning\n",
    "                        scalingRatio=10.0,\n",
    "                        strongGravityMode=True,\n",
    "                        gravity=0.1,\n",
    "\n",
    "                        # Log\n",
    "                        verbose=True)\n",
    "\n",
    "positions = forceatlas2.forceatlas2_networkx_layout(combined_nx[6], pos=None, iterations=2000)\n",
    "cmap = plt.get_cmap('hsv', max(partition.values()) + 1) \n",
    "\n",
    "nx.draw_networkx_nodes(combined_nx[6], positions, partition.keys(), node_size=50,\n",
    "                        cmap=cmap, node_color=list(partition.values()))\n",
    "nx.draw_networkx_edges(combined_nx[6], positions, edge_color='black', alpha=0.05)\n",
    "plt.title('FA2-plot of communities')\n",
    "plt.figtext(.5, 0, f\"The plot contains the 10 communities found in our DC data set.\\nEach community has a distinct color, edges have been colored green\", ha=\"center\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a38794",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "books_combined = combined_nx[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95475a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print which book, the communities and the size of each community\n",
    "\n",
    "for i, com in enumerate(book_communities):\n",
    "    print('Book no.: ' + str(i+1))\n",
    "    print(com)\n",
    "    for part in com:\n",
    "        print(len(part))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0152fb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wordclouds for all books combined\n",
    "dictionary = make_com_dicts(all_communities, combined_nx[6])\n",
    "draw_word_cloud(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f94eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_word_cloud(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311cda5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wordclouds for book 1:\n",
    "dictionary = make_com_dicts(book_communities[0], book_graphs[0])\n",
    "draw_word_cloud(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bdc490",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wordclouds for book 2:\n",
    "dictionary = make_com_dicts(book_communities[1], book_graphs[1])\n",
    "draw_word_cloud(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fe300f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wordclouds for book 3:\n",
    "dictionary = make_com_dicts(book_communities[2], book_graphs[2])\n",
    "draw_word_cloud(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4ffcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wordclouds for book 4:\n",
    "dictionary = make_com_dicts(book_communities[3], book_graphs[3])\n",
    "draw_word_cloud(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8244e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wordclouds for book 5:\n",
    "dictionary = make_com_dicts(book_communities[4], book_graphs[4])\n",
    "draw_word_cloud(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc093c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wordclouds for book 6:\n",
    "dictionary = make_com_dicts(book_communities[5], book_graphs[5])\n",
    "draw_word_cloud(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f3b49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wordclouds for book 7:\n",
    "dictionary = make_com_dicts(book_communities[6], book_graphs[6])\n",
    "draw_word_cloud(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c22042",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52a3f7b6",
   "metadata": {},
   "source": [
    "### Extracting texts from books belonging to communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec99f0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_com_texts(com_list, book_dir, sentence_no):\n",
    "    community_texts = []\n",
    "    maxrange = 0\n",
    "    #if we have less than 10 communities\n",
    "    if len(com_list) < 10:\n",
    "        maxrange = len(com_list)\n",
    "    else:\n",
    "        maxrange = 10\n",
    "\n",
    "\n",
    "    for community in com_list[:maxrange]:\n",
    "        \n",
    "        com_txt = []        \n",
    "        for chap in os.listdir(book_dir):\n",
    "            #if chap == 'replaced_ch1.txt':\n",
    "            with open(book_dir + chap) as f:\n",
    "                    text = f.read()\n",
    "\n",
    "            sentences = text.split(\". \")\n",
    "            count_start = 0\n",
    "            count_end = sentence_no\n",
    "            add_text_start = []\n",
    "            while (count_start < len(sentences)):\n",
    "                    current = sentences[count_start:count_end]\n",
    "                    current = \" \".join(current)\n",
    "                    #print(current)\n",
    "                    for char in community:\n",
    "                        #print(f\"count start is {count_start} and char is {char}\")\n",
    "                        if char in current:\n",
    "                            if not count_start in add_text_start:\n",
    "                                #print(f\"Char {char} is found in current with count start {count_start}\")\n",
    "                                #tokens = nltk.word_tokenize(BeautifulSoup(current, 'html.parser').get_text())\n",
    "                                tokens = word_tokenize(current)\n",
    "                                #file_text = [w.lower() for w in tokens if w.isalpha()]\n",
    "                                com_txt = com_txt + tokens\n",
    "                                add_text_start.append(count_start)\n",
    "                            #print(com_txt)                                         \n",
    "                    count_start = count_end\n",
    "                    count_end += sentence_no\n",
    "            \n",
    "                #if char == 'Avery_I'\n",
    "                #print(char)\n",
    "                # Look at specified amount of senteces at a time\n",
    "                \n",
    "                    \n",
    "        community_texts.append(com_txt)\n",
    "\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "    community_strings = []\n",
    "    for txt in community_texts:\n",
    "        com_words = [w for w in txt if w not in stopwords and len(w)>1]\n",
    "        community_strings.append(com_words)\n",
    "\n",
    "    return community_strings\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e225f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "com_strings1 = extract_com_texts(book_communities[0], './B1/', 5)\n",
    "com_strings2 = extract_com_texts(book_communities[1], './B2/', 5)\n",
    "com_strings3 = extract_com_texts(book_communities[2], './B3/', 5)\n",
    "com_strings4 = extract_com_texts(book_communities[3], './B4/', 5)\n",
    "com_strings5 = extract_com_texts(book_communities[4], './B5/', 5)\n",
    "com_strings6 = extract_com_texts(book_communities[5], './B6/', 5)\n",
    "com_strings7 = extract_com_texts(book_communities[6], './B7/', 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b05297",
   "metadata": {},
   "source": [
    "### Extract wikitext for each community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa33394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_com_wikitexts(com_list, directory):\n",
    "    community_texts = []\n",
    "    maxrange = 0\n",
    "    #if we have less than 10 communities\n",
    "    if len(com_list) < 10:\n",
    "        maxrange = len(com_list)\n",
    "    else:\n",
    "        maxrange = 10\n",
    "\n",
    "\n",
    "    for community in com_list[:maxrange]:\n",
    "        \n",
    "        com_txt = []  \n",
    "        \n",
    "        for char in community:\n",
    "            with open(directory + 'clean_' + char + '.txt') as f:\n",
    "                    text = f.read()\n",
    "            \n",
    "            tokens = nltk.word_tokenize(BeautifulSoup(text, 'html.parser').get_text())\n",
    "            #tokens = word_tokenize(current)\n",
    "            #file_text = [w.lower() for w in tokens if w.isalpha()]\n",
    "            com_txt = com_txt + tokens\n",
    "            \n",
    "        community_texts.append(com_txt)\n",
    "\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "    community_strings = []\n",
    "    for txt in community_texts:\n",
    "        com_words = [w for w in txt if w.lower() not in stopwords and len(w)>2]\n",
    "        community_strings.append(com_words)\n",
    "\n",
    "    return community_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202cb6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "com_wikistrings1 = extract_com_wikitexts(book_communities[0], './characters/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70c3b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the unique words in the community texts\n",
    "def unique(com_str):\n",
    "    unique_terms = []\n",
    "    for community_words in com_str:\n",
    "        unique_terms.append(list(set(community_words)))\n",
    "    \n",
    "    return unique_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f75fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_terms1 = unique(com_strings1)\n",
    "unique_terms2 = unique(com_strings2)\n",
    "unique_terms3 = unique(com_strings3)\n",
    "unique_terms4 = unique(com_strings4)\n",
    "unique_terms5 = unique(com_strings5)\n",
    "unique_terms6 = unique(com_strings6)\n",
    "unique_terms7 = unique(com_strings7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c71fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_wikiterms1 = unique(com_wikistrings1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973e4c5d",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5358bbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf(word, unique_list):\n",
    "    N = len(unique_list)\n",
    "    term_appears = 0\n",
    "    for sublist in unique_list:\n",
    "        if word in sublist:\n",
    "            term_appears+=1\n",
    "    idf_val = math.log(N/(1+term_appears))+1\n",
    "    return idf_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c2bf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(community_str, unique_words):\n",
    "    tfidf_list = []\n",
    "\n",
    "    for community_words in community_str:\n",
    "        fdist = FreqDist(community_words)\n",
    "        total_terms = len(community_words)\n",
    "        tfidf=[]\n",
    "        for word in fdist:\n",
    "            idf_val = idf(word, unique_words)\n",
    "            tf_val = fdist[word]/total_terms\n",
    "            tfidf_elem=(word, tf_val*idf_val)\n",
    "            tfidf.append(tfidf_elem)\n",
    "        tfidf_list.append(tfidf)\n",
    "        \n",
    "    return tfidf_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58a7cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the tfidf list for each book\n",
    "tfidf_list1 = tfidf(com_strings1, unique_terms1)\n",
    "tfidf_list2 = tfidf(com_strings2, unique_terms2)\n",
    "tfidf_list3 = tfidf(com_strings3, unique_terms3)\n",
    "tfidf_list4 = tfidf(com_strings4, unique_terms4)\n",
    "tfidf_list5 = tfidf(com_strings5, unique_terms5)\n",
    "tfidf_list6 = tfidf(com_strings6, unique_terms6)\n",
    "tfidf_list7 = tfidf(com_strings7, unique_terms7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cad88c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_wikilist1 = tfidf(com_wikistrings1, unique_wikiterms1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed1a9a4",
   "metadata": {},
   "source": [
    "## Wordclouds for communities with book text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b4e888",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordCloud(tfidf_list):\n",
    "    fig = plt.figure()\n",
    "    plt.rcParams['figure.figsize'] = [15, 20]\n",
    "\n",
    "    for i in range(len(tfidf_list)):\n",
    "        ax = fig.add_subplot(5,2,i+1)\n",
    "        wordcloud = WordCloud(background_color='black', width=2200,\n",
    "                          height=1800, collocations=False).generate_from_frequencies(dict(tfidf_list[i]))\n",
    "\n",
    "        ax.imshow(wordcloud)\n",
    "        ax.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56326d20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wordCloud(tfidf_list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce17439",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCloud(tfidf_list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064ffc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCloud(tfidf_list3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763a1a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCloud(tfidf_list4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf92cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCloud(tfidf_list5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af4fead",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCloud(tfidf_list6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6292a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCloud(tfidf_list7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4aa4ea",
   "metadata": {},
   "source": [
    "## Wordclouds for communities with wiki text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a377e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wordCloud(tfidf_wikilist1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
