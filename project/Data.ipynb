{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02433c8b",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5439def9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import urllib.request\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from urllib.parse import quote\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0812102a",
   "metadata": {},
   "source": [
    "## Choosing an online source\n",
    ">We have decided to use the [fandom](https://harrypotter.fandom.com/wiki/Main_Page) wikipedia instead of the regular [wikipedia](https://www.wikipedia.org/) for finding articles for our characters. The main reason for this being that the regular wikipedia had multiple characters in one article, e.g. [Ministry of Magic](https://en.wikipedia.org/wiki/Ministry_of_Magic) contains 23 characters. Whereas the fandom wikipedia has a dedicated article for each character.\n",
    "\n",
    "## Creating a list of characters\n",
    ">To create a list of characters we combined [wikipedia's list of Harry Potter characters](https://en.wikipedia.org/wiki/List_of_Harry_Potter_characters), with the [half- and full-blood lists](https://harrypotter.fandom.com/wiki/Category:Individuals_by_parentage) from the fandom wikipedia, and the characters from [Buzzfeed's Harry Potter Character Quiz](https://www.buzzfeed.com/sarahaspler/there-are-over-700-harry-potter-characters-and-i). The reason for combining these were to include as many characters as possible, without having to go through all of the articles on the fandom wikipedia. However, we found that we did have to check all of the articles from the combined list manually, because some of the names in the list were not unique, had incorrect spelling, did not match the article name etc. Hence we decided on the following criteria for the final list:\n",
    "\n",
    "- Characters are represented by the article name.\n",
    "- Characters must be from the actual books.\n",
    "- Characters must have an appearence in at least one book.\n",
    "\n",
    "\n",
    ">To clarify a character has an appearence in a book if they are represented in a book by some interaction with other characters. This is contrary to characters who are only mentioned, which means that it may just be a case of another character saying their name in conversation with some third character. These criteria also weeds out characters that are only from video games, or the Fantastic Beasts franchise etc. The reason for this initial sorting is that we want to use the books, hence we are removing a lot of noise by not having characters that have no text related to them in the books. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7fc6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving the characters from our .csv file into a list of tuples\n",
    "characters = []\n",
    "\n",
    "with open(\"HP_characters.csv\", \"r\", encoding=\"utf8\") as sent_file:\n",
    "    csv = csv.reader(sent_file, delimiter=\",\")\n",
    "    for row in csv:\n",
    "        name = row[0].replace(' ', '_') \n",
    "        # Parentage, House, and Occupation will be our attributes in the nodes\n",
    "        parentage = row[1]\n",
    "        house = row[2]\n",
    "        occupation = row[3]\n",
    "        characters.append((name, parentage, house, occupation))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209efd94",
   "metadata": {},
   "source": [
    "## Downloading the files\n",
    ">We are using the API from the course to download the articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b71957",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lengths = []\n",
    "\n",
    "#https://www.reddit.com/r/learnpython/comments/muwu7v/scraping_fandomwiki_pages/\n",
    "baseurl = \"https://harrypotter.fandom.com/api.php?\"\n",
    "action = \"action=query\"\n",
    "\n",
    "for character in characters:\n",
    "    # Set up the query for the character\n",
    "    title = \"titles=\" + character[0]\n",
    "    content = \"prop=revisions&rvprop=content\"\n",
    "    dataformat =\"format=json\"\n",
    "    query = \"{}{}&{}&{}&{}\".format(baseurl, action, content, title, dataformat)\n",
    "\n",
    "    # Since we have checked the articles, we know that urlopen will succeed\n",
    "    wikiresponse = urllib.request.urlopen(query)\n",
    "    wikidata = wikiresponse.read()\n",
    "    wikitext = wikidata.decode('utf-8')\n",
    "    lengths.append((character, len(wikitext)))\n",
    "\n",
    "    try:\n",
    "        jsonobj = json.loads(wikitext)\n",
    "    except ValueError: \n",
    "        print(f'Decoding JSON has failed for {character}, moving on...')\n",
    "    # Get the number for the article\n",
    "    num = list(jsonobj['query']['pages'].keys())[0]\n",
    "    # Get the wikitext\n",
    "    wikitext = jsonobj['query']['pages'][num]['revisions'][0]['*']\n",
    "    f_name = character[0] + '.txt'\n",
    "    \n",
    "    # Write to a file with that name\n",
    "    with open(\"characters/\" + f_name, 'w') as f:\n",
    "        f.write(wikitext)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e69ce2b",
   "metadata": {},
   "source": [
    "## Checking the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca7db8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the top/bottom of characters by length of articles\n",
    "# to see if it looks reasonable\n",
    "print(sorted(lengths, key=lambda x: x[1])[:10])\n",
    "print(sorted(lengths, key=lambda x: x[1])[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c412cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths_s = [x[1] for x in lengths]\n",
    "\n",
    "# Histogram for bad guys\n",
    "number_of_bins = 10\n",
    "histogram = np.histogram(lengths_s, number_of_bins)\n",
    "\n",
    "x_values = histogram[1][:-1]\n",
    "y_values = histogram[0]\n",
    "\n",
    "plt.plot(x_values, y_values)\n",
    "plt.title(f'Plot of length of articles')\n",
    "plt.xlabel('Length')\n",
    "plt.ylabel('Number of articles')\n",
    "plt.figtext(.5,-0.05, f\"text\", ha='center')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Minimum length = {np.min(lengths_s)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0aee8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram for bad guys\n",
    "number_of_bins = 10\n",
    "histogram = np.histogram(lengths_s, number_of_bins)\n",
    "\n",
    "x_values = histogram[1][:-1]\n",
    "y_values = histogram[0]\n",
    "\n",
    "plt.plot(x_values, y_values)\n",
    "plt.title(f'Log-Log plot of length of articles')\n",
    "plt.xlabel('Length')\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.ylabel('Number of articles')\n",
    "plt.figtext(.5,-0.05, f\"text\", ha='center')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Minimum length = {np.min(lengths_s)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e1c46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Average length of an article: {sum(lengths_s)/len(lengths_s)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d527d0",
   "metadata": {},
   "source": [
    "### Cleaning the wiki fandom pages even more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2805c211",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#src_path = \"Doctor_Strange.txt\"\n",
    "#src_path = \"Doctor_Octopus.txt\"\n",
    "#dst_path = \"Clean_\" + src_path\n",
    "\n",
    "def clean_text(directory):\n",
    "    \n",
    "    for char in os.listdir(directory):\n",
    "        if not 'clean' in char:\n",
    "            src_path = directory+char\n",
    "            dst_path = directory+'clean_'+char\n",
    "\n",
    "            with open(src_path, 'r') as f:\n",
    "                dirty_text = f.read()\n",
    "\n",
    "            #\n",
    "            clean_text = dirty_text\n",
    "\n",
    "            # Remove curly brackets including content (can be nested).\n",
    "            n = 1\n",
    "            while n > 0:\n",
    "                clean_text, n = re.subn(r'\\{\\{[^{}]*\\}\\}', '', clean_text)\n",
    "\n",
    "            # Remove <ref> tags including content.\n",
    "           # clean_text = re.sub(r'<ref.*?</ref>', \"\", clean_text)\n",
    "           # clean_text = re.sub(r'<ref.*?/>', \"\", clean_text)\n",
    "\n",
    "            # Remove category links.\n",
    "            clean_text = re.sub(r'\\[\\[Category:.*?\\]\\]', \"\", clean_text)\n",
    "\n",
    "            # Remove File links\n",
    "            clean_text = re.sub(r'\\[\\[File:.*?\\]\\]', \"\", clean_text)\n",
    "\n",
    "            clean_text = re.sub(r'\\<span.*?\\>', \"\", clean_text)\n",
    "            clean_text = re.sub(r'\\</span.*?\\>', \"\", clean_text)\n",
    "            clean_text = re.sub(r'\\=\\=.*?\\=\\=', \"\", clean_text)\n",
    "            clean_text = re.sub(r'\\=', \"\", clean_text)\n",
    "            clean_text = re.sub(r'\\<\\!\\-\\-.*?\\-\\-\\>', \"\", clean_text)\n",
    "\n",
    "            # Remove character name in different languages:\n",
    "            clean_text = re.sub(r'\\[\\[de:.*?\\]\\]', \"\", clean_text)\n",
    "            clean_text = re.sub(r'\\[\\[es:.*?\\]\\]', \"\", clean_text)\n",
    "            clean_text = re.sub(r'\\[\\[fr:.*?\\]\\]', \"\", clean_text)\n",
    "            clean_text = re.sub(r'\\[\\[it:.*?\\]\\]', \"\", clean_text)\n",
    "            clean_text = re.sub(r'\\[\\[ru:.*?\\]\\]', \"\", clean_text)\n",
    "            clean_text = re.sub(r'\\[\\[fi:.*?\\]\\]', \"\", clean_text)\n",
    "            clean_text = re.sub(r'\\[\\[pl:.*?\\]\\]', \"\", clean_text)\n",
    "            clean_text = re.sub(r'\\[\\[de2:.*?\\]\\]', \"\", clean_text)\n",
    "            clean_text = re.sub(r'\\[\\[ja:.*?\\]\\]', \"\", clean_text)\n",
    "            clean_text = re.sub(r'\\[\\[pt\\-br:.*?\\]\\]', \"\", clean_text)\n",
    "            clean_text = re.sub(r'\\[\\[ar:.*?\\]\\]', \"\", clean_text)\n",
    "            clean_text = re.sub(r'\\[\\[ca:.*?\\]\\]', \"\", clean_text)\n",
    "            clean_text = re.sub(r'\\[\\[da:.*?\\]\\]', \"\", clean_text)\n",
    "            clean_text = re.sub(r'\\[\\[cs:.*?\\]\\]', \"\", clean_text)\n",
    "            clean_text = re.sub(r'\\[\\[el:.*?\\]\\]', \"\", clean_text)\n",
    "            clean_text = re.sub(r'\\[\\[et:.*?\\]\\]', \"\", clean_text)\n",
    "            clean_text = re.sub(r'\\[\\[he:.*?\\]\\]', \"\", clean_text)\n",
    "            clean_text = re.sub(r'\\[\\[id:.*?\\]\\]', \"\", clean_text)\n",
    "            clean_text = re.sub(r'\\[\\[lt:.*?\\]\\]', \"\", clean_text)\n",
    "            clean_text = re.sub(r'\\[\\[mk:.*?\\]\\]', \"\", clean_text)\n",
    "            clean_text = re.sub(r'\\[\\[nl:.*?\\]\\]', \"\", clean_text)\n",
    "            clean_text = re.sub(r'\\[\\[no:.*?\\]\\]', \"\", clean_text)\n",
    "            clean_text = re.sub(r'\\[\\[oc:.*?\\]\\]', \"\", clean_text)\n",
    "            clean_text = re.sub(r'\\[\\[pt:.*?\\]\\]', \"\", clean_text)\n",
    "            clean_text = re.sub(r'\\[\\[sr:.*?\\]\\]', \"\", clean_text)\n",
    "            clean_text = re.sub(r'\\[\\[sv:.*?\\]\\]', \"\", clean_text)\n",
    "            clean_text = re.sub(r'\\[\\[tr:.*?\\]\\]', \"\", clean_text)\n",
    "            clean_text = re.sub(r'\\[\\[uk:.*?\\]\\]', \"\", clean_text)\n",
    "            clean_text = re.sub(r'\\[\\[zh:.*?\\]\\]', \"\", clean_text)\n",
    "            clean_text = re.sub(r'\\[\\[ro:.*?\\]\\]', \"\", clean_text)\n",
    "            clean_text = re.sub(r'\\[\\[vi:.*?\\]\\]', \"\", clean_text)\n",
    "            clean_text = re.sub(r'\\[\\[ka:.*?\\]\\]', \"\", clean_text)\n",
    "            clean_text = re.sub(r'\\[\\[sl:.*?\\]\\]', \"\", clean_text)\n",
    "\n",
    "            # Remove http links.\n",
    "            clean_text = re.sub(r'\\[http.*?\\]', \"\", clean_text)\n",
    "\n",
    "            # Remove tables\n",
    "            clean_text = re.sub(r'\\{\\|(.|\\s)*?\\|\\}', \"\", clean_text)\n",
    "\n",
    "            #\n",
    "            with open(dst_path, 'w') as f:\n",
    "                 f.write(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696abb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text('./characters/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653f8a04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
