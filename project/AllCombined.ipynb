{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02433c8b",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5439def9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import urllib.request\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import cm\n",
    "from urllib.parse import quote\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, FreqDist\n",
    "import math\n",
    "import networkx as nx\n",
    "from networkx.algorithms import community\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "from bs4 import BeautifulSoup\n",
    "from community import community_louvain\n",
    "from fa2 import ForceAtlas2\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0812102a",
   "metadata": {},
   "source": [
    "## Choosing an online source\n",
    ">We have decided to use the [fandom](https://harrypotter.fandom.com/wiki/Main_Page) wikipedia instead of the regular [wikipedia](https://www.wikipedia.org/) for finding articles for our characters. The main reason for this being that the regular wikipedia had multiple characters in one article, e.g. [Ministry of Magic](https://en.wikipedia.org/wiki/Ministry_of_Magic) contains 23 characters. Whereas the fandom wikipedia has a dedicated article for each character.\n",
    "\n",
    "## Creating a list of characters\n",
    ">To create a list of characters we combined [wikipedia's list of Harry Potter characters](https://en.wikipedia.org/wiki/List_of_Harry_Potter_characters), with the [half- and full-blood lists](https://harrypotter.fandom.com/wiki/Category:Individuals_by_parentage) from the fandom wikipedia, and the characters from [Buzzfeed's Harry Potter Character Quiz](https://www.buzzfeed.com/sarahaspler/there-are-over-700-harry-potter-characters-and-i). The reason for combining these were to include as many characters as possible, without having to go through all of the articles on the fandom wikipedia. However, we found that we did have to check all of the articles from the combined list manually, because some of the names in the list were not unique, had incorrect spelling, did not match the article name etc. Hence we decided on the following criteria for the final list:\n",
    "\n",
    "- Characters are represented by the article name.\n",
    "- Characters must be from the actual books.\n",
    "- Characters must have an appearence in at least one book.\n",
    "\n",
    "\n",
    ">To clarify a character has an appearence in a book if they are represented in a book by some interaction with other characters. This is contrary to characters who are only mentioned, which means that it may just be a case of another character saying their name in conversation with some third character. These criteria also weeds out characters that are only from video games, or the Fantastic Beasts franchise etc. The reason for this initial sorting is that we want to use the books, hence we are removing a lot of noise by not having characters that have no text related to them in the books. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7fc6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving the characters from our .csv file into a list of tuples\n",
    "characters = []\n",
    "\n",
    "with open(\"HP_characters.csv\", \"r\", encoding=\"utf8\") as sent_file:\n",
    "    csv = csv.reader(sent_file, delimiter=\",\")\n",
    "    for row in csv:\n",
    "        name = row[0].replace(' ', '_') \n",
    "        # Parentage, House,Occupation and loyalty will be our attributes in the nodes\n",
    "        parentage = row[1]\n",
    "        house = row[2]\n",
    "        occupation = row[3]\n",
    "        loyalty = row[4]\n",
    "        characters.append((name, parentage, house, occupation, loyalty))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209efd94",
   "metadata": {},
   "source": [
    "## Downloading the files\n",
    ">We are using the API from the course to download the articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b71957",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lengths = []\n",
    "\n",
    "#https://www.reddit.com/r/learnpython/comments/muwu7v/scraping_fandomwiki_pages/\n",
    "baseurl = \"https://harrypotter.fandom.com/api.php?\"\n",
    "action = \"action=query\"\n",
    "\n",
    "for character in characters:\n",
    "    # Set up the query for the character\n",
    "    title = \"titles=\" + character[0]\n",
    "    content = \"prop=revisions&rvprop=content\"\n",
    "    dataformat =\"format=json\"\n",
    "    query = \"{}{}&{}&{}&{}\".format(baseurl, action, content, title, dataformat)\n",
    "\n",
    "    # Since we have checked the articles, we know that urlopen will succeed\n",
    "    wikiresponse = urllib.request.urlopen(query)\n",
    "    wikidata = wikiresponse.read()\n",
    "    wikitext = wikidata.decode('utf-8')\n",
    "    lengths.append((character, len(wikitext)))\n",
    "\n",
    "    try:\n",
    "        jsonobj = json.loads(wikitext)\n",
    "    except ValueError: \n",
    "        print(f'Decoding JSON has failed for {character}, moving on...')\n",
    "    # Get the number for the article\n",
    "    num = list(jsonobj['query']['pages'].keys())[0]\n",
    "    # Get the wikitext\n",
    "    wikitext = jsonobj['query']['pages'][num]['revisions'][0]['*']\n",
    "    f_name = character[0] + '.txt'\n",
    "    \n",
    "    # Write to a file with that name\n",
    "    with open(\"characters/\" + f_name, 'w') as f:\n",
    "        f.write(wikitext)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e69ce2b",
   "metadata": {},
   "source": [
    "## Checking the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca7db8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the top/bottom of characters by length of articles\n",
    "# to see if it looks reasonable\n",
    "print(sorted(lengths, key=lambda x: x[1])[:10])\n",
    "print(sorted(lengths, key=lambda x: x[1])[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c412cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram for the lengths of the articles\n",
    "# This was used in our video presentation\n",
    "lengths_s = [x[1] for x in lengths]\n",
    "\n",
    "number_of_bins = 10\n",
    "histogram = np.histogram(lengths_s, number_of_bins)\n",
    "\n",
    "x_values = histogram[1][:-1]\n",
    "y_values = histogram[0]\n",
    "\n",
    "plt.plot(x_values, y_values)\n",
    "plt.title(f'Plot of length of articles')\n",
    "plt.xlabel('Length')\n",
    "plt.ylabel('Number of articles')\n",
    "plt.figtext(.5,-0.05, f\"text\", ha='center')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Minimum length = {np.min(lengths_s)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0aee8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram for the lengths of the articles with log-log axes\n",
    "# This was used in our video presentation\n",
    "number_of_bins = 10\n",
    "histogram = np.histogram(lengths_s, number_of_bins)\n",
    "\n",
    "x_values = histogram[1][:-1]\n",
    "y_values = histogram[0]\n",
    "\n",
    "plt.plot(x_values, y_values)\n",
    "plt.title(f'Log-Log plot of length of articles')\n",
    "plt.xlabel('Length')\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.ylabel('Number of articles')\n",
    "plt.figtext(.5,-0.05, f\"text\", ha='center')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Minimum length = {np.min(lengths_s)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e1c46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Average length of an article: {sum(lengths_s)/len(lengths_s)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a53bad5",
   "metadata": {},
   "source": [
    "# Alias extraction\n",
    "> To do out analysis we have to find all the aliases that each character can have in the books. The wiki fandom pages have a section with those listed. We extract those by using regular expressions. Then we create a dictionary with the found aliases and go through the book texts and replace all the aliases with the full character names. \n",
    "Some of the characters had aliases that weren't mentioned or that collapses with other characters, so as explained in the following code we had to do some adjustments to the dictionary manually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca89db6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function to extract the aliases from the wiki fandom pages and save them in a dictionary \n",
    "with the alias as key and the full character name as value.\n",
    "\n",
    "Input: A text (we use the wiki fandom text we have extracted),\n",
    "       a dictionary for the found aliases,\n",
    "       the name the aliases belong to\n",
    "Output: None, the alias dictionary is global\n",
    "       \n",
    "\"\"\"\n",
    "def find_aliases(text, alias_dict, name):\n",
    "    #Find all aliases. Will be between |alias and |, can be on several lines\n",
    "    aliases = re.findall(r'\\|alias = (.(.|\\s)*?)\\|', text)\n",
    "    \n",
    "    if len(aliases)==0:\n",
    "        return\n",
    "    else:\n",
    "        #Take the first match in the first group and split by new line so each alias becomes an element\n",
    "        all_aliases = aliases[0][0].split(\"\\n\")\n",
    "        \n",
    "        #Find the aliases to keep\n",
    "        for alias in all_aliases:\n",
    "            #Don't keep the ones used for a disguise\n",
    "            if \"disguise\" in alias:\n",
    "                continue\n",
    "            #Mudblood not an alias\n",
    "            if \"Mudblood\" in alias:\n",
    "                continue\n",
    "            #Don't keep the ones used to tell others a wrong name \n",
    "            if \"the name he told\" in alias:\n",
    "                continue\n",
    "            if \"the name she told\" in alias:\n",
    "                continue\n",
    "            #If alias is empty string then don't keep\n",
    "            if alias == '':\n",
    "                continue\n",
    "            #Cleaning of the aliases\n",
    "            processed_alias = alias.split(\" (\")[0]\n",
    "            if '{{' in processed_alias:\n",
    "                processed_alias = alias.split(\"{{\")[0]\n",
    "            processed_alias = processed_alias.replace('*', '')\n",
    "            processed_alias = processed_alias.replace('[[', '').replace(']]', '')\n",
    "            processed_alias = processed_alias.replace('\"', '')\n",
    "            # This print is to see which aliases might belong to several characters\n",
    "            if processed_alias in alias_dict:\n",
    "                print(\"Processed alias: \" + processed_alias + \", belonging to: \" + name)\n",
    "            else:\n",
    "                alias_dict[processed_alias.lower()] = name   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc623ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "alias_dictionary = {}\n",
    "not_added = []\n",
    "# For each character page in the directory, find the aliases\n",
    "for file_ in os.listdir(\"./characters\"):\n",
    "    name = file_[:-4]\n",
    "    with open(\"./characters/\"+file_, 'r') as f:\n",
    "        text = f.read()\n",
    "    first_name = name.split(\"_\")[0].lower()\n",
    "    full_name = name.replace('_', ' ').lower()\n",
    "    # If the first name is not unique, print so we can see who have duplicates\n",
    "    if first_name in alias_dictionary:\n",
    "        not_added.append((first_name, name))\n",
    "    else:\n",
    "        alias_dictionary[first_name] = name\n",
    "    # The full name of Stebbins in the wiki fandom pages is Stebbins_(1990s_Hogwarts_student)\n",
    "    # This full name won't be in the books\n",
    "    if not first_name == 'stebbins':\n",
    "        alias_dictionary[full_name] = name\n",
    "    find_aliases(text, alias_dictionary, name)\n",
    "\n",
    "# Print all the names not added and the alike ones to see who are in the dict instead    \n",
    "for item in not_added:\n",
    "    print(\"Dict has: \" + alias_dictionary[item[0]] + \" instead of: \" + item[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e74e437",
   "metadata": {},
   "source": [
    "### Notes to aliases/ first names that collapses:\n",
    "**Following characters only appears in the last chapter of B7:**\n",
    "- James Potter II \n",
    "- Edward Lupin\n",
    "- Rose Granger-Weasley\n",
    "- Lily L. Potter\n",
    "\n",
    "- Frank Bryce only appears in first chapter of B4\n",
    "\n",
    "For the above characters we will change dictionary when looking in the corresponding chapters\n",
    "\n",
    "**Following characters has first names that are the same and appears in different books:**\n",
    "- Marcus Flint (appears in B1, B2, B3) and Marcus Belby (appears in B6), equally important\n",
    "- Graham Montague (in B3, B5) more important than Graham Pritchard (in B4)\n",
    "- Frank Longbottom (in B5) more important than Frank Bryce (in B4)\n",
    "- Avery II (in B4, B5) more important than Avery I (in B6)\n",
    "\n",
    "**Following characters has first names that are the same and appears in the same books:**\n",
    "- Graham Montague (in B3, B5) more important than Graham Montague's father (in B5) and mother (in B5)\n",
    "- Hermione Granger (in all books) more important than Hermione Granger's Father(in B2, B5) and Hermione Granger's Mother (in B2, B5)\n",
    "- Dennis Creevey (in B4-B6) more important than Dennis (in B5)\n",
    "- Ernest Macmillan (in B2, B4-B7) more important than Ernest Prang (in B3, B6)\n",
    "- Fat Friar (in B1, B2, B5) and Fat Lady (in all books) equally important\n",
    "- Mary Cattermole (in B7) equally important as Mary Macdonald (in B7)\n",
    "- Evans sister's father (in B7) equally important as Evans sister's mother (in B7)\n",
    "\n",
    "For those not in the same books we will make dictionary to fit with book. \n",
    "For those in the same books we will have to find another solution and prioritize the most important characters. \n",
    "\n",
    "Mr. and Mrs. Dursley appears often like this. Needs to be replaced with Vernon_Dursley and Petunia_Dursley.\n",
    "The Potters should be replaced with James_Potter_I and Lily_J._Potter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb83133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjusting dictionary manually to contain those characters that are most important or appears in most books\n",
    "# Also some characters haven't had any aliases belonging to them in the wiki fandom pages, but do have aliases in\n",
    "# the books, so these are put in here as well\n",
    "alias_dictionary['james'] = 'James_Potter_I'\n",
    "alias_dictionary['frank'] = 'Frank_Longbottom'\n",
    "alias_dictionary['marcus'] = 'Marcus_Flint'\n",
    "alias_dictionary['avery'] = 'Avery_II'\n",
    "alias_dictionary['hermione'] = 'Hermione_Granger'\n",
    "alias_dictionary['mr. dursley'] = 'Vernon_Dursley'\n",
    "alias_dictionary['mrs. dursley'] = 'Petunia_Dursley'\n",
    "alias_dictionary['mcgonagall'] = 'Minerva_McGonagall'\n",
    "alias_dictionary['dumbledore'] = 'Albus_Dumbledore'\n",
    "alias_dictionary['ernest'] = 'Ernest_Macmillan'\n",
    "alias_dictionary['dennis'] = 'Dennis_Creevey'\n",
    "alias_dictionary['mr mason'] = 'Mason'\n",
    "alias_dictionary['mrs mason'] = \"Mason's_wife\"\n",
    "alias_dictionary['pince'] = \"Irma_Pince\"\n",
    "alias_dictionary['vulchanov'] = \"Pyotr_Vulchanov\"\n",
    "alias_dictionary['volkov'] = \"Ivan_Volkov\"\n",
    "alias_dictionary['ivanova'] = \"Clara_Ivanova\"\n",
    "alias_dictionary['grubbly-plank'] = \"Wilhelmina_Grubbly-Plank\"\n",
    "alias_dictionary['mrs. figg'] = \"Arabella_Figg\"\n",
    "alias_dictionary['mrs figg'] = \"Arabella_Figg\"\n",
    "alias_dictionary['hooch'] = \"Rolanda_Hooch\"\n",
    "alias_dictionary['ollivander'] = \"Garrick_Ollivander\"\n",
    "alias_dictionary['mr ollivander'] = \"Garrick_Ollivander\"\n",
    "alias_dictionary['mr. ollivander'] = \"Garrick_Ollivander\"\n",
    "alias_dictionary['quirrell'] = \"Quirinus_Quirrell\"\n",
    "alias_dictionary['trelawney'] = \"Sybill_Trelawney\"\n",
    "alias_dictionary[' tonks'] = \" Nymphadora_Tonks\"\n",
    "alias_dictionary['belby'] = \"Marcus_Belby\"\n",
    "alias_dictionary['montague'] = \"Graham_Montague\"\n",
    "alias_dictionary['warrington'] = \"Cassius_Warrington\"\n",
    "alias_dictionary['pritchard'] = \"Graham_Pritchard\"\n",
    "alias_dictionary['gregorovitch'] = \"Mykew_Gregorowitch\"\n",
    "alias_dictionary['bode'] = \"Broderick_Bode\"\n",
    "alias_dictionary['carmichael'] = \"Eddie_Carmichael\"\n",
    "alias_dictionary['mr granger'] = \"Hermione_Granger's_father\"\n",
    "alias_dictionary['sinistra'] = \"Aurora_Sinistra\"\n",
    "alias_dictionary['bole'] = \"Lucian_Bole\"\n",
    "alias_dictionary['yaxley'] = \"Corban_Yaxley\"\n",
    "alias_dictionary['greyback'] = \"Fenrir_Greyback\"\n",
    "alias_dictionary['vector'] = \"Septima_Vector\"\n",
    "\n",
    "#delete those first names that collapses with other characters or words and aren't able to find a solution to\n",
    "#i.e. rose also in 'the sun rose' and evans also in evans sister's father. \n",
    "#check first if they are a key to avoid errors\n",
    "if 'rose' in alias_dictionary:\n",
    "    del alias_dictionary['rose']\n",
    "\n",
    "if 'graham' in alias_dictionary:\n",
    "    del alias_dictionary['graham']  \n",
    "\n",
    "if 'mary' in alias_dictionary:\n",
    "    del alias_dictionary['mary'] \n",
    "\n",
    "if 'evans' in alias_dictionary:\n",
    "    del alias_dictionary['evans'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5ef12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove generic words used as keys\n",
    "if 'mr' in alias_dictionary:\n",
    "    del alias_dictionary['mr']\n",
    "if 'mrs' in alias_dictionary:\n",
    "    del alias_dictionary['mrs']\n",
    "if 'fat' in alias_dictionary:\n",
    "    del alias_dictionary['fat']\n",
    "if 'pig' in alias_dictionary:\n",
    "    del alias_dictionary['pig']\n",
    "if 'lord voldemort (self-proclaimed title and chosen name) ' in alias_dictionary:\n",
    "    alias_dictionary['lord voldemort'] = 'Tom_Riddle'\n",
    "    del alias_dictionary['lord voldemort (self-proclaimed title and chosen name) ']\n",
    "\n",
    "#if a key is short, put white space aroung to make sure it won't be found as a substring in another word\n",
    "# i.e al is a key but also appears in the word normal\n",
    "for alias, name in alias_dictionary.items():\n",
    "    if (len(alias)<4):   \n",
    "            value = alias_dictionary[alias]\n",
    "            alias_dictionary[' '+alias+' '] = alias_dictionary.pop(alias)\n",
    "            alias_dictionary[' '+alias+' '] = \" \"+value+\" \"\n",
    "# Some manually adjustments to problems we experienced when looking over the texts after replacements\n",
    "alias_dictionary[' ron,'] = \" \"+value\n",
    "alias_dictionary[\" ron'\"] = \" \"+value\n",
    "alias_dictionary[' ron.'] = \" \"+value\n",
    "alias_dictionary[' ron?'] = \" \"+value\n",
    "alias_dictionary[' ron!'] = \" \"+value\n",
    "value = alias_dictionary['ivan']\n",
    "alias_dictionary[' ivan '] = alias_dictionary.pop('ivan')\n",
    "alias_dictionary[' ivan '] = \" \"+value+\" \"\n",
    "value = alias_dictionary['bill']\n",
    "alias_dictionary[' bill '] = alias_dictionary.pop('bill')\n",
    "alias_dictionary[' bill '] = \" \"+value+\" \"\n",
    "value = alias_dictionary['stan']\n",
    "alias_dictionary[' stan '] = alias_dictionary.pop('stan')\n",
    "alias_dictionary[' stan '] = \" \"+value+\" \"\n",
    "value = alias_dictionary['michael']\n",
    "alias_dictionary[' michael '] = alias_dictionary.pop('michael')\n",
    "alias_dictionary[' michael '] = \" \"+value+\" \"\n",
    "value = alias_dictionary['dora']\n",
    "alias_dictionary[' dora '] = alias_dictionary.pop('dora')\n",
    "alias_dictionary[' dora '] = \" \"+value+\" \"\n",
    "value = alias_dictionary['bella']\n",
    "alias_dictionary[' bella '] = alias_dictionary.pop('bella')\n",
    "alias_dictionary[' bella '] = \" \"+value+\" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0272f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.geeksforgeeks.org/python-program-to-sort-dictionary-by-key-lengths/\n",
    "\n",
    "# Sort dictionary by the length of the key, so we replace those with longest names first\n",
    "# This is to make sure we replace the longest names in the dictionary first, \n",
    "# so we won't get errors with wrong replacements\n",
    "test_dict_list = sorted(list(alias_dictionary.items()), key = lambda key : len(key[0]), reverse = True)\n",
    "  \n",
    "# reordering to dictionary\n",
    "alias_dictionary = {ele[0] : ele[1]  for ele in test_dict_list}\n",
    "  \n",
    "# printing result \n",
    "print(alias_dictionary.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531f6832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace names in chapters so they are called our character names and not aliases\n",
    "\n",
    "def replace_aliases(srcdir, dstdir):\n",
    "    for chap in os.listdir(srcdir):     \n",
    "        # do not look in files where aliases already have been replaced\n",
    "        if 'replace' in chap:\n",
    "            continue\n",
    "        clean_text = ''\n",
    "        with open(srcdir+'/'+chap, 'r') as f:\n",
    "                clean_text = f.read()\n",
    "                f.close()\n",
    "        \n",
    "        # remove all tabs and new lines, lower all text \n",
    "        #replace a contraction of 2 names with both their names\n",
    "        clean_text = clean_text.replace('\\t', ' ').replace('\\n', ' ').replace('  ', ' ')\n",
    "        clean_text = clean_text.lower()\n",
    "        clean_text = clean_text.replace('mr. and mrs. dursley', 'Vernon_Dursley and Petunia_Dursley')\n",
    "        clean_text = clean_text.replace('the potters', 'James_Potter_I and Lily_J._Potter')\n",
    "        clean_text = clean_text.replace('the dursleys', 'Vernon_Dursley and Petunia_Dursley')\n",
    "        clean_text = clean_text.replace('mr weasley', 'Arthur_Weasley')\n",
    "        clean_text = clean_text.replace('mrs weasley', 'Molly_Weasley')\n",
    "        clean_text = clean_text.replace('mr and mrs mason', \"Mason and Mason's_wife\")\n",
    "        clean_text = clean_text.replace('masons', \"Mason and Mason's_wife\")\n",
    "        clean_text = clean_text.replace('mr and mrs montague', \"Graham_Montague's_father and Graham_Montague's_mother\")\n",
    "        clean_text = clean_text.replace('weird sisters', \"Myron_Wagtail and Donaghan_Tremlett\")\n",
    "        clean_text = clean_text.replace('the grangers', \"Hermione_Granger's_father and Hermione_Granger's_mother\")\n",
    "        clean_text = clean_text.replace('mr and mrs granger', \"Hermione_Granger's_father and Hermione_Granger's_mother\")\n",
    "        clean_text = clean_text.replace('the carrows', \"Alecto_Carrow and Amycus_Carrow\")\n",
    "        clean_text = clean_text.replace('the lestranges', \"Rabastan_Lestrange, Rodolphus_Lestrange and Bellatrix_Lestrange\")                                \n",
    "        \n",
    "        # some change in dictionary as those characters only appears in one chapter \n",
    "        # and their names collapses with other characters\n",
    "        if (srcdir == './Chapters_withouth_replacement/B7'):\n",
    "            print(chap)\n",
    "            if chap == 'B7_Ch37.txt':\n",
    "                #print(\"in the very last chapter\")\n",
    "                alias_dictionary['james'] = 'James_Potter_II'\n",
    "                alias_dictionary['edward'] = 'Edward_Lupin'\n",
    "                alias_dictionary['rose'] = 'Rose_Granger-Weasley'\n",
    "                alias_dictionary['lily'] = 'Lily_L._Potter'\n",
    "                alias_dictionary['albus'] = 'Albus_Potter'\n",
    "                alias_dictionary['wife'] = 'Astoria_Malfoy'\n",
    "        if (srcdir == './Chapters_withouth_replacement/B1'):\n",
    "            if chap == 'B1_Ch4.txt':\n",
    "                alias_dictionary['mother and father'] = \"Evans_sister's_father and Evans_sister's_mother\"\n",
    "        if (srcdir == '/Chapters_withouth_replacement./B5'):\n",
    "            if chap == 'B5_Ch4.txt':\n",
    "                alias_dictionary['portrait'] = \"Walburga_Black\"\n",
    "        for alias, name in alias_dictionary.items():\n",
    "            clean_text = clean_text.replace(alias, name)\n",
    "        \n",
    "        if (srcdir == '/Chapters_withouth_replacement./B7'):\n",
    "            print(chap)\n",
    "            if chap == 'B7_Ch37.txt':\n",
    "                alias_dictionary['james'] = 'James_Potter_I'\n",
    "                alias_dictionary['edward'] = 'Edward_Tonks'\n",
    "                del alias_dictionary['rose']\n",
    "                alias_dictionary['lily'] = 'Lily_J._Potter'\n",
    "                alias_dictionary['albus'] = 'Albus_Dumbledore'\n",
    "                del alias_dictionary['wife']\n",
    "        if 'mother and father' in alias_dictionary:\n",
    "            print(\"in \" + srcdir + \"chap \" + chap + \", deleting m and f\")\n",
    "            del alias_dictionary['mother and father']\n",
    "        \n",
    "        if 'portrait' in alias_dictionary:\n",
    "            print(\"in \" + srcdir + \"chap \" + chap + \", deleting portrait\")\n",
    "            del alias_dictionary['portrait']\n",
    "            \n",
    "        # Write to a file with that name\n",
    "        filename = chap.split('_')[1].lower()\n",
    "        dst_filename = dstdir+'/replaced_' + filename\n",
    "\n",
    "        with open(dst_filename, 'w') as f:\n",
    "                f.write(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80390bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over chapters, will do it book by book as we have some places where we need to change dictionary\n",
    "   \n",
    "#Book 1\n",
    "replace_aliases('./Chapters_withouth_replacement/B1', './B1')\n",
    "\n",
    "#Book 2\n",
    "replace_aliases('./Chapters_withouth_replacement/B2', './B2')\n",
    "\n",
    "#Book 3\n",
    "alias_dictionary[' pig '] = 'Pigwidgeon'\n",
    "alias_dictionary['ernest'] = 'Ernest_Prang'\n",
    "replace_aliases('./Chapters_withouth_replacement/B3', './B3')\n",
    "alias_dictionary['ernest'] = 'Ernest_Macmillan'\n",
    "\n",
    "#Book 4\n",
    "alias_dictionary['frank'] = 'Frank_Bryce'\n",
    "replace_aliases('./Chapters_withouth_replacement/B4', './B4')\n",
    "alias_dictionary['frank'] = 'Frank_Longbottom'\n",
    "\n",
    "#Book 5\n",
    "alias_dictionary['mr and mrs potter'] = \"Fleamont_Potter and Euphemia_Potter\"\n",
    "alias_dictionary['rose'] = \"Rose_Zeller\"\n",
    "alias_dictionary['dennis'] = 'Dennis'\n",
    "replace_aliases('./Chapters_withouth_replacement/B5', './B5')\n",
    "del alias_dictionary['rose']\n",
    "del alias_dictionary['mr and mrs potter']\n",
    "alias_dictionary['dennis'] = 'Dennis Creevey'\n",
    "\n",
    "#Book 6\n",
    "alias_dictionary['avery'] = 'Avery_I'\n",
    "replace_aliases('./Chapters_withouth_replacement/B6', './B6')\n",
    "\n",
    "#Book 7\n",
    "replace_aliases('./Chapters_withouth_replacement/B7', './B7')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0637f32b",
   "metadata": {},
   "source": [
    "# Graph with weights\n",
    "Making a graph for each book. The nodes are the characters in the book, and edges are between characters that are in the same chapter. Edges have weight corresponding to the number of times those two characters are in the same chapter. Nodes have the attributes parentage, house, occupation and loyalty. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbac85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function to create a graph with weighted edges. \n",
    "For finding the edges we look at a given amount of sentences at a time \n",
    "and if two character are mentioned in this text piece they get an edge between them.\n",
    "The weight is the sum of how often two chracters appear together.\n",
    "\n",
    "Input: A list of character names with their attributes, \n",
    "       the path of the book,\n",
    "       how many sentences to look at at time\n",
    "Output: A weighted graph\n",
    "\"\"\"\n",
    "\n",
    "def weighted_temporal_graphs(character_list, path, sentence_no):\n",
    "    \n",
    "    G = nx.Graph()\n",
    "  \n",
    "    # Go throug each chapter in the book\n",
    "    for chapter in os.listdir(path): \n",
    "        #only look at the files where aliases have been replaced with character names\n",
    "        if \"replaced\" in chapter:   \n",
    "\n",
    "            # Get text\n",
    "            with open(path + chapter) as f:\n",
    "                text = f.read()\n",
    "            \n",
    "            # Put all characters from the chapter in the graph if they are not already there\n",
    "            for character in character_list:\n",
    "                if character[0] in text and character[0] not in list(G.nodes):\n",
    "                    G.add_node(character[0], parentage = character[1], \n",
    "                               house = character[2], occupation = character[3], loyalty = character[4])\n",
    "            \n",
    "            # Split the text in sentences \n",
    "            sentences = text.split(\". \")\n",
    "            count_start = 0\n",
    "            count_end = sentence_no\n",
    "            \n",
    "            # Look at specified amount of senteces at a time\n",
    "            while (count_start < len(sentences)):\n",
    "                current = sentences[count_start:count_end]\n",
    "                current = \" \".join(current)\n",
    "                \n",
    "                # Go through the nodes and check if two diffferent nodes appear in the same text piece\n",
    "                # if so add an edge\n",
    "                # weight is the amount of times they appear together throughout the book\n",
    "                for character_source in list(G.nodes):\n",
    "                    #print(character_source)\n",
    "                    for character_target in list(G.nodes):\n",
    "                        #print(character_target)\n",
    "                        if character_source is character_target:\n",
    "                            continue\n",
    "                        elif (character_source in current and character_target in current):\n",
    "                            if G.has_edge(character_source, character_target):\n",
    "                                G[character_source][character_target]['weight'] += 1\n",
    "                            else:\n",
    "                                G.add_edge(character_source, character_target, weight=1)\n",
    "                            #print(\"added some edge\")\n",
    "                count_start = count_end\n",
    "                count_end += sentence_no\n",
    "    # Remove nodes without edges\n",
    "    print(list(nx.isolates(G)))\n",
    "    G.remove_nodes_from(list(nx.isolates(G)))\n",
    "    #print(\"Done with graph\")\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae37627",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.operators.binary.compose.html#networkx.algorithms.operators.binary.compose\n",
    "\n",
    "\"\"\"\n",
    "Function to combine two graphs and adding the weights for the edges. \n",
    "Input: two graphs to combine\n",
    "Output: the combined graph, including the weights of the edges added together if same edges\n",
    "\"\"\"\n",
    "\n",
    "def combine_graphs(g1, g2):\n",
    "    combined= nx.compose(g1, g2)\n",
    "    edge_data = {e: g1.edges[e]['weight'] + g2.edges[e]['weight'] \n",
    "                 for e in g1.edges & g2.edges}\n",
    "    nx.set_edge_attributes(combined, edge_data, 'weight')\n",
    "    \n",
    "    return combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007c57a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Function to get the weight sum for each node in a graph.\n",
    "Input: a graph\n",
    "Output: A list of of the summed weights for the edges for each node,\n",
    "        this list is ordered as the list of nodes returned from graph.nodes\n",
    "\"\"\"\n",
    "\n",
    "def get_weight_sums(graph):\n",
    "    weight_sums = []\n",
    "    for node in list(graph.nodes):\n",
    "        sum = 0\n",
    "        for source, target in list(graph.edges):\n",
    "            if node is source or node is target:\n",
    "                sum += graph[source][target][\"weight\"]\n",
    "        weight_sums.append(sum)\n",
    "    return weight_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c64f45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/5294955/how-to-scale-down-a-range-of-numbers-with-a-known-min-and-max-value\n",
    "\"\"\"\n",
    "Function to scale the weight sums for the nodes. \n",
    "This is used when drawing the networks so we won't get too small or too big nodes. \n",
    "Input: a = minmum value for scaled weights\n",
    "       b = maximum value for scaled weights\n",
    "       G = graph\n",
    "Output: A list with the scaled weights\n",
    "\"\"\"\n",
    "\n",
    "def scaled_weights(a, b, G):\n",
    "    weights = get_weight_sums(G)\n",
    "    max_weight = max(weights)\n",
    "    min_weight = min(weights)\n",
    "    scaled = []\n",
    "    for w in weights:\n",
    "        scaled.append(((b - a) * (w - min_weight) // (max_weight - min_weight)) + a)\n",
    "    return scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e066bed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/13070461/get-indices-of-the-top-n-values-of-a-list\n",
    "\n",
    "\"\"\"\n",
    "Funtion to get the n top and bottom scaled node weights. \n",
    "Input: G = graph\n",
    "       n = Number of nodes from top to bottom\n",
    "       f = flag, True = top, False = bottom\n",
    "Output: A list with names of the n nodes\n",
    "        A list of node sizes as a tuple\n",
    "        A list of the indices\n",
    "\"\"\"\n",
    "\n",
    "def get_nodes_extreme(G, n, f):\n",
    "    if f:\n",
    "        n_indices = np.argsort(get_weight_sums(G))[-n:]\n",
    "    else:\n",
    "        n_indices = np.argsort(get_weight_sums(G))[0:n]\n",
    "    sc_weights = scaled_weights(50, 800, G)\n",
    "    names = []\n",
    "    weights = []\n",
    "    indices = []\n",
    "    for i in n_indices:\n",
    "        names.append(list(G.nodes())[i])\n",
    "        weights.append(sc_weights[i])\n",
    "        indices.append(i)\n",
    "    return (names, weights, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171452fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function to draw a network.\n",
    "Input: a graph,\n",
    "       title for the plot, default is empty string\n",
    "Output: plots the graph using forceAtlas\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def draw_network(graph, title=\"\"):\n",
    "    # Adjusting figure size\n",
    "    plt.rcParams['figure.figsize'] = [10, 10]\n",
    "\n",
    "    forceatlas2 = ForceAtlas2(\n",
    "                            # Behavior alternatives\n",
    "                            outboundAttractionDistribution=True,  # Dissuade hubs\n",
    "                            linLogMode=False,  # NOT IMPLEMENTED\n",
    "                            adjustSizes=False,  # Prevent overlap (NOT IMPLEMENTED)\n",
    "                            edgeWeightInfluence=1.0,\n",
    "\n",
    "                            # Performance\n",
    "                            jitterTolerance=1.0,  # Tolerance\n",
    "                            barnesHutOptimize=True,\n",
    "                            barnesHutTheta=1.2, # original 1.2\n",
    "                            multiThreaded=False,  # NOT IMPLEMENTED\n",
    "\n",
    "                            # Tuning\n",
    "                            scalingRatio=2.0,\n",
    "                            strongGravityMode=True,\n",
    "                            gravity=0.1, # original 0.5\n",
    "\n",
    "                            # Log\n",
    "                            verbose=True)\n",
    "\n",
    "    positions = forceatlas2.forceatlas2_networkx_layout(graph, pos=None, iterations=2000)\n",
    "    nx.draw_networkx_edges(graph, positions, edge_color=\"black\", alpha=0.1)\n",
    "    \n",
    "    # Setting the size of the nodes   \n",
    "    # Making 3 lists: top n max_weights, bottom n min_weights, rest\n",
    "    max_nodes, max_sizes, max_indices = get_nodes_extreme(graph, 10, True)\n",
    "    min_nodes, min_sizes, min_indices = get_nodes_extreme(graph, 10, False)\n",
    "    \n",
    "    rest_nodes = [n for n in list(graph.nodes()) if n not in max_nodes and n not in min_nodes]\n",
    "    rest_sizes = []\n",
    "    indices_to_remove = max_indices + min_indices\n",
    "    G_scaled_weights = scaled_weights(50, 800, graph)\n",
    "    for i in range(len(G_scaled_weights)):\n",
    "        if i not in indices_to_remove:\n",
    "            rest_sizes = G_scaled_weights[i]\n",
    "            \n",
    "    nx.draw_networkx_nodes(graph, positions, nodelist=rest_nodes, node_color='#efbc2f', node_size=rest_sizes,edgecolors = 'black', alpha=1)\n",
    "    nx.draw_networkx_nodes(graph, positions, nodelist=min_nodes, node_color='#366447', node_size=min_sizes, edgecolors = 'black', alpha=1)\n",
    "    nx.draw_networkx_nodes(graph, positions, nodelist=max_nodes, node_color='#a6332e', node_size=max_sizes, edgecolors = 'black', alpha=1)\n",
    "    \n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    plt.figtext(.5, -0.05, f\"The size of a note indicates the scaled sum of its weights.\", ha=\"center\")\n",
    "    # Used to save the fig for the paper\n",
    "    #plt.savefig('networkCombined.png', format='png', transparent=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843b30d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making graphs of book 1 where the no of senteces are changed\n",
    "graph_list = []\n",
    "graph_list.append(weighted_temporal_graphs(characters, \"B1/\", 5))\n",
    "graph_list.append(weighted_temporal_graphs(characters, \"B1/\", 10))\n",
    "graph_list.append(weighted_temporal_graphs(characters, \"B1/\", 20))\n",
    "graph_list.append(weighted_temporal_graphs(characters, \"B1/\", 30))\n",
    "graph_list.append(weighted_temporal_graphs(characters, \"B1/\", 40))\n",
    "graph_list.append(weighted_temporal_graphs(characters, \"B1/\", 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb66a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the networks with different no of sentences to see how big a text piece would be good\n",
    "sentence_len = [5, 10, 20, 30, 40, 50]\n",
    "\n",
    "for i, graph in enumerate(graph_list):\n",
    "    draw_network(graph, \"Book one network with interval of \"+str(sentence_len[i])+\" sentences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ed80b2",
   "metadata": {},
   "source": [
    ">By looking at the different networks we can see that it quickly becomes very dense with bigger text pieces, so we choose to use 5 sentences for the further graph creation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5d09d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making network separately for each book\n",
    "\n",
    "book_graphs = []\n",
    "book_graphs.append(weighted_temporal_graphs(characters, \"B1/\", 5))\n",
    "book_graphs.append(weighted_temporal_graphs(characters, \"B2/\", 5))\n",
    "book_graphs.append(weighted_temporal_graphs(characters, \"B3/\", 5))\n",
    "book_graphs.append(weighted_temporal_graphs(characters, \"B4/\", 5))\n",
    "book_graphs.append(weighted_temporal_graphs(characters, \"B5/\", 5))\n",
    "book_graphs.append(weighted_temporal_graphs(characters, \"B6/\", 5))\n",
    "book_graphs.append(weighted_temporal_graphs(characters, \"B7/\", 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a170f1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to create a figure of network of book 1 to save for the paper\n",
    "draw_network(book_graphs[0], \"Network of book 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff07a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drawing network of each book\n",
    "for i, graph in enumerate(book_graphs):\n",
    "    draw_network(graph, \"Network of book \" + str(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba365e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the networks of the books\n",
    "combined_nx = [book_graphs[0]]\n",
    "combined_nx.append(combine_graphs(combined_nx[0], book_graphs[1]))\n",
    "combined_nx.append(combine_graphs(combined_nx[1], book_graphs[2]))\n",
    "combined_nx.append(combine_graphs(combined_nx[2], book_graphs[3]))\n",
    "combined_nx.append(combine_graphs(combined_nx[3], book_graphs[4]))\n",
    "combined_nx.append(combine_graphs(combined_nx[4], book_graphs[5]))\n",
    "combined_nx.append(combine_graphs(combined_nx[5], book_graphs[6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94faf993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to create a figure of the network of all the books combined for the paper\n",
    "draw_network(combined_nx[6], \"Combined network of all books\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63b8842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drawing network of combined books\n",
    "for i, graph in enumerate(combined_nx):\n",
    "    title = ''\n",
    "    if i == 0:\n",
    "        title = \"Network of book 1\"\n",
    "    else:\n",
    "        title = \"Network of book 1-\" + str(i+1) \n",
    "    draw_network(graph, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a773bff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the character with biggest increase in percentage of its edges\n",
    "# Very hacky solution\n",
    "edge_increase = []\n",
    "startG = book_graphs[0]\n",
    "endG = combined_nx[6]\n",
    "for char in characters:\n",
    "    edgeStart = len(startG.edges(char[0]))\n",
    "    if edgeStart == 0:\n",
    "        edgeStart = len(book_graphs[1].edges(char[0]))\n",
    "        if edgeStart == 0:\n",
    "            edgeStart = len(book_graphs[2].edges(char[0]))\n",
    "            if edgeStart == 0:\n",
    "                edgeStart = len(book_graphs[3].edges(char[0]))\n",
    "                if edgeStart == 0:\n",
    "                    edgeStart = len(book_graphs[4].edges(char[0]))\n",
    "                    if edgeStart == 0:\n",
    "                        edgeStart = len(book_graphs[5].edges(char[0]))\n",
    "                        if edgeStart == 0:\n",
    "                            edgeStart = len(book_graphs[6].edges(char[0]))\n",
    "    if edgeStart == 0:\n",
    "        print(char[0])\n",
    "        continue\n",
    "    edgeEnd = len(endG.edges(char[0]))\n",
    "    increase = edgeEnd-edgeStart\n",
    "    percentage = increase*100/edgeStart\n",
    "    edge_increase += [(char[0], edgeStart, edgeEnd, percentage)]\n",
    "\n",
    "print(max(edge_increase,key=lambda item:item[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05ff024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to find info for the paper\n",
    "print(len(book_graphs[0].edges('Harry_Potter')))\n",
    "print(len(combined_nx[6].edges('Harry_Potter')))\n",
    "print(len(book_graphs[0].edges('Ronald_Weasley')))\n",
    "print(len(combined_nx[6].edges('Ronald_Weasley')))\n",
    "print(len(book_graphs[0].edges('Hermione_Granger')))\n",
    "print(len(combined_nx[6].edges('Hermione_Granger')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b715872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting number of edges belong to ten characters throughout the series:\n",
    "char = [\"Harry_Potter\", \"Ronald_Weasley\", \"Hermione_Granger\", \"Albus_Dumbledore\", \"Severus_Snape\", \"Tom_Riddle\", \n",
    "        \"Rubeus_Hagrid\", \"Draco_Malfoy\", \"Ginevra_Weasley\", \"Neville_Longbottom\"]\n",
    "font = 15\n",
    "no_edges = []\n",
    "networks = [1, 2, 3, 4, 5, 6, 7]\n",
    "plt.rcParams[\"figure.figsize\"] = (10,11)\n",
    "for c in char:\n",
    "    edges = []\n",
    "    for graph in combined_nx:\n",
    "        edges.append(len(list(graph.edges(c))))\n",
    "    no_edges.append(edges)\n",
    "\n",
    "colors = ['#a6332e', '#efbc2f', '#3c4e91', '#366447', '#aaaaaa', '#946b2d', 'orchid', '#d3a625', 'orangered', 'green']\n",
    "for i, e_list in enumerate(no_edges):\n",
    "    plt.plot(networks, e_list, label = char[i].replace('_', ' '), color = colors[i])\n",
    "plt.xlabel(\"No. of books combined\", fontsize = font)\n",
    "plt.ylabel(\"No. of edges\", fontsize = font)\n",
    "plt.legend(fontsize = font)\n",
    "plt.xticks(fontsize=font)\n",
    "plt.yticks(fontsize=font)\n",
    "plt.title(\"Evolution of the network of ten main characters\")\n",
    "plt.savefig('plotMainCharacters.png', format='png', transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a3b6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of the increase of edges and nodes throughout the books. With different y-axes in same plot. \n",
    "# Not used in paper as it was difficult to read\n",
    "no_nodes = []\n",
    "no_edges = []\n",
    "networks = [1, 2, 3, 4, 5, 6, 7]\n",
    "for graph in combined_nx:\n",
    "    no_nodes.append(graph.number_of_nodes())\n",
    "    no_edges.append(graph.number_of_edges())\n",
    "\n",
    "ax1 = plt.subplot()\n",
    "l1, = ax1.plot(networks, no_nodes, color='red')\n",
    "ax2 = ax1.twinx()\n",
    "l2, = ax2.plot(networks, no_edges, color='blue')\n",
    "ax1.set_xlabel('No. of books combined')\n",
    "ax1.tick_params(axis=\"y\", labelcolor='red')\n",
    "ax2.tick_params(axis=\"y\", labelcolor='blue')\n",
    "ax1.set_ylabel(\"No. of nodes\")\n",
    "ax2.set_ylabel(\"No. of edges\")\n",
    "\n",
    "plt.legend([l1, l2], [\"No. of nodes\", \"No. of edges\"])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb880ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot showing how many nodes there are in each book and for the books combined\n",
    "no_bookNodes = []\n",
    "no_combinedNodes = []\n",
    "font = 20\n",
    "#no_edges = []\n",
    "networks = [1, 2, 3, 4, 5, 6, 7]\n",
    "for graph in combined_nx:\n",
    "    no_combinedNodes.append(graph.number_of_nodes())\n",
    "for graph in book_graphs:   \n",
    "    no_bookNodes.append(graph.number_of_nodes())\n",
    "    \n",
    "counts, bins = np.histogram(no_combinedNodes, bins = 7)\n",
    "\n",
    "plt.plot(networks, no_combinedNodes, color = '#efbc2f', alpha = 1, label = 'Combined books')\n",
    "plt.bar(networks, no_bookNodes, color = '#3c4e91', edgecolor = 'black', alpha = 0.8)\n",
    "plt.xlabel('Book no.', fontsize=font)\n",
    "plt.ylabel('Number of nodes', fontsize=font)\n",
    "plt.legend(fontsize = font)\n",
    "plt.xticks(fontsize=font)\n",
    "plt.yticks(fontsize=font)\n",
    "plt.savefig('plotNodes.png', format='png', transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8386c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot showing how many edges there are in each book and for the books combined\n",
    "no_bookEdges = []\n",
    "no_combinedEdges = []\n",
    "#no_edges = []\n",
    "networks = [1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "font = 15\n",
    "for graph in combined_nx:\n",
    "    no_combinedEdges.append(graph.number_of_edges())\n",
    "for graph in book_graphs:   \n",
    "    no_bookEdges.append(graph.number_of_edges())\n",
    "\n",
    "plt.plot(networks, no_combinedEdges, color = '#366447', alpha = 1, label = 'Combined books')\n",
    "plt.bar(networks, no_bookEdges, color = '#a6332e', edgecolor = 'black', alpha = 0.8)\n",
    "plt.xlabel('Book no.', fontsize=font)\n",
    "plt.ylabel('Number of nodes', fontsize=font)\n",
    "plt.legend(fontsize = font)\n",
    "plt.xticks(fontsize=font)\n",
    "plt.yticks(fontsize=font)\n",
    "plt.savefig('plotEdges.png', format='png', transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c570421b",
   "metadata": {},
   "source": [
    "# Communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7bc300",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function to divide the graph in communities using the Louvain algorithm. \n",
    "Input: a graph to divide in communities\n",
    "Output: A list with the different communities\n",
    "\"\"\"\n",
    "\n",
    "def communities(graph):\n",
    "    partition = community_louvain.best_partition(graph)\n",
    "    #print(partition)\n",
    "    partition_list = []\n",
    "    \n",
    "    for com in set(partition.values()) :\n",
    "        list_nodes = [nodes for nodes in partition.keys()\n",
    "                                if partition[nodes] == com]\n",
    "        partition_list.append(list_nodes)\n",
    "    partition_list = sorted(partition_list, key=len, reverse=True)\n",
    "    #print(partition_list)\n",
    "    return partition_list   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63638be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make dictionary wordclouds with the character names and their weights\n",
    "# equal attributes are summarized\n",
    "# Was used to try and make wordclouds with the names of \n",
    "#the characters in the communities and their corresponding attributes\n",
    "#But this did not end up as we wanted so not used in the paper\n",
    "\n",
    "\"\"\"\n",
    "Input: a community and the graph it is extracted from\n",
    "Output: A dictionary of all the characters in the community,\n",
    "        the parentages, houses and occupations belonging to the characters.\n",
    "        Each character gets a value according to their sum\n",
    "        Each parentage, house and accupation is summed up for the total no. of characters belonging\n",
    "        to that parentage, house or occupation.\n",
    "\"\"\"\n",
    "\n",
    "def wordcloud_dict(community, graph):\n",
    "    cloud_freq = {}   \n",
    "    subG = graph.subgraph(community)\n",
    "    nodes = list(subG.nodes)\n",
    "    weights = get_weight_sums(subG)\n",
    "    parentages = nx.get_node_attributes(subG, 'parentage')\n",
    "    houses = nx.get_node_attributes(subG, 'house')\n",
    "    occupations = nx.get_node_attributes(subG, 'occupation')\n",
    "    \n",
    "    \n",
    "    for character in community:\n",
    "        parentage = parentages[character]\n",
    "        house = houses[character]\n",
    "        occupation = occupations[character]\n",
    "        \n",
    "        cloud_freq[character.replace('_', ' ')] = weights[nodes.index(character)]\n",
    "        \n",
    "        if parentage != 'other':\n",
    "            if parentage in cloud_freq:\n",
    "                cloud_freq[parentage] = cloud_freq.get(parentage) + 1\n",
    "            else:\n",
    "                cloud_freq[parentage] = 1\n",
    "        \n",
    "        if house != 'other':\n",
    "            if house in cloud_freq:\n",
    "                cloud_freq[house] = cloud_freq.get(house) + 1\n",
    "            else:\n",
    "                cloud_freq[house] = 1\n",
    "        \n",
    "        if occupation != 'other':\n",
    "            if occupation in cloud_freq:\n",
    "                cloud_freq[occupation] = cloud_freq.get(occupation) + 1\n",
    "            else:\n",
    "                cloud_freq[occupation] = 1\n",
    "    \n",
    "    return cloud_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df085a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Was used to try and make wordclouds with the names of \n",
    "#the characters in the communities and their corresponding attributes\n",
    "#But this did not end up as we wanted so not used in the paper\n",
    "\n",
    "\"\"\"\n",
    "Input: a list of communities and the graph they're extracted from\n",
    "Output: a list with a dictionary for each community\n",
    "\"\"\"\n",
    "\n",
    "def make_com_dicts(com_list, graph):\n",
    "    com_dicts = []\n",
    "\n",
    "    for com in com_list:\n",
    "        com_dicts.append(wordcloud_dict(com, graph))\n",
    "    \n",
    "    return com_dicts\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41715ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Was used to try and make wordclouds with the names of \n",
    "#the characters in the communities and their corresponding attributes\n",
    "#But this did not end up as we wanted so not used in the paper\n",
    "\n",
    "\"\"\"\n",
    "Input: a list of dictionaries\n",
    "Output: Wordclouds plotted for the community dictionaries given\n",
    "\"\"\"\n",
    "def draw_word_cloud(dicts):\n",
    "    fig = plt.figure()\n",
    "    plt.rcParams['figure.figsize'] = [15, 20]\n",
    "\n",
    "    for i in range(len(dicts)):\n",
    "        ax = fig.add_subplot(5,2,i+1)\n",
    "        wordcloud = WordCloud(background_color='black', width=2200,\n",
    "                          height=1800, collocations=False, \n",
    "                              # Different colormaps https://matplotlib.org/stable/tutorials/colors/colormaps.html\n",
    "                          colormap = plt.get_cmap('hsv', max(partition.values()) + 30)\n",
    "                        ).generate_from_frequencies(dicts[i])\n",
    "\n",
    "        ax.imshow(wordcloud)\n",
    "        ax.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6234ea89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Communitites from all books\n",
    "all_communities = communities(combined_nx[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d17996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# communities in each book:\n",
    "\n",
    "book_communities = []\n",
    "\n",
    "for graph in book_graphs:\n",
    "    book_communities.append(communities(graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b62a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print which book, the communities and the size of each community\n",
    "\n",
    "for i, com in enumerate(book_communities):\n",
    "    print('Book no.: ' + str(i+1))\n",
    "    print(com)\n",
    "    for part in com:\n",
    "        print(len(part))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4292c153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All those are not used. Was to see how it would turn out with using the names and attributes\n",
    "\n",
    "# Wordclouds for all books combined\n",
    "dictionary = make_com_dicts(all_communities, combined_nx[6])\n",
    "draw_word_cloud(dictionary)\n",
    "\n",
    "#Wordclouds for book 1:\n",
    "dictionary1 = make_com_dicts(book_communities[0], book_graphs[0])\n",
    "draw_word_cloud(dictionary1)\n",
    "\n",
    "#Wordclouds for book 2:\n",
    "dictionary2 = make_com_dicts(book_communities[1], book_graphs[1])\n",
    "draw_word_cloud(dictionary2)\n",
    "\n",
    "#Wordclouds for book 3:\n",
    "dictionary3 = make_com_dicts(book_communities[2], book_graphs[2])\n",
    "draw_word_cloud(dictionary3)\n",
    "\n",
    "#Wordclouds for book 4:\n",
    "dictionary4 = make_com_dicts(book_communities[3], book_graphs[3])\n",
    "draw_word_cloud(dictionary4)\n",
    "\n",
    "#Wordclouds for book 5:\n",
    "dictionary5 = make_com_dicts(book_communities[4], book_graphs[4])\n",
    "draw_word_cloud(dictionary5)\n",
    "\n",
    "#Wordclouds for book 6:\n",
    "dictionary6 = make_com_dicts(book_communities[5], book_graphs[5])\n",
    "draw_word_cloud(dictionary6)\n",
    "\n",
    "#Wordclouds for book 7:\n",
    "dictionary7 = make_com_dicts(book_communities[6], book_graphs[6])\n",
    "draw_word_cloud(dictionary7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e23e240",
   "metadata": {},
   "source": [
    "### Extracting texts from books belonging to communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8249497c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function to extract the text belonging to a community. \n",
    "We look in the book the community belongs to, and then for each character \n",
    "find the text pieces where they are mentioned and saves this text piece. \n",
    "Input: A list of communities.\n",
    "       the directory of the book,\n",
    "       how many sentences to read at a time\n",
    "Output: A list with the strings belonging to each community\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def extract_com_texts(com_list, book_dir, sentence_no):\n",
    "    community_texts = []\n",
    "    maxrange = 0\n",
    "    #if we have less than 10 communities\n",
    "    if len(com_list) < 10:\n",
    "        maxrange = len(com_list)\n",
    "    else:\n",
    "        maxrange = 10\n",
    "\n",
    "\n",
    "    for community in com_list[:maxrange]:\n",
    "        \n",
    "        com_txt = []        \n",
    "        for chap in os.listdir(book_dir):\n",
    "            #if chap == 'replaced_ch1.txt':\n",
    "            with open(book_dir + chap) as f:\n",
    "                    text = f.read()\n",
    "\n",
    "            sentences = text.split(\". \")\n",
    "            count_start = 0\n",
    "            count_end = sentence_no\n",
    "            add_text_start = []\n",
    "            \n",
    "            while (count_start < len(sentences)):\n",
    "                    current = sentences[count_start:count_end]\n",
    "                    current = \" \".join(current)\n",
    "                    for char in community:\n",
    "                        if char in current:\n",
    "                            # If we haven't already added the textpiece to this community, then add it\n",
    "                            # To make sure that we won't get the same textpiece several times in one community\n",
    "                            if not count_start in add_text_start:\n",
    "                                tokens = word_tokenize(current)\n",
    "                                com_txt = com_txt + tokens\n",
    "                                add_text_start.append(count_start)                                      \n",
    "                    count_start = count_end\n",
    "                    count_end += sentence_no\n",
    "                    \n",
    "        community_texts.append(com_txt)\n",
    "\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "    community_strings = []\n",
    "    for txt in community_texts:\n",
    "        com_words = [w for w in txt if w not in stopwords and len(w)>1]\n",
    "        community_strings.append(com_words)\n",
    "\n",
    "    return community_strings\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead4a3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the strings for the communities in each book\n",
    "com_strings1 = extract_com_texts(book_communities[0], './B1/', 5)\n",
    "com_strings2 = extract_com_texts(book_communities[1], './B2/', 5)\n",
    "com_strings3 = extract_com_texts(book_communities[2], './B3/', 5)\n",
    "com_strings4 = extract_com_texts(book_communities[3], './B4/', 5)\n",
    "com_strings5 = extract_com_texts(book_communities[4], './B5/', 5)\n",
    "com_strings6 = extract_com_texts(book_communities[5], './B6/', 5)\n",
    "com_strings7 = extract_com_texts(book_communities[6], './B7/', 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a047f235",
   "metadata": {},
   "source": [
    "### Extract wikitext for each community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37995f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to find the wikitexts belonging to each character in a community \n",
    "# Not used as we went with the text from the books instead\n",
    "def extract_com_wikitexts(com_list, directory):\n",
    "    community_texts = []\n",
    "    maxrange = 0\n",
    "    #if we have less than 10 communities\n",
    "    if len(com_list) < 10:\n",
    "        maxrange = len(com_list)\n",
    "    else:\n",
    "        maxrange = 10\n",
    "\n",
    "\n",
    "    for community in com_list[:maxrange]:\n",
    "        \n",
    "        com_txt = []  \n",
    "        \n",
    "        for char in community:\n",
    "            with open(directory + 'clean_' + char + '.txt') as f:\n",
    "                    text = f.read()\n",
    "            \n",
    "            tokens = nltk.word_tokenize(BeautifulSoup(text, 'html.parser').get_text())\n",
    "            #tokens = word_tokenize(current)\n",
    "            #file_text = [w.lower() for w in tokens if w.isalpha()]\n",
    "            com_txt = com_txt + tokens\n",
    "            \n",
    "        community_texts.append(com_txt)\n",
    "\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "    community_strings = []\n",
    "    for txt in community_texts:\n",
    "        com_words = [w for w in txt if w.lower() not in stopwords and len(w)>2]\n",
    "        community_strings.append(com_words)\n",
    "\n",
    "    return community_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c28e69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "com_wikistrings1 = extract_com_wikitexts(book_communities[0], './characters/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7774a06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Funtion to make a list of each unique word for the communities\n",
    "Input: List of community strings\n",
    "Output: A list with the unique terms for each community\n",
    "\"\"\"\n",
    "def unique(com_str):\n",
    "    unique_terms = []\n",
    "    for community_words in com_str:\n",
    "        unique_terms.append(list(set(community_words)))\n",
    "    \n",
    "    return unique_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea647d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_terms1 = unique(com_strings1)\n",
    "unique_terms2 = unique(com_strings2)\n",
    "unique_terms3 = unique(com_strings3)\n",
    "unique_terms4 = unique(com_strings4)\n",
    "unique_terms5 = unique(com_strings5)\n",
    "unique_terms6 = unique(com_strings6)\n",
    "unique_terms7 = unique(com_strings7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0449c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_wikiterms1 = unique(com_wikistrings1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daef0f17",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc07d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A function to calculate the IDF value for a word\n",
    "Input: A word,\n",
    "       a list with the uniqe term for each community\n",
    "Output: The IDF value found for the word\n",
    "\"\"\"\n",
    "\n",
    "def idf(word, unique_list):\n",
    "    N = len(unique_list)\n",
    "    term_appears = 0\n",
    "    for sublist in unique_list:\n",
    "        if word in sublist:\n",
    "            term_appears+=1\n",
    "    idf_val = math.log(N/(1+term_appears))+1\n",
    "    return idf_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047a4ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function to calculate the TF-IDF value for each word belonging to a community\n",
    "Input: A list with the community string,\n",
    "       a list with the uniqe terms for each community\n",
    "Output: A list containg the words and their TF-IDF values for each community\n",
    "\"\"\"\n",
    "\n",
    "def tfidf(community_str, unique_words):\n",
    "    tfidf_list = []\n",
    "\n",
    "    for community_words in community_str:\n",
    "        fdist = FreqDist(community_words)\n",
    "        total_terms = len(community_words)\n",
    "        tfidf=[]\n",
    "        for word in fdist:\n",
    "            idf_val = idf(word, unique_words)\n",
    "            tf_val = fdist[word]/total_terms\n",
    "            tfidf_elem=(word, tf_val*idf_val)\n",
    "            tfidf.append(tfidf_elem)\n",
    "        tfidf_list.append(tfidf)\n",
    "        \n",
    "    return tfidf_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9620235a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the tfidf list for each book\n",
    "tfidf_list1 = tfidf(com_strings1, unique_terms1)\n",
    "tfidf_list2 = tfidf(com_strings2, unique_terms2)\n",
    "tfidf_list3 = tfidf(com_strings3, unique_terms3)\n",
    "tfidf_list4 = tfidf(com_strings4, unique_terms4)\n",
    "tfidf_list5 = tfidf(com_strings5, unique_terms5)\n",
    "tfidf_list6 = tfidf(com_strings6, unique_terms6)\n",
    "tfidf_list7 = tfidf(com_strings7, unique_terms7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab70110",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_wikilist1 = tfidf(com_wikistrings1, unique_wikiterms1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e321f40",
   "metadata": {},
   "source": [
    "## Wordclouds for communities with book text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4e6d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function to create and plot the word clouds for the communities\n",
    "Input: A list with the words and their TF-IDF values for each community\n",
    "Output: A wordcloud plot for each community with their corresponding words\n",
    "\"\"\"\n",
    "\n",
    "def wordCloud(tfidf_list):\n",
    "    fig = plt.figure()\n",
    "    plt.rcParams['figure.figsize'] = [15, 20]\n",
    "\n",
    "    for i in range(len(tfidf_list)):\n",
    "        ax = fig.add_subplot(5,2,i+1)\n",
    "        wordcloud = WordCloud(background_color='white', width=2200,\n",
    "                          height=1800, collocations=False).generate_from_frequencies(dict(tfidf_list[i]))\n",
    "\n",
    "        ax.imshow(wordcloud)\n",
    "        ax.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a91c135",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCloud(tfidf_list1)\n",
    "\n",
    "wordCloud(tfidf_list2)\n",
    "\n",
    "wordCloud(tfidf_list3)\n",
    "\n",
    "wordCloud(tfidf_list4)\n",
    "\n",
    "wordCloud(tfidf_list5)\n",
    "\n",
    "wordCloud(tfidf_list6)\n",
    "\n",
    "wordCloud(tfidf_list7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4b480e",
   "metadata": {},
   "source": [
    "## Wordclouds for communities with wiki text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af46886",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCloud(tfidf_wikilist1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fac3fd4",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    ">We want to be able to analyse the sentiment of a character throughout the books. Our idea for achieving this is to use concordance from nltk with the character name, this way we will get all of the context surrounding a character. For each of these occurrences we can compute the sentiment for the context, and use that sentiment as a representative for the character. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5362befe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vriables to be used for the sentiment analysis\n",
    "average_length_of_sentence = 114\n",
    "sentences_for_con = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980e8534",
   "metadata": {},
   "source": [
    "## Sentiment calculations from LabMT1.0 vs VADER-Sentiment\n",
    ">We have considered two options for calculating our sentiment. Either we could use the the LabMT1.0 data set to find the sentiment of a portion of text by assigning each word in that text a value based on LabMT1.0, and then taking the average of those words. Or we could use <a href=\"https://github.com/cjhutto/vaderSentiment/blob/master/README.rst\">VADER-Sentiment</a>. We wanted to experiment with the VADER solution, since our initial findings for sentiment using LabMT1.0 had very similar values around 5.5. To experiment we have made a graph of sentiments for each chapter of Book 7, for Harry Potter, Voldemort, and Snape for both methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2ca583",
   "metadata": {},
   "source": [
    "###  Sentiment for concordance of character, LabMT1.0\n",
    "> First we create a list of tuples containing each word and the average happiness for that word. This allows us to go through a portion of text and look up the average happiness for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c6c79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store tuples of words and their \n",
    "# average happiness score\n",
    "sent_list_labmt10 = []\n",
    "\n",
    "# Read in the .tsv file\n",
    "with open(\"LabMT1.0.tsv\", \"r\", encoding=\"utf8\") as sent_file:\n",
    "    tsv_reader = csv.DictReader(sent_file, delimiter=\"\\t\")\n",
    "    # For each sentiment in the file, save the word and average happiness in a tuple\n",
    "    # and add it to the list\n",
    "    for sent in tsv_reader:\n",
    "        word = sent[\"word\"]\n",
    "        average = sent[\"happiness_average\"]\n",
    "        sent_list_labmt10.append((word, average))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64e103d",
   "metadata": {},
   "source": [
    ">We now define a function to calculate the average sentiment for a set of tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9f740a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function to calculate sentiment for a list of tokens\n",
    "# Function for calculating the sentiment of a file from the frequency distribution for that file\n",
    "def sentiment_labmt10(tokens):\n",
    "    # Total sentiment score of file\n",
    "    sent_sum_labmt10 = 0.0\n",
    "    # Total number of occurences of words\n",
    "    occ_sum_labmt10 = 0\n",
    "    \n",
    "    # For each token and associated number of occurences\n",
    "    for token, occ in tokens.items():\n",
    "        # If the token is in the given list of words with rated happiness\n",
    "        for word, score in sent_list_labmt10:\n",
    "            if token == word:\n",
    "                sent_sum_labmt10 += (float(score) * occ)\n",
    "                occ_sum_labmt10 += occ\n",
    "    return sent_sum_labmt10 / occ_sum_labmt10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc071cf",
   "metadata": {},
   "source": [
    ">And a function to calculate the sentiment for a concordance list found by nltk, and define a list of stopwords to be filtered out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18261157",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_labmt10 = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9cf5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding sentiment from a concordance list\n",
    "def con_sentiment_labmt10(con_list):\n",
    "    sent_sum = 0\n",
    "    line_num = 0\n",
    "    for item in con_list:\n",
    "        left = [ch.lower() for ch in item.left if ch.isalpha()]\n",
    "        right = [ch.lower() for ch in item.right if ch.isalpha()]\n",
    "        # Make left and right into one list and remove stopwords\n",
    "        combined = [w for w in (left + right) if w not in stopwords_labmt10]\n",
    "        \n",
    "        # Make frequency distribution \n",
    "        fd = nltk.FreqDist(combined)\n",
    "        sent_sum += sentiment_labmt10(fd)\n",
    "        line_num += 1\n",
    "    if line_num == 0:\n",
    "        return None # Character had no appearences in chapter\n",
    "    return sent_sum / line_num"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb0184e",
   "metadata": {},
   "source": [
    ">We then run our test as described previously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d10fcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define character list with the three characteres\n",
    "char_list_labmt10 = [\"Harry_Potter\", \"Severus_Snape\", \"Tom_Riddle\"] \n",
    "\n",
    "# Create a list of the chapters of book 7\n",
    "chapters_labmt10 = os.listdir(\"B7\")\n",
    "\n",
    "# Init a list to tuples (chraracter, [sentiments for each chapter for that character])\n",
    "sentiment_by_character_labmt10 = []\n",
    "\n",
    "# For each of the characters\n",
    "for character in char_list_labmt10:\n",
    "    sentiments = []\n",
    "    # For each chapter\n",
    "    for chapter in chapters_labmt10:\n",
    "        # Read in and tokenize the chapter\n",
    "        if \"replaced\" in chapter:\n",
    "            with open(\"B7/\" + chapter) as f:\n",
    "                    raw = f.read()\n",
    "            tokens = nltk.word_tokenize(raw)\n",
    "            text = nltk.Text(tokens)\n",
    "        \n",
    "            # Make concordance for that character\n",
    "            con = text.concordance_list(character, width = sentences_for_con * average_length_of_sentence)\n",
    "        \n",
    "            # Calculate sentiments and append to the list for that character\n",
    "            sentiments.append(con_sentiment_labmt10(con))\n",
    "    # Append the character with its full sentiment list\n",
    "    sentiment_by_character_labmt10.append((character, sentiments))\n",
    "\n",
    "# https://stackoverflow.com/questions/4971269/how-to-pick-a-new-color-for-each-plotted-line-within-a-figure-in-matplotlib\n",
    "color = iter(cm.rainbow(np.linspace(0, 1, len(char_list_labmt10))))\n",
    "\n",
    "for name, sentiments in sentiment_by_character_labmt10:\n",
    "    c = next(color)\n",
    "    plt.plot(sentiments, c=c, label=name)\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Chapter')\n",
    "plt.ylabel('Average sentiment')\n",
    "plt.title('Average sentiment by chapter for selected characters')\n",
    "plt.figtext(.5, -0.1, f\"Plot of the average sentiment for Harry, Snape, and Voldemort in book 7 when calculating sentiment from LabMT1.0.\", ha=\"center\")\n",
    "\n",
    "plt_labmt = plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c79393f",
   "metadata": {},
   "source": [
    "### Sentiment for concordance of character, vaderSentiment\n",
    ">For VADER we use the same approach and code, but we have to redefine how we calculate sentiment from concordance, since we just have to pass a string to the analyzer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea50c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding sentiment from a concordance list\n",
    "def con_sentiment(con):\n",
    "    sent_sum = 0\n",
    "    line_num = 0\n",
    "    combined = con.left + con.right\n",
    "    combined = \" \".join(combined)\n",
    "\n",
    "    vs = analyzer.polarity_scores(combined)\n",
    "\n",
    "    if vs == 0:\n",
    "        return None # Character had no appearences in chapter\n",
    "    return vs[\"compound\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215959f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define character list with the three characteres\n",
    "char_list_vader = [\"Harry_Potter\", \"Severus_Snape\", \"Tom_Riddle\"] \n",
    "\n",
    "# Create a list of the chapters of book 7\n",
    "chapters_vader = os.listdir(\"B7\")\n",
    "\n",
    "# Init a list to tuples (chraracter, [sentiments for each chapter for that character])\n",
    "sentiment_by_character_vader  = []\n",
    "\n",
    "# For each of the characters\n",
    "for character in char_list_vader :\n",
    "    sentiments = []\n",
    "    for chapter in chapters_vader :\n",
    "        if \"replaced\" in chapter:\n",
    "            with open(\"B7/\" + chapter) as f:\n",
    "                    raw = f.read()\n",
    "            tokens = nltk.word_tokenize(raw)\n",
    "            text = nltk.Text(tokens)\n",
    "            cons = text.concordance_list(character, width = sentences_for_con * average_length_of_sentence)\n",
    "            sent_sum = 0\n",
    "            lines = 0\n",
    "            for con in cons:\n",
    "                sent_sum += con_sentiment(con)\n",
    "                lines += 1\n",
    "            if lines == 0:\n",
    "                sentiments.append(None)\n",
    "            else: \n",
    "                sentiments.append(sent_sum / lines)\n",
    "    sentiment_by_character_vader .append((character, sentiments))\n",
    "\n",
    "# https://stackoverflow.com/questions/4971269/how-to-pick-a-new-color-for-each-plotted-line-within-a-figure-in-matplotlib\n",
    "color_vader  = iter(cm.rainbow(np.linspace(0, 1, len(char_list_vader ))))\n",
    "\n",
    "for name, sentiments in sentiment_by_character_vader :\n",
    "    c = next(color_vader)\n",
    "    plt.plot(sentiments, c=c, label=name)\n",
    "\n",
    "plt.axhline(y = 0.05, color =\"purple\", linestyle = '--', label=\"Neutral region\")\n",
    "plt.axhline(y = -0.05, color =\"purple\", linestyle = '--')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Chapter')\n",
    "plt.ylabel('Average sentiment')\n",
    "plt.title('Average sentiment by chapter for selected characters')\n",
    "plt.figtext(.5, -0.1, f\"Plot of the average sentiment for Harry, Snape, and Voldemort in book 7 when calculating sentiment with VADER.\", ha=\"center\")\n",
    "\n",
    "plt_vader = plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423a00aa",
   "metadata": {},
   "source": [
    "### Conclusion: LabMT1.0 vs vaderSentiment\n",
    ">Both results are similar, but we see an advantage in using VADER when we consider the sentiment for Harry Potter. Looking at the graphs we can see that there are similar trends for Harry throughout the book, but with VADER the sentiment becomes more consistent, in that it appears to be in the neutral region at some points, and then go out of it. On the other hand the MatLab1.0 seems to indicate that Harry is well above 5.1 throughout the book, which we would consider to be above neutral. Based on this preliminary test we believe that we will get a more clear picture from VADER."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaa99f4",
   "metadata": {},
   "source": [
    "## Functions for calculating sentiment\n",
    ">The following section contains various functions we have defined to find and plot sentiment for various cases.\n",
    "\n",
    "### ```sent_chars_book(char_list, path_to_book)```\n",
    ">The function takes a list of character names and a path to a book. It computes the average sentiment of each character in the list throughout the book on a chapter basis. This can be used to find out how the sentiment of a single character changes throughout a book or a group of characters such as a house."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5f83c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input: A list of character names to look for as tokens\n",
    "        and a path to the chapters of the book\n",
    "Output: A list of tuples, with the character name and a list\n",
    "        of the sentiments for each chapter for that character\n",
    "\"\"\"\n",
    "def sent_chars_book_list(char_list, book_list, sentences):\n",
    "    # Init list to hold tuples\n",
    "    sentiment_of_character = [(n, []) for n in char_list]\n",
    "    #print(sentiment_of_character)\n",
    "    #print(len(sentiment_of_character))\n",
    "    #print(char_list)\n",
    "    \n",
    "    chapter_counter = 0\n",
    "    for book in book_list:\n",
    "        \n",
    "        # For each chapter\n",
    "        for chapter in os.listdir(book):\n",
    "            if \"replaced\" in chapter:\n",
    "                # Read in the chapter and tokenize\n",
    "                with open(book + chapter) as f:\n",
    "                    raw = f.read()\n",
    "                tokens = nltk.word_tokenize(raw)\n",
    "                text = nltk.Text(tokens)\n",
    "\n",
    "                character_counter = 0\n",
    "                # For each character in the given list\n",
    "                for character in char_list:\n",
    "                    # Make concordance for that character in that chapter\n",
    "                    cons = text.concordance_list(character, width = sentences * average_length_of_sentence)\n",
    "                    #print(character)\n",
    "                    #print(f\"character = {character}, sentiment_of_character[{character_counter}] = {sentiment_of_character[character_counter]}\")\n",
    "\n",
    "                    sent_sum = 0\n",
    "                    lines = 0\n",
    "                    # For each concordance line\n",
    "                    for con in cons:\n",
    "                        # Calculate the sentiment for that concordance line\n",
    "                        sent_sum += con_sentiment(con)\n",
    "                        lines += 1\n",
    "                    if lines == 0:\n",
    "                        # If there were no lines, the character did not appear\n",
    "                        sentiment_of_character[character_counter][1].append(None)\n",
    "                    else: \n",
    "                        sentiment_of_character[character_counter][1].append(sent_sum / lines)\n",
    "                    if character_counter > len(sentiment_of_character):\n",
    "                        print(\"!!!!!!!!!!!!!!!!!!!!!!!! WRONG\")\n",
    "                    character_counter += 1\n",
    "    # Returns a list of the sentiments for that character for each chapter of that book\n",
    "    return sentiment_of_character"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d21481",
   "metadata": {},
   "source": [
    "### ```sent_book(path_to_book)```\n",
    ">Calculates the sentiment for a book on chapter basis. In this function each chapter of a book is read in and the sentiment for the text is calculated. This allows us to see how the sentiment for a book changes as it progresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66d62ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input: The path to a book.\n",
    "Output: A list of sentiments for each chapter of the book.\n",
    "\"\"\"\n",
    "def sent_book(path_to_book):\n",
    "    chapters = os.listdir(path_to_book)\n",
    "\n",
    "    # Making a data table (char_list)x(num_chapters) to hold sentiments for each char in each chap\n",
    "    # tuple list with tuples (character, [sent chapter1, sent chapter2, ...])\n",
    "    sentiments_by_chapter = []\n",
    "    \n",
    "    for chapter in chapters:\n",
    "        if \"replaced\" in chapter:\n",
    "            with open(path_to_book + chapter) as f:\n",
    "                raw = f.read()\n",
    "            sentiments_by_chapter.append(analyzer.polarity_scores(raw)[\"compound\"])\n",
    "    # Returns a list of the sentiments for that character for each chapter of that book\n",
    "    return sentiments_by_chapter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3425de43",
   "metadata": {},
   "source": [
    "### ```sent_group(group, label, book_list)```\n",
    ">This function calculates the average sentiments for all of the names given in ```group``` and returns it as as the sentiment for the name from ```label_group```. The sentiments are calculated from the books given in ```book_list```. e.g. given the list of names of Gryffindor students, with label \"Gryffindor\" returns a tuple (\"Gryffindor\", sentiment list), where the sentiment list contains the average sentiment for each chapter for those students."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c3930f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avgsent_group(group, label, book_list, sentences):\n",
    "    # Count number of chapters\n",
    "    num_chapters = 0\n",
    "    for book in book_list:\n",
    "        for chapter in os.listdir(book):\n",
    "            if \"replaced\" in chapter:\n",
    "                num_chapters += 1\n",
    "   \n",
    "    sent_chapters = [0] * num_chapters\n",
    "    sent_group = (label, sent_chapters)\n",
    "    \n",
    "    # Counter for current chapter\n",
    "    c = 0\n",
    "    \n",
    "    # For each book in the list\n",
    "    for book in book_list:  \n",
    "        # For each chapter going by numbering\n",
    "        for chapter in os.listdir(book):\n",
    "            if \"replaced\" in chapter:\n",
    "                # Init counter for counting occurences in chapter for average\n",
    "                occurences = 0\n",
    "\n",
    "                # Read in the chapter\n",
    "                with open(book + chapter) as f:\n",
    "                        raw = f.read()\n",
    "                tokens = nltk.word_tokenize(raw)\n",
    "                # Prepare nltk text\n",
    "                text = nltk.Text(tokens)\n",
    "\n",
    "                # For each member of the group\n",
    "                for member in group:\n",
    "                    # Make concordance for that member for that chapter\n",
    "                    con_list = text.concordance_list(member, width = sentences * average_length_of_sentence)\n",
    "\n",
    "                    # For each concordance line in the list\n",
    "                    for con in con_list:\n",
    "                        # Calculate the sentiment\n",
    "                        sent = con_sentiment(con)\n",
    "                        if sent != 0:\n",
    "                            # Sum up the sentiment for that chapter for that member\n",
    "                            # with sentiments for all other members of group\n",
    "                            #print(f\"c = {c}\")\n",
    "                            sent_group[1][c] += sent\n",
    "                            occurences += 1\n",
    "                # Divide by the total number of occurences \n",
    "                if occurences == 0:\n",
    "                    sent_group[1][c] = None\n",
    "                else:\n",
    "                    sent_group[1][c] = sent_group[1][c] / occurences\n",
    "                c += 1\n",
    "    return sent_group                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1946d9c4",
   "metadata": {},
   "source": [
    "### ```plot_sentiments(sentiment_by_character, figure_text, xs_vertical_lines, show_legend)```\n",
    ">The functions takes a list of tuples, where each tuple contains a name and a list of sentiments. A figure text, a list of tuples for placing vertical lines with labels, and a ```True```/```False```flag for show lgend. This function may take the result of ```sent_char_books``` a sinput for the list of tuples with names and sentiments. This allows for fast and simple plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7ecdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input: A list of tuples containing a name and a sentiment list, a figure text, \n",
    "       a list of tuples with labels and coordinates for vertical lines, and a\n",
    "       true/false value for showing labels.\n",
    "Output: void, shows a plot\n",
    "\"\"\"\n",
    "def plot_sentiments(sentiment_by_character, figure_text, xs_vertical_lines, show_legend):\n",
    "    # Init iterator\n",
    "    color = iter(cm.rainbow(np.linspace(0, 3, len(sentiment_by_character)*3)))\n",
    "    plt.xlim(0, len(sentiment_by_character[0][1]))\n",
    "    \n",
    "    for name, sentiments in sentiment_by_character:\n",
    "        # Try to give colors according the name associated with the sentiments\n",
    "        if (\"Gryffindor\" in name):\n",
    "            c = \"red\"\n",
    "            a = 1\n",
    "        elif (\"Slytherin\" in name and name != \"Salazar_Slytherin\"):\n",
    "            c = \"green\"\n",
    "            a = 1\n",
    "        elif (\"Hufflepuff\" in name):\n",
    "            c = \"yellow\"\n",
    "            a = 1\n",
    "        elif (\"Ravenclaw\" in name):\n",
    "            c = \"blue\"\n",
    "            a = 1\n",
    "        elif (\"Average\" in name):\n",
    "            c = \"black\"\n",
    "            a = 1\n",
    "        else:\n",
    "            c = next(color)\n",
    "            a = 0.5\n",
    "        plt.plot(sentiments, c=c, alpha=a, label=name)\n",
    "    \n",
    "    # Make horizontal lines to indicate the neutral region\n",
    "    plt.axhline(y = 0.05, color =\"purple\", linestyle = '--', label=\"Neutral\")\n",
    "    plt.axhline(y = -0.05, color =\"purple\", linestyle = '--')\n",
    "    \n",
    "    # If vertical lines for book has been specified insert them\n",
    "    for book, label, vertical_line in xs_vertical_lines:\n",
    "        plt.axvline(x = vertical_line, color = 'black')\n",
    "        \n",
    "    # Get list of labels and vertical_lines\n",
    "    if xs_vertical_lines != []:\n",
    "        xs = []\n",
    "        labels = []\n",
    "        for book, x_coordinate, label in xs_vertical_lines:\n",
    "            xs.append(x_coordinate)\n",
    "            labels.append(label)\n",
    "        plt.xticks(labels, xs ,rotation=45)\n",
    "\n",
    "    # If legend has been requested\n",
    "    if show_legend:\n",
    "        plt.legend()\n",
    "\n",
    "    #plt.xlabel('Chapter')\n",
    "    plt.ylabel('Average sentiment')\n",
    "    plt.figtext(.5, -0.2, figure_text, ha=\"center\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3965615e",
   "metadata": {},
   "source": [
    "## Sentiment for books\n",
    ">First we want to explore how the sentiment is throughout the books. We have two ideas for measuring this: Take each chapter as a text and have VADER analyze the sentiment of that text for us. Or for each character from our list of characters, make concordance for each chapter for that character, and divide it by the total number of concordance lines for that chapter. We are going to test these two methods out to see which is more expressive:\n",
    "\n",
    ">We start by defining some list needed for plotting and reading in all the books:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25d08d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuple list containing the first chapter of the next book, e.g. (\"B1\", 18), indicates\n",
    "# all chapters up to 18 excluded are from book 1\n",
    "book_list_wchapter = [(\"B1\", \"Philosopher's Stone\", 0), \n",
    "                      (\"B2\", \"Chamber of Secrets\", 18),\n",
    "                      (\"B3\", \"Prisoner of Azkaban\", 35), \n",
    "                      (\"B4\", \"Goblet of Fire\", 57), \n",
    "                      (\"B5\", \"Order of the Phoenix\", 94), \n",
    "                      (\"B6\", \"Half-Blood Prince\", 134), \n",
    "                      (\"B7\", \"Deathly Hallows\", 162)]\n",
    "# Lists of paths to the folders holding the text from the chapters\n",
    "book_list = [\"B1/\", \"B2/\", \"B3/\", \"B4/\", \"B5/\", \"B6/\", \"B7/\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d1f8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init list to hold sentiment values\n",
    "series_sent = []\n",
    "# Compute and append the sentiment values for the seven books\n",
    "series_sent += sent_book(\"B1/\")\n",
    "series_sent += sent_book(\"B2/\")\n",
    "series_sent += sent_book(\"B3/\")\n",
    "series_sent += sent_book(\"B4/\")\n",
    "series_sent += sent_book(\"B5/\")\n",
    "series_sent += sent_book(\"B6/\")\n",
    "series_sent += sent_book(\"B7/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e48dfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure text\n",
    "sent_by_chapter = \"Sentiment by chapter for entire series analyzed one chapter at a time\"\n",
    "# Adjust for a wider figure size\n",
    "plt.rcParams['figure.figsize'] = [15, 5]\n",
    "# Plot the sentiment values for all of the books when VADER analyzed each chapter as a whole text\n",
    "plot_sentiments([(\"Sentiment for series\", series_sent)], sent_by_chapter, book_list_wchapter, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1f979c",
   "metadata": {},
   "source": [
    ">The above figure does not convey changes in the book very well, at most it seems that we can get an idea of the overall tone of the chapter, but not how it relates to the other chapters or how the story evolves and changes. \n",
    "\n",
    ">For the next part we are going to try the approach with making concordance for all of the characters for each chapter and taking the average. However ```concordance_list``` uses a default width of 80 characters around the string that it is making concordance for. First we have to investigate if this default value is good. To investigate this we first determine the average length in characters for the books:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bb7d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What the average length of a sentence in harry potter? in characters\n",
    "# Read in all of the chapters and count the length of the sentences, divide by the number ound\n",
    "# use this for average to determine how many sentences we want included for each concordance?\n",
    "number_of_sentences = 0\n",
    "number_of_characters = 0\n",
    "for book in book_list:\n",
    "    for chapter in os.listdir(book):\n",
    "        if \"replaced\" in chapter:\n",
    "            with open(book + chapter) as f:\n",
    "                raw = f.read()\n",
    "            sentences = nltk.sent_tokenize(raw)\n",
    "            for sentence in sentences:\n",
    "                number_of_characters += len(sentence)\n",
    "                number_of_sentences += 1\n",
    "average_characters_in_sent = number_of_characters / number_of_sentences\n",
    "print(average_characters_in_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253c5f1b",
   "metadata": {},
   "source": [
    ">The average sentence in the books is 114 characters long. Our default width for a concordance should then be 114 characters, assuming the character name is in the middle of the sentence and has an even length. So now we can test out how many sentences should be used. We are going to test with a smaller size of a half sentence, default, one sentence, two, three, and four to start with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4f83e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "con_sentences_test = []\n",
    "average_length_of_sentence = 79\n",
    "con_sentences_test.append(avgsent_group([\"Harry_Potter\"], \"Harry Potter Default\", [\"B1/\"], 1))\n",
    "average_length_of_sentence = 114\n",
    "con_sentences_test.append(avgsent_group([\"Harry_Potter\"], \"Harry Potter 1\", [\"B1/\"], 1))\n",
    "con_sentences_test.append(avgsent_group([\"Harry_Potter\"], \"Harry Potter 2\", [\"B1/\"], 2))\n",
    "con_sentences_test.append(avgsent_group([\"Harry_Potter\"], \"Harry Potter 3\", [\"B1/\"], 3))\n",
    "con_sentences_test.append(avgsent_group([\"Harry_Potter\"], \"Harry Potter 4\", [\"B1/\"], 4))\n",
    "plot_sentiments(con_sentences_test, \"Average sentiment for Harry Potter in book one, with 0.5 sentences used for concordance\", [], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c151870b",
   "metadata": {},
   "outputs": [],
   "source": [
    "con_sentences_test = []\n",
    "con_sentences_test.append(avgsent_group([\"Harry_Potter\"], \"Harry Potter 8\", [\"B1/\"], 8))\n",
    "con_sentences_test.append(avgsent_group([\"Harry_Potter\"], \"Harry Potter 16\", [\"B1/\"], 16))\n",
    "con_sentences_test.append(avgsent_group([\"Harry_Potter\"], \"Harry Potter 32\", [\"B1/\"], 32))\n",
    "con_sentences_test.append(avgsent_group([\"Harry_Potter\"], \"Harry Potter 64\", [\"B1/\"], 64))\n",
    "plot_sentiments(con_sentences_test, \"Average sentiment for Harry Potter in book one, with differening number of sentences used for concordance\", [], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356c2ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the character names from our list with attributes\n",
    "character_names = [n for n, b, h, o in characters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bab531",
   "metadata": {},
   "outputs": [],
   "source": [
    "con_test_all = []\n",
    "con_test_all.append(avgsent_group(character_names, \"All characters, 2 sentences con\", [\"B1/\"], 2))\n",
    "con_test_all.append(avgsent_group(character_names, \"All characters, 3 sentences con\", [\"B1/\"], 3))\n",
    "con_test_all.append(avgsent_group(character_names, \"All characters, 4 sentences con\", [\"B1/\"], 4))\n",
    "con_test_all.append(avgsent_group(character_names, \"All characters, 32 sentences con\", [\"B1/\"], 32))\n",
    "con_test_all.append((\"book 1 sent\", sent_book(\"B1/\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d43ec70",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sentiments(con_test_all, sent_by_chapter, [], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ad47b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "con_test_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312e8d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average sentiment for each chapter\n",
    "avgsent_all = avgsent_group(character_names, \"Average sentiment of all characters\", book_list, sentences_for_con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b40c5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_books = \"Average sentiment from concordance of character names throughout the books.\"\n",
    "plot_sentiments([avgsent_all], sentiment_books, book_list_wchapter, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a080d1e",
   "metadata": {},
   "source": [
    "## Sentiment for houses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9066b11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gryffindors = [n for n, p, h, o in characters if h == \"Gryffindor\"]\n",
    "hufflepuffs = [n for n, p, h, o in characters if h == \"Hufflepuff\"]\n",
    "ravenclaws = [n for n, p, h, o in characters if h == \"Ravenclaw\"]\n",
    "slytherins = [n for n, p, h, o in characters if h == \"Slytherin\"]\n",
    "\n",
    "print(f\"Number of characters from Gryffindor: {len(gryffindors)}\")\n",
    "print(f\"Number of characters from Hufflepuff: {len(hufflepuffs)}\")\n",
    "print(f\"Number of characters from Ravenclaw: {len(ravenclaws)}\")\n",
    "print(f\"Number of characters from slytherin: {len(slytherins)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b937a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "avgsent_gryffindor = avgsent_group(gryffindors, \"Gryffindors\", book_list, sentences_for_con)\n",
    "avgsent_hufflepuff = avgsent_group(hufflepuffs, \"Hufflepuffs\", book_list, sentences_for_con)\n",
    "avgsent_ravenclaw = avgsent_group(ravenclaws, \"Ravencalws\", book_list, sentences_for_con)\n",
    "avgsent_slytherin = avgsent_group(slytherins, \"Slytherins\", book_list, sentences_for_con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444839c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_sent_list = [avgsent_gryffindor, \n",
    "                 avgsent_slytherin,\n",
    "                 avgsent_hufflepuff,\n",
    "                 avgsent_ravenclaw]\n",
    "plot_sentiments(avg_sent_list, \"Average sentiment for the four houses throughout the books\", book_list_wchapter, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bf90b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = \"Gryffindors\"\n",
    "gryff_sents = sent_chars_book_list(gryffindors, book_list, sentences_for_con) + [avgsent_gryffindor]\n",
    "\n",
    "text_gryff_sent = \"Average sentiments for Gryffindor, and sentimens for all Gryffindors\"\n",
    "plot_sentiments(gryff_sents, text_gryff_sent, book_list_wchapter, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0705e878",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = \"Slytherins\"\n",
    "slyth_sents = sent_chars_book_list(slytherins, book_list, sentences_for_con) + [avgsent_slytherin]\n",
    "\n",
    "text_slyth_sent = \"Average sentiments for Slytherin, and sentimens for all Slytherins\"\n",
    "plot_sentiments(slyth_sents, text_slyth_sent, book_list_wchapter, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14b0768",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = \"Hufflepuffs\"\n",
    "huff_sents = sent_chars_book_list(hufflepuffs, book_list, sentences_for_con) + [avgsent_hufflepuff]\n",
    "\n",
    "# This plot have some None values so will be holes in the plot\n",
    "text_huff_sent = \"Average sentiments for Hufflepuff, and sentimens for all Hufflepuffs\"\n",
    "plot_sentiments(huff_sents, text_huff_sent, book_list_wchapter, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07719b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = \"Ravenclaws\"\n",
    "rave_sents = sent_chars_book_list(ravenclaws, book_list, sentences_for_con) + [avgsent_ravenclaw]\n",
    "\n",
    "# This plot have some None values so will be holes in the plot\n",
    "text_rave_sent = \"Average sentiments for Ravenclaw, and sentimens for all Ravenclaws\"\n",
    "plot_sentiments(rave_sents, text_rave_sent, book_list_wchapter, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a94f8cc",
   "metadata": {},
   "source": [
    "## Highest and lowest sentiments\n",
    ">Investigating which characters have the highest and lowest sentiments by summing up their average sentiment values for each chapter they appeared in, and dividing by the number of chapters they appeared in into a sentiment score for that character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8318a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "avgsent_all_individual = sent_chars_book_list(character_names, book_list, sentences_for_con)\n",
    "character_sent_scores = []\n",
    "for name, sent_list in avgsent_all_individual:\n",
    "    sent_score = 0\n",
    "    sent_sum = 0\n",
    "    chapter_occurences = 0\n",
    "    for sent in sent_list:\n",
    "        if sent != None:\n",
    "            sent_sum += sent\n",
    "            lines += 1\n",
    "    sent_score = sent_sum\n",
    "    character_sent_scores.append((name, sent_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90d164a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://bobbyhadz.com/blog/python-sort-list-of-tuples-by-second-element\n",
    "sorted_list = sorted(\n",
    "    character_sent_scores,\n",
    "    key=lambda t: t[1]\n",
    ")\n",
    "top_names = [n for n, s in sorted_list[-5:]]\n",
    "top = [(n, l) for n, l in avgsent_all_individual if n in top_names]\n",
    "bottom_names = [n for n, s in sorted_list[0:5]]\n",
    "bottom = [(n, l) for n, l in avgsent_all_individual if n in bottom_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3a2c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_text = \"Sentiment values of 5 highest sentiment, with total average for comparison.\"\n",
    "plot_sentiments([avgsent_all] + top, top_text, book_list_wchapter, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e14462a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fred_george = [(n, l) for n, l in avgsent_all_individual if n in [\"Fred_Weasley\", \"George_Weasley\"]]\n",
    "fred_george_text = \"Sentiment for Fred and George Weasley\"\n",
    "plot_sentiments(fred_george, fred_george_text, book_list_wchapter, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85642413",
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_text = \"Sentiment values of 5 lowest sentiment, with total average for comparison.\"\n",
    "plot_sentiments([avgsent_all] + bottom, bottom_text, book_list_wchapter, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ce6f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sirius_black = [(n, l) for n, l in avgsent_all_individual if n in [\"Sirius_Black\"]]\n",
    "sirius_black_text = \"Sentiment for Sirius Black\"\n",
    "plot_sentiments(sirius_black, sirius_black_text, book_list_wchapter, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5462e2ae",
   "metadata": {},
   "source": [
    "## Sentiment for main characters\n",
    ">Sentiment for the 3 main characters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ed889c",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_character_list = [\"Harry_Potter\", \"Ronald_Weasley\", \"Hermione_Granger\"]\n",
    "main_characters = [(n, l) for n, l in avgsent_all_individual if n in main_character_list]\n",
    "bottom_text = \"Sentiment values of 5 lowest sentiment, with total average for comparison.\"\n",
    "plot_sentiments([avgsent_all] + main_characters, bottom_text, book_list_wchapter, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
